{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2f0b866-6200-40dd-80c6-d6101575fabe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'course': 'data-engineering-zoomcamp',\n",
       "  'documents': [{'text': \"The purpose of this document is to capture frequently asked technical questions\\nThe exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  “Office Hours'' live.1\\nSubscribe to course public Google Calendar (it works from Desktop only).\\nRegister before the course starts using this link.\\nJoin the course Telegram channel with announcements.\\nDon’t forget to register in DataTalks.Club's Slack and join the channel.\",\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'Course - When will the course start?'},\n",
       "   {'text': 'GitHub - DataTalksClub data-engineering-zoomcamp#prerequisites',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'Course - What are the prerequisites for this course?'},\n",
       "   {'text': \"Yes, even if you don't register, you're still eligible to submit the homeworks.\\nBe aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute.\",\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'Course - Can I still join the course after the start date?'},\n",
       "   {'text': \"You don't need it. You're accepted. You can also just start learning and submitting homework without registering. It is not checked against any registered list. Registration is just to gauge interest before the start date.\",\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'Course - I have registered for the Data Engineering Bootcamp. When can I expect to receive the confirmation email?'},\n",
       "   {'text': 'You can start by installing and setting up all the dependencies and requirements:\\nGoogle cloud account\\nGoogle Cloud SDK\\nPython 3 (installed with Anaconda)\\nTerraform\\nGit\\nLook over the prerequisites and syllabus to see if you are comfortable with these subjects.',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'Course - What can I do before the course starts?'},\n",
       "   {'text': \"There are 3 Zoom Camps in a year, as of 2024. However, they are for separate courses:\\nData-Engineering (Jan - Apr)\\nMLOps (May - Aug)\\nMachine Learning (Sep - Jan)\\nThere's only one Data-Engineering Zoomcamp “live” cohort per year, for the certification. Same as for the other Zoomcamps.\\nThey follow pretty much the same schedule for each cohort per zoomcamp. For Data-Engineering it is (generally) from Jan-Apr of the year. If you’re not interested in the Certificate, you can take any zoom camps at any time, at your own pace, out of sync with any “live” cohort.\",\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'Course - how many Zoomcamps in a year?'},\n",
       "   {'text': 'Yes. For the 2024 edition we are using Mage AI instead of Prefect and re-recorded the terraform videos, For 2023, we used Prefect instead of Airflow..',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'Course - Is the current cohort going to be different from the previous cohort?'},\n",
       "   {'text': 'Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.\\nYou can also continue looking at the homeworks and continue preparing for the next cohort. I guess you can also start working on your final capstone project.',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'Course - Can I follow the course after it finishes?'},\n",
       "   {'text': 'Yes, the slack channel remains open and you can ask questions there. But always sDocker containers exit code w search the channel first and second, check the FAQ (this document), most likely all your questions are already answered here.\\nYou can also tag the bot @ZoomcampQABot to help you conduct the search, but don’t rely on its answers 100%, it is pretty good though.',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'Course - Can I get support if I take the course in the self-paced mode?'},\n",
       "   {'text': 'All the main videos are stored in the Main “DATA ENGINEERING” playlist (no year specified). The Github repository has also been updated to show each video with a thumbnail, that would bring you directly to the same playlist below.\\nBelow is the MAIN PLAYLIST’. And then you refer to the year specific playlist for additional videos for that year like for office hours videos etc. Also find this playlist pinned to the slack channel.\\nh\\nttps://youtube.com/playlist?list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&si=NspQhtZhZQs1B9F-',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'Course - Which playlist on YouTube should I refer to?'},\n",
       "   {'text': 'It depends on your background and previous experience with modules. It is expected to require about 5 - 15 hours per week. [source1] [source2]\\nYou can also calculate it yourself using this data and then update this answer.',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'Course - \\u200b\\u200bHow many hours per week am I expected to spend on this  course?'},\n",
       "   {'text': \"No, you can only get a certificate if you finish the course with a “live” cohort. We don't award certificates for the self-paced mode. The reason is you need to peer-review capstone(s) after submitting a project. You can only peer-review projects at the time the course is running.\",\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'Certificate - Can I follow the course in a self-paced mode and get a certificate?'},\n",
       "   {'text': 'The zoom link is only published to instructors/presenters/TAs.\\nStudents participate via Youtube Live and submit questions to Slido (link would be pinned in the chat when Alexey goes Live). The video URL should be posted in the announcements channel on Telegram & Slack before it begins. Also, you will see it live on the DataTalksClub YouTube Channel.\\nDon’t post your questions in chat as it would be off-screen before the instructors/moderators have a chance to answer it if the room is very active.',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'Office Hours - What is the video/zoom link to the stream for the “Office Hour” or workshop sessions?'},\n",
       "   {'text': 'Yes! Every “Office Hours” will be recorded and available a few minutes after the live session is over; so you can view (or rewatch) whenever you want.',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'Office Hours - I can’t attend the “Office hours” / workshop, will it be recorded?'},\n",
       "   {'text': 'You can find the latest and up-to-date deadlines here: https://docs.google.com/spreadsheets/d/e/2PACX-1vQACMLuutV5rvXg5qICuJGL-yZqIV0FBD84CxPdC5eZHf8TfzB-CJT_3Mo7U7oGVTXmSihPgQxuuoku/pubhtml\\nAlso, take note of Announcements from @Au-Tomator for any extensions or other news. Or, the form may also show the updated deadline, if Instructor(s) has updated it.',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'Homework - What are homework and project deadlines?'},\n",
       "   {'text': 'No, late submissions are not allowed. But if the form is still not closed and it’s after the due date, you can still submit the homework. confirm your submission by the date-timestamp on the Course page.y\\nOlder news:[source1] [source2]',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'Homework - Are late submissions of homework allowed?'},\n",
       "   {'text': 'Answer: In short, it’s your repository on github, gitlab, bitbucket, etc\\nIn long, your repository or any other location you have your code where a reasonable person would look at it and think yes, you went through the week and exercises.',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'Homework - What is the homework URL in the homework link?'},\n",
       "   {'text': 'After you submit your homework it will be graded based on the amount of questions in a particular homework. You can see how many points you have right on the page of the homework up top. Additionally in the leaderboard you will find the sum of all points you’ve earned - points for Homeworks, FAQs and Learning in Public. If homework is clear, others work as follows: if you submit something to FAQ, you get one point, for each learning in a public link you get one point.\\n(https://datatalks-club.slack.com/archives/C01FABYF2RG/p1706846846359379?thread_ts=1706825019.546229&cid=C01FABYF2RG)',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'Homework and Leaderboard - what is the system for points in the course management platform?'},\n",
       "   {'text': 'When you set up your account you are automatically assigned a random name such as “Lucid Elbakyan” for example. If you want to see what your Display name is.\\nGo to the Homework submission link →  https://courses.datatalks.club/de-zoomcamp-2024/homework/hw2 - Log in > Click on ‘Data Engineering Zoom Camp 2024’ > click on ‘Edit Course Profile’ - your display name is here, you can also change it should you wish:',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'Leaderboard - I am not on the leaderboard / how do I know which one I am on the leaderboard?'},\n",
       "   {'text': 'Yes, for simplicity (of troubleshooting against the recorded videos) and stability. [source]\\nBut Python 3.10 and 3.11 should work fine.',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'Environment - Is Python 3.9 still the recommended version to use in 2024?'},\n",
       "   {'text': 'You can set it up on your laptop or PC if you prefer to work locally from your laptop or PC.\\nYou might face some challenges, especially for Windows users. If you face cnd2\\nIf you prefer to work on the local machine, you may start with the week 1 Introduction to Docker and follow through.\\nHowever, if you prefer to set up a virtual machine, you may start with these first:\\nUsing GitHub Codespaces\\nSetting up the environment on a cloudV Mcodespace\\nI decided to work on a virtual machine because I have different laptops & PCs for my home & office, so I can work on this boot camp virtually anywhere.',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'Environment - Should I use my local machine, GCP, or GitHub Codespaces for my environment?'},\n",
       "   {'text': 'GitHub Codespaces offers you computing Linux resources with many pre-installed tools (Docker, Docker Compose, Python).\\nYou can also open any GitHub repository in a GitHub Codespace.',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'Environment - Is GitHub codespaces an alternative to using cli/git bash to ingest the data and create a docker file?'},\n",
       "   {'text': \"It's up to you which platform and environment you use for the course.\\nGithub codespaces or GCP VM are just possible options, but you can do the entire course from your laptop.\",\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'Environment - Do we really have to use GitHub codespaces? I already have PostgreSQL & Docker installed.'},\n",
       "   {'text': 'Choose the approach that aligns the most with your idea for the end project\\nOne of those should suffice. However, BigQuery, which is part of GCP, will be used, so learning that is probably a better option. Or you can set up a local environment for most of this course.',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'Environment - Do I need both GitHub Codespaces and GCP?'},\n",
       "   {'text': '1. To open Run command window, you can either:\\n(1-1) Use the shortcut keys: \\'Windows + R\\', or\\n(1-2) Right Click \"Start\", and click \"Run\" to open.\\n2. Registry Values Located in Registry Editor, to open it: Type \\'regedit\\' in the Run command window, and then press Enter.\\' 3. Now you can change the registry values \"Autorun\" in \"HKEY_CURRENT_USER\\\\Software\\\\Microsoft\\\\Command Processor\" from \"if exists\" to a blank.\\nAlternatively, You can simplify the solution by deleting the fingerprint saved within the known_hosts file. In Windows, this file is placed at  C:\\\\Users\\\\<your_user_name>\\\\.ssh\\\\known_host',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'This happens when attempting to connect to a GCP VM using VSCode on a Windows machine. Changing registry value in registry editor'},\n",
       "   {'text': 'For uniformity at least, but you’re not restricted to GCP, you can use other cloud platforms like AWS if you’re comfortable with other cloud platforms, since you get every service that’s been provided by GCP in Azure and AWS or others..\\nBecause everyone has a google account, GCP has a free trial period and gives $300 in credits  to new users. Also, we are working with BigQuery, which is a part of GCP.\\nNote that to sign up for a free GCP account, you must have a valid credit card.',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'Environment - Why are we using GCP and not other cloud providers?'},\n",
       "   {'text': 'No, if you use GCP and take advantage of their free trial.',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'Should I pay for cloud services?'},\n",
       "   {'text': 'You can do most of the course without a cloud. Almost everything we use (excluding BigQuery) can be run locally. We won’t be able to provide guidelines for some things, but most of the materials are runnable without GCP.\\nFor everything in the course, there’s a local alternative. You could even do the whole course locally.',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'Environment - The GCP and other cloud providers are unavailable in some countries. Is it possible to provide a guide to installing a home lab?'},\n",
       "   {'text': 'Yes, you can. Just remember to adapt all the information on the videos to AWS. Besides, the final capstone will be evaluated based on the task: Create a data pipeline! Develop a visualisation!\\nThe problem would be when you need help. You’d need to rely on  fellow coursemates who also use AWS (or have experience using it before), which might be in smaller numbers than those learning the course with GCP.\\nAlso see Is it possible to use x tool instead of the one tool you use?',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'Environment - I want to use AWS. May I do that?'},\n",
       "   {'text': 'We will probably have some calls during the Capstone period to clear some questions but it will be announced in advance if that happens.',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'Besides the “Office Hour” which are the live zoom calls?'},\n",
       "   {'text': 'We will use the same data, as the project will essentially remain the same as last year’s. The data is available here',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'Are we still using the NYC Trip data for January 2021? Or are we using the 2022 data?'},\n",
       "   {'text': 'No, but we moved the 2022 stuff here',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'Is the 2022 repo deleted?'},\n",
       "   {'text': 'Yes, you can use any tool you want for your project.',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'Can I use Airflow instead for my final project?'},\n",
       "   {'text': 'Yes, this applies if you want to use Airflow or Prefect instead of Mage, AWS or Snowflake instead of GCP products or Tableau instead of Metabase or Google data studio.\\nThe course covers 2 alternative data stacks, one using GCP and one using local installation of everything. You can use one of them or use your tool of choice.\\nShould you consider it instead of the one tool you use? That we can’t support you if you choose to use a different stack, also you would need to explain the different choices of tool for the peer review of your capstone project.',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'Is it possible to use tool “X” instead of the one tool you use in the course?'},\n",
       "   {'text': 'Star the repo! Share it with friends if you find it useful ❣️\\nCreate a PR if you see you can improve the text or the structure of the repository.',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'How can we contribute to the course?'},\n",
       "   {'text': 'Yes! Linux is ideal but technically it should not matter. Students last year used all 3 OSes successfully',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'Environment - Is the course [Windows/mac/Linux/...] friendly?'},\n",
       "   {'text': \"Have no idea how past cohorts got past this as I haven't read old slack messages, and no FAQ entries that I can find.\\nLater modules (module-05 & RisingWave workshop) use shell scripts in *.sh files and most Windows users not using WSL would hit a wall and cannot continue, even in git bash or MINGW64. This is why WSL environment setup is recommended from the start.\",\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'Environment - Roadblock for Windows users in modules with *.sh (shell scripts).'},\n",
       "   {'text': 'Yes to both! check out this document: https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/awesome-data-engineering.md',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'Any books or additional resources you recommend?'},\n",
       "   {'text': 'You will have two attempts for a project. If the first project deadline is over and you’re late or you submit the project and fail the first attempt, you have another chance to submit the project with the second attempt.',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'Project - What is Project Attemp #1 and Project Attempt #2 exactly?'},\n",
       "   {'text': \"The first step is to try to solve the issue on your own. Get used to solving problems and reading documentation. This will be a real life skill you need when employed. [ctrl+f] is your friend, use it! It is a universal shortcut and works in all apps/browsers.\\nWhat does the error say? There will often be a description of the error or instructions on what is needed or even how to fix it. I have even seen a link to the solution. Does it reference a specific line of your code?\\nRestart app or server/pc.\\nGoogle it, use ChatGPT, Bing AI etc.\\nIt is going to be rare that you are the first to have the problem, someone out there has posted the fly issue and likely the solution.\\nSearch using: <technology> <problem statement>. Example: pgcli error column c.relhasoids does not exist.\\nThere are often different solutions for the same problem due to variation in environments.\\nCheck the tech’s documentation. Use its search if available or use the browsers search function.\\nTry uninstall (this may remove the bad actor) and reinstall of application or reimplementation of action. Remember to restart the server/pc for reinstalls.\\nSometimes reinstalling fails to resolve the issue but works if you uninstall first.\\nPost your question to Stackoverflow. Read the Stackoverflow guide on posting good questions.\\nhttps://stackoverflow.com/help/how-to-ask\\nThis will be your real life. Ask an expert in the future (in addition to coworkers).\\nAsk in Slack\\nBefore asking a question,\\nCheck Pins (where the shortcut to the repo and this FAQ is located)\\nUse the slack app’s search function\\nUse the bot @ZoomcampQABot to do the search for you\\ncheck the FAQ (this document), use search [ctrl+f]\\nWhen asking a question, include as much information as possible:\\nWhat are you coding on? What OS?\\nWhat command did you run, which video did you follow? Etc etc\\nWhat error did you get? Does it have a line number to the “offending” code and have you check it for typos?\\nWhat have you tried that did not work? This answer is crucial as without it, helpers would ask you to do the suggestions in the error log first. Or just read this FAQ document.\\nDO NOT use screenshots, especially don’t take pictures from a phone.\\nDO NOT tag instructors, it may discourage others from helping you. Copy and paste errors; if it’s long, just post it in a reply to your thread.\\nUse ``` for formatting your code.\\nUse the same thread for the conversation (that means reply to your own thread).\\nDO NOT create multiple posts to discuss the issue.\\nlearYou may create a new post if the issue reemerges down the road. Describe what has changed in the environment.\\nProvide additional information in the same thread of the steps you have taken for resolution.\\nTake a break and come back later. You will be amazed at how often you figure out the solution after letting your brain rest. Get some fresh air, workout, play a video game, watch a tv show, whatever allows your brain to not think about it for a little while or even until the next day.\\nRemember technology issues in real life sometimes take days or even weeks to resolve.\\nIf somebody helped you with your problem and it's not in the FAQ, please add it there. It will help other students.\",\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'How to troubleshoot issues'},\n",
       "   {'text': 'When the troubleshooting guide above does not help resolve it and you need another pair of eyeballs to spot mistakes. When asking a question, include as much information as possible:\\nWhat are you coding on? What OS?\\nWhat command did you run, which video did you follow? Etc etc\\nWhat error did you get? Does it have a line number to the “offending” code and have you check it for typos?\\nWhat have you tried that did not work? This answer is crucial as without it, helpers would ask you to do the suggestions in the error log first. Or just read this FAQ document.',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'How to ask questions'},\n",
       "   {'text': 'After you create a GitHub account, you should clone the course repo to your local machine using the process outlined in this video: Git for Everybody: How to Clone a Repository from GitHub\\nHaving this local repository on your computer will make it easy for you to access the instructors’ code and make pull requests (if you want to add your own notes or make changes to the course content).\\nYou will probably also create your own repositories that host your notes, versions of your file, to do this. Here is a great tutorial that shows you how to do this: https://www.atlassian.com/git/tutorials/setting-up-a-repository\\nRemember to ignore large database, .csv, and .gz files, and other files that should not be saved to a repository. Use .gitignore for this: https://www.atlassian.com/git/tutorials/saving-changes/gitignore NEVER store passwords or keys in a git repo (even if that repo is set to private).\\nThis is also a great resource: https://dangitgit.com/',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'How do I use Git / GitHub for this course?'},\n",
       "   {'text': 'Error: Makefile:2: *** missing separator.  Stop.\\nSolution: Tabs in document should be converted to Tab instead of spaces. Follow this stack.',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'VS Code: Tab using spaces'},\n",
       "   {'text': \"If you’re running Linux on Windows Subsystem for Linux (WSL) 2, you can open HTML files from the guest (Linux) with whatever Internet Browser you have installed on the host (Windows). Just install wslu and open the page with wslview <file>, for example:\\nwslview index.html\\nYou can customise which browser to use by setting the BROWSER environment variable first. For example:\\nexport BROWSER='/mnt/c/Program Files/Firefox/firefox.exe'\",\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'Opening an HTML file with a Windows browser from Linux running on WSL'},\n",
       "   {'text': 'This tutorial shows you how to set up the Chrome Remote Desktop service on a Debian Linux virtual machine (VM) instance on Compute Engine. Chrome Remote Desktop allows you to remotely access applications with a graphical user interface.\\nTaxi Data - Yellow Taxi Trip Records downloading error, Error no or XML error webpage\\nWhen you try to download the 2021 data from TLC website, you get this error:\\nIf you click on the link, and ERROR 403: Forbidden on the terminal.\\nWe have a backup, so use it instead: https://github.com/DataTalksClub/nyc-tlc-data\\nSo the link should be https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz\\nNote: Make sure to unzip the “gz” file (no, the “unzip” command won’t work for this.)\\n“gzip -d file.gz”g',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'Set up Chrome Remote Desktop for Linux on Compute Engine'},\n",
       "   {'text': 'In this video, we store the data file as “output.csv”. The data file won’t store correctly if the file extension is csv.gz instead of csv. One alternative is to replace csv_name = “output.cs -v” with the file name given at the end of the URL. Notice that the URL for the yellow taxi data is: https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz where the highlighted part is the name of the file. We can parse this file name from the URL and use it as csv_name. That is, we can replace csv_name = “output.csv” with\\ncsv_name = url.split(“/”)[-1] . Then when we use csv_name to using pd.read_csv, there won’t be an issue even though the file name really has the extension csv.gz instead of csv since the pandas read_csv function can read csv.gz files directly.',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'Taxi Data - How to handle taxi data files, now that the files are available as *.csv.gz?'},\n",
       "   {'text': 'Yellow Trips: https://www1.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf\\nGreen Trips: https://www1.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_green.pdf',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'Taxi Data - Data Dictionary for NY Taxi data?'},\n",
       "   {'text': 'You can unzip this downloaded parquet file, in the command line. The result is a csv file which can be imported with pandas using the pd.read_csv() shown in the videos.\\n‘’’gunzip green_tripdata_2019-09.csv.gz’’’\\nSOLUTION TO USING PARQUET FILES DIRECTLY IN PYTHON SCRIPT ingest_data.py\\nIn the def main(params) add this line\\nparquet_name= \\'output.parquet\\'\\nThen edit the code which downloads the files\\nos.system(f\"wget {url} -O {parquet_name}\")\\nConvert the download .parquet file to csv and rename as csv_name to keep it relevant to the rest of the code\\ndf = pd.read_parquet(parquet_name)\\ndf.to_csv(csv_name, index=False)',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'Taxi Data - Unzip Parquet file'},\n",
       "   {'text': '“wget is not recognized as an internal or external command”, you need to install it.\\nOn Ubuntu, run:\\n$ sudo apt-get install wget\\nOn MacOS, the easiest way to install wget is to use Brew:\\n$ brew install wget\\nOn Windows, the easiest way to install wget is to use Chocolatey:\\n$ choco install wget\\nOr you can download a binary (https://gnuwin32.sourceforge.net/packages/wget.htm) and put it to any location in your PATH (e.g. C:/tools/)\\nAlso, you can following this step to install Wget on MS Windows\\n* Download the latest wget binary for windows from [eternallybored] (https://eternallybored.org/misc/wget/) (they are available as a zip with documentation, or just an exe)\\n* If you downloaded the zip, extract all (if windows built in zip utility gives an error, use [7-zip] (https://7-zip.org/)).\\n* Rename the file `wget64.exe` to `wget.exe` if necessary.\\n* Move wget.exe to your `Git\\\\mingw64\\\\bin\\\\`.\\nAlternatively, you can use a Python wget library, but instead of simply using “wget” you’ll need to use\\npython -m wget\\nYou need to install it with pip first:\\npip install wget\\nAlternatively, you can just paste the file URL into your web browser and download the file normally that way. You’ll want to move the resulting file into your working directory.\\nAlso recommended a look at the python library requests for the loading gz file  https://pypi.org/project/requests',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'lwget is not recognized as an internal or external command'},\n",
       "   {'text': 'Firstly, make sure that you add “!” before wget if you’re running your command in a Jupyter Notebook or CLI. Then, you can check one of this 2 things (from CLI):\\nUsing the Python library wget you installed with pip, try python -m wget <url>\\nWrite the usual command and add --no-check-certificate at the end. So it should be:\\n!wget <website_url> --no-check-certificate',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'wget - ERROR: cannot verify <website> certificate  (MacOS)'},\n",
       "   {'text': 'For those who wish to use the backslash as an escape character in Git Bash for Windows (as Alexey normally does), type in the terminal: bash.escapeChar=\\\\ (no need to include in .bashrc)',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'Git Bash - Backslash as an escape character in Git Bash for Windows'},\n",
       "   {'text': 'Instruction on how to store secrets that will be avialable in GitHub  Codespaces.\\nManaging your account-specific secrets for GitHub Codespaces - GitHub Docs',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'GitHub Codespaces - How to store secrets'},\n",
       "   {'text': \"Make sure you're able to start the Docker daemon, and check the issue immediately down below:\\nAnd don’t forget to update the wsl in powershell the  command is wsl –update\",\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'Docker - Cannot connect to Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?'},\n",
       "   {'text': \"As the official Docker for Windows documentation says, the Docker engine can either use the\\nHyper-V or WSL2 as its backend. However, a few constraints might apply\\nWindows 10 Pro / 11 Pro Users: \\nIn order to use Hyper-V as its back-end, you MUST have it enabled first, which you can do by following the tutorial: Enable Hyper-V Option on Windows 10 / 11\\nWindows 10 Home / 11 Home Users: \\nOn the other hand, Users of the 'Home' version do NOT have the option Hyper-V option enabled, which means, you can only get Docker up and running using the WSL2 credentials(Windows Subsystem for Linux). Url\\nYou can find the detailed instructions to do so here: rt ghttps://pureinfotech.com/install-wsl-windows-11/\\nIn case, you run into another issue while trying to install WSL2 (WslRegisterDistribution failed with error: 0x800701bc), Make sure you update the WSL2 Linux Kernel, following the guidelines here: \\n\\nhttps://github.com/microsoft/WSL/issues/5393\",\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'Docker - Error during connect: In the default daemon configuration on Windows, the docker client must be run with elevated privileges to connect.: Post: \"http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.24/containers/create\" : open //./pipe/docker_engine: The system cannot find the file specified'},\n",
       "   {'text': 'Whenever a `docker pull is performed (either manually or by `docker-compose up`), it attempts to fetch the given image name (pgadmin4, for the example above) from a repository (dbpage).\\nIF the repository is public, the fetch and download happens without any issue whatsoever.\\nFor instance:\\ndocker pull postgres:13\\ndocker pull dpage/pgadmin4\\nBE ADVISED:\\n\\nThe Docker Images we\\'ll be using throughout the Data Engineering Zoomcamp are all public (except when or if explicitly said otherwise by the instructors or co-instructors).\\n\\nMeaning: you are NOT required to perform a docker login to fetch them. \\n\\nSo if you get the message above saying \"docker login\\': denied: requested access to the resource is denied. That is most likely due to a typo in your image name:\\n\\nFor instance:\\n$ docker pull dbpage/pgadmin4\\nWill throw that exception telling you \"repository does not exist or may require \\'docker login\\'\\nError response from daemon: pull access denied for dbpage/pgadmin4, repository does not exist or \\nmay require \\'docker login\\': denied: requested access to the resource is denied\\nBut that actually happened because the actual image is dpage/pgadmin4 and NOT dbpage/pgadmin4\\nHow to fix it:\\n$ docker pull dpage/pgadmin4\\nEXTRA NOTES:\\nIn the real world, occasionally, when you\\'re working for a company or closed organisation, the Docker image you\\'re trying to fetch might be under a private repo that your DockerHub Username was granted access to.\\nFor which cases, you must first execute:\\n$ docker login\\nFill in the details of your username and password.\\nAnd only then perform the `docker pull` against that private repository\\nWhy am I encountering a \"permission denied\" error when creating a PostgreSQL Docker container for the New York Taxi Database with a mounted volume on macOS M1?\\nIssue Description:\\nWhen attempting to run a Docker command similar to the one below:\\ndocker run -it \\\\\\n-e POSTGRES_USER=\"root\" \\\\\\n-e POSTGRES_PASSWORD=\"root\" \\\\\\n-e POSTGRES_DB=\"ny_taxi\" \\\\\\n-v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n-p 5432:5432 \\\\mount\\npostgres:13\\nYou encounter the error message:\\ndocker: Error response from daemon: error while creating mount source path \\'/path/to/ny_taxi_postgres_data\\': chown /path/to/ny_taxi_postgres_data: permission denied.\\nSolution:\\n1- Stop Rancher Desktop:\\nIf you are using Rancher Desktop and face this issue, stop Rancher Desktop to resolve compatibility problems.\\n2- Install Docker Desktop:\\nInstall Docker Desktop, ensuring that it is properly configured and has the required permissions.\\n2-Retry Docker Command:\\nRun the Docker command again after switching to Docker Desktop. This step resolves compatibility issues on some systems.\\nNote: The issue occurred because Rancher Desktop was in use. Switching to Docker Desktop resolves compatibility problems and allows for the successful creation of PostgreSQL containers with mounted volumes for the New York Taxi Database on macOS M1.',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'Docker - docker pull dbpage'},\n",
       "   {'text': 'When I runned command to create postgre in docker container it created folder on my local machine to mount it to volume inside container. It has write and read protection and owned by user 999, so I could not delete it by simply drag to trash.  My obsidian could not started due to access error, so I had to change placement of this folder and delete old folder by this command:\\nsudo rm -r -f docker_test/\\n- where `rm` - remove, `-r` - recursively, `-f` - force, `docker_test/` - folder.',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'Docker - can’t delete local folder that mounted to docker volume'},\n",
       "   {'text': 'First off, make sure you\\'re running the latest version of Docker for Windows, which you can download from here. Sometimes using the menu to \"Upgrade\" doesn\\'t work (which is another clear indicator for you to uninstall, and reinstall with the latest version)\\nIf docker is stuck on starting, first try to switch containers by right clicking the docker symbol from the running programs and switch the containers from windows to linux or vice versa\\n[Windows 10 / 11 Pro Edition] The Pro Edition of Windows can run Docker either by using Hyper-V or WSL2 as its backend (Docker Engine)\\nIn order to use Hyper-V as its back-end, you MUST have it enabled first, which you can do by following the tutorial: Enable Hyper-V Option on Windows 10 / 11\\nIf you opt-in for WSL2, you can follow the same steps as detailed in the tutorial here',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': \"Docker - Docker won't start or is stuck in settings (Windows 10 / 11)\"},\n",
       "   {'text': \"It is recommended by the Docker do\\n[Windows 10 / 11 Home Edition] If you're running a Home Edition, you can still make it work with WSL2 (Windows Subsystem for Linux) by following the tutorial here\\nIf even after making sure your WSL2 (or Hyper-V) is set up accordingly, Docker remains stuck, you can try the option to Reset to Factory Defaults or do a fresh install.\",\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'Should I run docker commands from the windows file system or a file system of a Linux distribution in WSL?'},\n",
       "   {'text': 'More info in the Docker Docs on Best Practises',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'Docker - cs to store all code in your default Linux distro to get the best out of file system performance (since Docker runs on WSL2 backend by default for Windows 10 Home / Windows 11 Home users).'},\n",
       "   {'text': 'You may have this error:\\n$ docker run -it ubuntu bash\\nthe input device is not a TTY. If you are using mintty, try prefixing the command with \\'winpty\\'\\nerror:\\nSolution:\\nUse winpty before docker command (source)\\n$ winpty docker run -it ubuntu bash\\nYou also can make an alias:\\necho \"alias docker=\\'winpty docker\\'\" >> ~/.bashrc\\nOR\\necho \"alias docker=\\'winpty docker\\'\" >> ~/.bash_profile',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'Docker - The input device is not a TTY (Docker run for Windows)'},\n",
       "   {'text': \"You may have this error:\\nRetrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.u\\nrllib3.connection.HTTPSConnection object at 0x7efe331cf790>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')':\\n/simple/pandas/\\nPossible solution might be:\\n$ winpty docker run -it --dns=8.8.8.8 --entrypoint=bash python:3.9\",\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'Docker - Cannot pip install on Docker container (Windows)'},\n",
       "   {'text': 'Even after properly running the docker script the folder is empty in the vs code  then try this (For Windows)\\nwinpty docker run -it \\\\\\n-e POSTGRES_USER=\"root\" \\\\\\n-e POSTGRES_PASSWORD=\"root\" \\\\\\n-e POSTGRES_DB=\"ny_taxi\" \\\\\\n-v \"C:\\\\Users\\\\abhin\\\\dataengg\\\\DE_Project_git_connected\\\\DE_OLD\\\\week1_set_up\\\\docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data\" \\\\\\n-p 5432:5432 \\\\\\npostgres:13\\nHere quoting the absolute path in  the -v parameter is solving the issue and all the files are visible in the Vs-code ny_taxi folder as shown in the video',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'Docker - ny_taxi_postgres_data is empty'},\n",
       "   {'text': 'Check this article for details - Setting up docker in macOS\\nFrom researching it seems this method might be out of date, it seems that since docker changed their licensing model, the above is a bit hit and miss. What worked for me was to just go to the docker website and download their dmg. Haven’t had an issue with that method.',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'dasDocker - Setting up Docker on Mac'},\n",
       "   {'text': '$ docker run -it\\\\\\n-e POSTGRES_USER=\"root\" \\\\\\n-e POSTGRES_PASSWORD=\"admin\" \\\\\\n-e POSTGRES_DB=\"ny_taxi\" \\\\\\n-v \"/mnt/path/to/ny_taxi_postgres_data\":\"/var/lib/postgresql/data\" \\\\\\n-p 5432:5432 \\\\\\npostgres:13\\nCCW\\nThe files belonging to this database system will be owned by user \"postgres\".\\nThis use The database cluster will be initialized with locale \"en_US.utf8\".\\nThe default databerrorase encoding has accordingly been set to \"UTF8\".\\nxt search configuration will be set to \"english\".\\nData page checksums are disabled.\\nfixing permissions on existing directory /var/lib/postgresql/data ... initdb: f\\nerror: could not change permissions of directory \"/var/lib/postgresql/data\": Operation not permitted  volume\\nOne way to solve this issue is to create a local docker volume and map it to postgres data directory /var/lib/postgresql/data\\nThe input dtc_postgres_volume_local must match in both commands below\\n$ docker volume create --name dtc_postgres_volume_local -d local\\n$ docker run -it\\\\\\n-e POSTGRES_USER=\"root\" \\\\\\n-e POSTGRES_PASSWORD=\"root\" \\\\\\n-e POSTGRES_DB=\"ny_taxi\" \\\\\\n-v dtc_postgres_volume_local:/var/lib/postgresql/data \\\\\\n-p 5432:5432\\\\\\npostgres:13\\nTo verify the above command works in (WSL2 Ubuntu 22.04, verified 2024-Jan), go to the Docker Desktop app and look under Volumes - dtc_postgres_volume_local would be listed there. The folder ny_taxi_postgres_data would however be empty, since we used an alternative config.\\nAn alternate error could be:\\ninitdb: error: directory \"/var/lib/postgresql/data\" exists but is not empty\\nIf you want to create a new database system, either remove or empthe directory \"/var/lib/postgresql/data\" or run initdb\\nwitls',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': '1Docker - Could not change permissions of directory \"/var/lib/postgresql/data\": Operation not permitted'},\n",
       "   {'text': 'Mapping volumes on Windows could be tricky. The way it was done in the course video doesn’t work for everyone.\\nFirst, if yo\\nmove your data to some folder without spaces. E.g. if your code is in “C:/Users/Alexey Grigorev/git/…”, move it to “C:/git/…”\\nTry replacing the “-v” part with one of the following options:\\n-v /c:/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\\n-v //c:/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\\n-v /c/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\\n-v //c/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\\n--volume //driveletter/path/ny_taxi_postgres_data/:/var/lib/postgresql/data\\nwinpty docker run -it\\n-e POSTGRES_USER=\"root\"\\n-e POSTGRES_PASSWORD=\"root\"\\n-e POSTGRES_DB=\"ny_taxi\"\\n-v /c:/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\\n-p 5432:5432\\npostgres:1\\nTry adding winpty before the whole command\\n3\\nwin\\nTry adding quotes:\\n-v \"/c:/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\"\\n-v \"//c:/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\"\\n-v “/c/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\"\\n-v \"//c/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\"\\n-v \"c:\\\\some\\\\path\\\\ny_taxi_postgres_data\":/var/lib/postgresql/data\\nNote:  (Window) if it automatically creates a folder called “ny_taxi_postgres_data;C” suggests you have problems with volume mapping, try deleting both folders and replacing “-v” part with other options. For me “//c/” works instead of “/c/”. And it will work by automatically creating a correct folder called “ny_taxi_postgres_data”.\\nA possible solution to this error would be to use /”$(pwd)”/ny_taxi_postgres_data:/var/lib/postgresql/data (with quotes’ position varying as in the above list).\\nYes for windows use the command it works perfectly fine\\n-v /”$(pwd)”/ny_taxi_postgres_data:/var/lib/postgresql/data\\nImportant: note how the quotes are placed.\\nIf none of these options work, you can use a volume name instead of the path:\\n-v ny_taxi_postgres_data:/var/lib/postgresql/data\\nFor Mac: You can wrap $(pwd) with quotes like the highlighted.\\ndocker run -it \\\\\\n-e POSTGRES_USER=\"root\" \\\\\\n-e POSTGRES_PASSWORD=\"root\" \\\\\\n-e POSTGRES_DB=\"ny_taxi\" \\\\\\n-v \"$(pwd)\"/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n-p 5432:5432 \\\\\\nPostgres:13\\ndocker run -it \\\\\\n-e POSTGRES_USER=\"root\" \\\\\\n-e POSTGRES_PASSWORD=\"root\" \\\\\\n-e POSTGRES_DB=\"ny_taxi\" \\\\\\n-v \"$(pwd)\"/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n-p 5432:5432 \\\\\\npostgres:13\\nSource:https://stackoverflow.com/questions/48522615/docker-error-invalid-reference-format-repository-name-must-be-lowercase',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'Docker - invalid reference format: repository name must be lowercase (Mounting volumes with Docker on Windows)'},\n",
       "   {'text': 'Change the mounting path. Replace it with one of following:\\n-v /e/zoomcamp/...:/var/lib/postgresql/data\\n-v /c:/.../ny_taxi_postgres_data:/var/lib/postgresql/data\\\\ (leading slash in front of c:)',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'Docker - Error response from daemon: invalid mode: \\\\Program Files\\\\Git\\\\var\\\\lib\\\\postgresql\\\\data.'},\n",
       "   {'text': 'When you run this command second time\\ndocker run -it \\\\\\n-e POSTGRES_USER=\"root\" \\\\\\n-e POSTGRES_PASSWORD=\"root\" \\\\\\n-e POSTGRES_DB=\"ny_taxi\" \\\\\\n-v <your path>:/var/lib/postgresql/data \\\\\\n-p 5432:5432 \\\\\\npostgres:13\\nThe error message above could happen. That means you should not mount on the second run. This command helped me:\\nWhen you run this command second time\\ndocker run -it \\\\\\n-e POSTGRES_USER=\"root\" \\\\\\n-e POSTGRES_PASSWORD=\"root\" \\\\\\n-e POSTGRES_DB=\"ny_taxi\" \\\\\\n-p 5432:5432 \\\\\\npostgres:13',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': \"Docker - Error response from daemon: error while creating buildmount source path '/run/desktop/mnt/host/c/<your path>': mkdir /run/desktop/mnt/host/c: file exists\"},\n",
       "   {'text': 'This error appeared when running the command: docker build -t taxi_ingest:v001 .\\nWhen feeding the database with the data the user id of the directory ny_taxi_postgres_data was changed to 999, so my user couldn’t access it when running the above command. Even though this is not the problem here it helped to raise the error due to the permission issue.\\nSince at this point we only need the files Dockerfile and ingest_data.py, to fix this error one can run the docker build command on a different directory (having only these two files).\\nA more complete explanation can be found here: https://stackoverflow.com/questions/41286028/docker-build-error-checking-context-cant-stat-c-users-username-appdata\\nYou can fix the problem by changing the permission of the directory on ubuntu with following command:\\nsudo chown -R $USER dir_path\\nOn windows follow the link: https://thegeekpage.com/take-ownership-of-a-file-folder-through-command-prompt-in-windows-10/ \\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tAdded by\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tKenan Arslanbay',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': \"Docker - build error: error checking context: 'can't stat '/home/user/repos/data-engineering/week_1_basics_n_setup/2_docker_sql/ny_taxi_postgres_data''.\"},\n",
       "   {'text': 'You might have installed docker via snap. Run “sudo snap status docker” to verify.\\nIf you have “error: unknown command \"status\", see \\'snap help\\'.” as a response than deinstall docker and install via the official website\\nBind for 0.0.0.0:5432 failed: port is a',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'Docker - ERRO[0000] error waiting for container: context canceled'},\n",
       "   {'text': 'Found the issue in the PopOS linux. It happened because our user didn’t have authorization rights to the host folder ( which also caused folder seems empty, but it didn’t!).\\n✅Solution:\\nJust add permission for everyone to the corresponding folder\\nsudo chmod -R 777 <path_to_folder>\\nExample:\\nsudo chmod -R 777 ny_taxi_postgres_data/',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'Docker - build error checking context: can’t stat ‘/home/fhrzn/Projects/…./ny_taxi_postgres_data’'},\n",
       "   {'text': 'This happens on Ubuntu/Linux systems when trying to run the command to build the Docker container again.\\n$ docker build -t taxi_ingest:v001 .\\nA folder is created to host the Docker files. When the build command is executed again to rebuild the pipeline or create a new one the error is raised as there are no permissions on this new folder. Grant permissions by running this comtionmand;\\n$ sudo chmod -R 755 ny_taxi_postgres_data\\nOr use 777 if you still see problems. 755 grants write access to only the owner.',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'Docker - failed to solve with frontend dockerfile.v0: failed to read dockerfile: error from sender: open ny_taxi_postgres_data: permission denied.'},\n",
       "   {'text': 'Get the network name via: $ docker network ls.',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'Docker - Docker network name'},\n",
       "   {'text': 'Sometimes, when you try to restart a docker image configured with a network name, the above message appears. In this case, use the following command with the appropriate container name:\\n>>> If the container is running state, use docker stop <container_name>\\n>>> then, docker rm pg-database\\nOr use docker start instead of docker run in order to restart the docker image without removing it.',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'Docker - Error response from daemon: Conflict. The container name \"pg-database\" is already in use by container “xxx”.  You have to remove (or rename) that container to be able to reuse that name.'},\n",
       "   {'text': 'Typical error: sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name \"pgdatabase\" to address: Name or service not known\\nWhen running docker-compose up -d see which network is created and use this for the ingestions script instead of pg-network and see the name of the database to use instead of pgdatabase\\nE.g.:\\npg-network becomes 2docker_default\\nPgdatabase becomes 2docker-pgdatabase-1',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'Docker - ingestion when using docker-compose could not translate host name'},\n",
       "   {'text': 'terraformRun this command before starting your VM:\\nOn Intel CPU:\\nmodprobe -r kvm_intel\\nmodprobe kvm_intel nested=1\\nOn AMD CPU:\\nmodprobe -r kvm_amd\\nmodprobe kvm_amd nested=1',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'Docker - Cannot install docker on MacOS/Windows 11 VM running on top of Linux (due to Nested virtualization).'},\n",
       "   {'text': 'It’s very easy to manage your docker container, images, network and compose projects from VS Code.\\nJust install the official extension and launch it from the left side icon.\\nIt will work even if your Docker runs on WSL2, as VS Code can easily connect with your Linux.\\nDocker - How to stop a container?\\nUse the following command:\\n$ docker stop <container_id>',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'Docker - Connecting from VS Code'},\n",
       "   {'text': \"When you see this in logs, your container with postgres is not accepting any requests, so if you attempt to connect, you'll get this error:\\nconnection failed: server closed the connection unexpectedly\\nThis probably means the server terminated abnormally before or while processing the request.\\nIn this case, you need to delete the directory with data (the one you map to the container with the -v flag) and restart the container.\",\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'Docker - PostgreSQL Database directory appears to contain a database. Database system is shut down'},\n",
       "   {'text': 'On few versions of Ubuntu, snap command can be used to install Docker.\\nsudo snap install docker',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'Docker not installable on Ubuntu'},\n",
       "   {'text': 'error: could not change permissions of directory \"/var/lib/postgresql/data\": Operation not permitted  volume\\nif you have used the prev answer (just before this) and have created a local docker volume, then you need to tell the compose file about the named volume:\\nvolumes:\\ndtc_postgres_volume_local:  # Define the named volume here\\n# services mentioned in the compose file auto become part of the same network!\\nservices:\\nyour remaining code here . . .\\nnow use docker volume inspect dtc_postgres_volume_local to see the location by checking the value of Mountpoint\\nIn my case, after i ran docker compose up the mounting dir created was named ‘docker_sql_dtc_postgres_volume_local’ whereas it should have used the already existing ‘dtc_postgres_volume_local’\\nAll i did to fix this is that I renamed the existing ‘dtc_postgres_volume_local’ to ‘docker_sql_dtc_postgres_volume_local’ and removed the newly created one (just be careful when doing this)\\nrun docker compose up again and check if the table is there or not!',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'Docker-Compose - mounting error'},\n",
       "   {'text': 'Couldn’t translate host name to address\\nMake sure postgres database is running.\\n\\n\\u200b\\u200bUse the command to start containers in detached mode: docker-compose up -d\\n(data-engineering-zoomcamp) hw % docker compose up -d\\n[+] Running 2/2\\n⠿ Container pg-admin     Started                                                                                                                                                                      0.6s\\n⠿ Container pg-database  Started\\nTo view the containers use: docker ps.\\n(data-engineering-zoomcamp) hw % docker ps\\nCONTAINER ID   IMAGE            COMMAND                  CREATED          STATUS          PORTS                           NAMES\\nfaf05090972e   postgres:13      \"docker-entrypoint.s…\"   39 seconds ago   Up 37 seconds   0.0.0.0:5432->5432/tcp          pg-database\\n6344dcecd58f   dpage/pgadmin4   \"/entrypoint.sh\"         39 seconds ago   Up 37 seconds   443/tcp, 0.0.0.0:8080->80/tcp   pg-admin\\nhw\\nTo view logs for a container: docker logs <containerid>\\n(data-engineering-zoomcamp) hw % docker logs faf05090972e\\nPostgreSQL Database directory appears to contain a database; Skipping initialization\\n2022-01-25 05:58:45.948 UTC [1] LOG:  starting PostgreSQL 13.5 (Debian 13.5-1.pgdg110+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit\\n2022-01-25 05:58:45.948 UTC [1] LOG:  listening on IPv4 address \"0.0.0.0\", port 5432\\n2022-01-25 05:58:45.948 UTC [1] LOG:  listening on IPv6 address \"::\", port 5432\\n2022-01-25 05:58:45.954 UTC [1] LOG:  listening on Unix socket \"/var/run/postgresql/.s.PGSQL.5432\"\\n2022-01-25 05:58:45.984 UTC [28] LOG:  database system was interrupted; last known up at 2022-01-24 17:48:35 UTC\\n2022-01-25 05:58:48.581 UTC [28] LOG:  database system was not properly shut down; automatic recovery in\\nprogress\\n2022-01-25 05:58:48.602 UTC [28] LOG:  redo starts at 0/872A5910\\n2022-01-25 05:59:33.726 UTC [28] LOG:  invalid record length at 0/98A3C160: wanted 24, got 0\\n2022-01-25 05:59:33.726 UTC [28\\n] LOG:  redo done at 0/98A3C128\\n2022-01-25 05:59:48.051 UTC [1] LOG:  database system is ready to accept connections\\nIf docker ps doesn’t show pgdatabase running, run: docker ps -a\\nThis should show all containers, either running or stopped.\\nGet the container id for pgdatabase-1, and run',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'Docker-Compose - Error translating host name to address'},\n",
       "   {'text': 'After executing `docker-compose up` - if you lose database data and are unable to successfully execute your Ingestion script (to re-populate your database) but receive the following error:\\nsqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name /data_pgadmin:/var/lib/pgadmin\"pg-database\" to address: Name or service not known\\nDocker compose is creating its own default network since it is no longer specified in a docker execution command or file. Docker Compose will emit to logs the new network name. See the logs after executing `docker compose up` to find the network name and change the network name argument in your Ingestion script.\\nIf problems persist with pgcli, we can use HeidiSQL,usql\\nKrishna Anand',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'Docker-Compose -  Data retention (could not translate host name \"pg-database\" to address: Name or service not known)'},\n",
       "   {'text': 'It returns --> Error response from daemon: network 66ae65944d643fdebbc89bd0329f1409dec2c9e12248052f5f4c4be7d1bdc6a3 not found\\nTry:\\ndocker ps -a to see all the stopped & running containers\\nd to nuke all the containers\\nTry: docker-compose up -d again ports\\nOn localhost:8080 server → Unable to connect to server: could not translate host name \\'pg-database\\' to address: Name does not resolve\\nTry: new host name, best without “ - ” e.g. pgdatabase\\nAnd on docker-compose.yml, should specify docker network & specify the same network in both  containers\\nservices:\\npgdatabase:\\nimage: postgres:13\\nenvironment:\\n- POSTGRES_USER=root\\n- POSTGRES_PASSWORD=root\\n- POSTGRES_DB=ny_taxi\\nvolumes:\\n- \"./ny_taxi_postgres_data:/var/lib/postgresql/data:rw\"\\nports:\\n- \"5431:5432\"\\nnetworks:\\n- pg-network\\npgadmin:\\nimage: dpage/pgadmin4\\nenvironment:\\n- PGADMIN_DEFAULT_EMAIL=admin@admin.com\\n- PGADMIN_DEFAULT_PASSWORD=root\\nports:\\n- \"8080:80\"\\nnetworks:\\n- pg-network\\nnetworks:\\npg-network:\\nname: pg-network',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'Docker-Compose - Hostname does not resolve'},\n",
       "   {'text': 'So one common issue is when you run docker-compose on GCP, postgres won’t persist it’s data to mentioned path for example:\\nservices:\\n…\\n…\\npgadmin:\\n…\\n…\\nVolumes:\\n“./pgadmin”:/var/lib/pgadmin:wr”\\nMight not work so in this use you can use Docker Volume to make it persist, by simply changing\\nservices:\\n…\\n….\\npgadmin:\\n…\\n…\\nVolumes:\\npgadmin:/var/lib/pgadmin\\nvolumes:\\nPgadmin:',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'Docker-Compose - Persist PGAdmin docker contents on GCP'},\n",
       "   {'text': 'The docker will keep on crashing continuously\\nNot working after restart\\ndocker engine stopped\\nAnd failed to fetch extensions pop ups will on screen non-stop\\nSolution :\\nTry checking if latest version of docker is installed / Try updating the docker\\nIf Problem still persist then final solution is to reinstall docker\\n(Just have to fetch images again else no issues)',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'Docker engine stopped_failed to fetch extensions'},\n",
       "   {'text': 'As per the lessons,\\nPersisting pgAdmin configuration (i.e. server name) is done by adding a “volumes” section:\\nservices:\\npgdatabase:\\n[...]\\npgadmin:\\nimage: dpage/pgadmin4\\nenvironment:\\n- PGADMIN_DEFAULT_EMAIL=admin@admin.com\\n- PGADMIN_DEFAULT_PASSWORD=root\\nvolumes:\\n- \"./pgAdmin_data:/var/lib/pgadmin/sessions:rw\"\\nports:\\n- \"8080:80\"\\nIn the example above, ”pgAdmin_data” is a folder on the host machine, and “/var/lib/pgadmin/sessions” is the session settings folder in the pgAdmin container.\\nBefore running docker-compose up on the YAML file, we also need to give the pgAdmin container access to write to the “pgAdmin_data” folder. The container runs with a username called “5050” and user group “5050”. The bash command to give access over the mounted volume is:\\nsudo chown -R 5050:5050 pgAdmin_data',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'Docker-Compose - Persist PGAdmin configuration'},\n",
       "   {'text': 'This happens if you did not create the docker group and added your user. Follow these steps from the link:\\nguides/docker-without-sudo.md at main · sindresorhus/guides · GitHub\\nAnd then press ctrl+D to log-out and log-in again. pgAdmin: Maintain state so that it remembers your previous connection\\nIf you are tired of having to setup your database connection each time that you fire up the containers, all you have to do is create a volume for pgAdmin:\\nIn your docker-compose.yaml file, enter the following into your pgAdmin declaration:\\nvolumes:\\n- type: volume\\nsource: pgadmin_data\\ntarget: /var/lib/pgadmin\\nAlso add the following to the end of the file:ls\\nvolumes:\\nPgadmin_data:',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'Docker-Compose - dial unix /var/run/docker.sock: connect: permission denied'},\n",
       "   {'text': 'This is happen to me after following 1.4.1 video where we are installing docker compose in our Google Cloud VM. In my case, the docker-compose file downloaded from github named docker-compose-linux-x86_64 while it is more convenient to use docker-compose command instead. So just change the docker-compose-linux-x86_64 into docker-compose.',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'Docker-Compose - docker-compose still not available after changing .bashrc'},\n",
       "   {'text': 'Installing pass via ‘sudo apt install pass’ helped to solve the issue. More about this can be found here: https://github.com/moby/buildkit/issues/1078',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'Docker-Compose - Error getting credentials after running docker-compose up -d'},\n",
       "   {'text': \"For everyone who's having problem with Docker compose, getting the data in postgres and similar issues, please take care of the following:\\ncreate a new volume on docker (either using the command line or docker desktop app)\\nmake the following changes to your docker-compose.yml file (see attachment)\\nset low_memory=false when importing the csv file (df = pd.read_csv('yellow_tripdata_2021-01.csv', nrows=1000, low_memory=False))\\nuse the below function (in the upload-data.ipynb) for better tracking of your ingestion process (see attachment)\\nOrder of execution:\\n(1) open terminal in 2_docker_sql folder and run docker compose up\\n(2) ensure no other containers are running except the one you just executed (pgadmin and pgdatabase)\\n(3) open jupyter notebook and begin the data ingestion\\n(4) open pgadmin and set up a server (make sure you use the same configurations as your docker-compose.yml file like the same name (pgdatabase), port, databasename (ny_taxi) etc.\",\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'Docker-Compose - Errors pertaining to docker-compose.yml and pgadmin setup'},\n",
       "   {'text': 'Locate config.json file for docker (check your home directory; Users/username/.docker).\\nModify credsStore to credStore\\nSave and re-run',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'Docker Compose up -d error getting credentials - err: exec: \"docker-credential-desktop\": executable file not found in %PATH%, out: ``'},\n",
       "   {'text': 'To figure out which docker-compose you need to download from https://github.com/docker/compose/releases you can check your system with these commands:\\nuname -s  -> return Linux most likely\\nuname -m -> return \"flavor\"\\nOr try this command -\\nsudo curl -L \"https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'Docker-Compose - Which docker-compose binary to use for WSL?'},\n",
       "   {'text': 'If you wrote the docker-compose.yaml file exactly like the video, you might run into an error like this:dev\\nservice \"pgdatabase\" refers to undefined volume dtc_postgres_volume_local: invalid compose project\\nIn order to make it work, you need to include the volume in your docker-compose file. Just add the following:\\nvolumes:\\ndtc_postgres_volume_local:\\n(Make sure volumes are at the same level as services.)',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'Docker-Compose - Error undefined volume in Windows/WSL'},\n",
       "   {'text': 'Error:  initdb: error: could not change permissions of directory\\nIssue: WSL and Windows do not manage permissions in the same way causing conflict if using the Windows file system rather than the WSL file system.\\nSolution: Use Docker volumes.\\nWhy: Volume is used for storage of persistent data and not for use of transferring files. A local volume is unnecessary.\\nBenefit: This resolves permission issues and allows for better management of volumes.\\nNOTE: the ‘user:’ is not necessary if using docker volumes, but is if using local drive.\\n</>  docker-compose.yaml\\nservices:\\npostgres:\\nimage: postgres:15-alpine\\ncontainer_name: postgres\\nuser: \"0:0\"\\nenvironment:\\n- POSTGRES_USER=postgres\\n- POSTGRES_PASSWORD=postgres\\n- POSTGRES_DB=ny_taxi\\nvolumes:\\n- \"pg-data:/var/lib/postgresql/data\"\\nports:\\n- \"5432:5432\"\\nnetworks:\\n- pg-network\\npgadmin:\\nimage: dpage/pgadmin4\\ncontainer_name: pgadmin\\nuser: \"${UID}:${GID}\"\\nenvironment:\\n- PGADMIN_DEFAULT_EMAIL=email@some-site.com\\n- PGADMIN_DEFAULT_PASSWORD=pgadmin\\nvolumes:\\n- \"pg-admin:/var/lib/pgadmin\"\\nports:\\n- \"8080:80\"\\nnetworks:\\n- pg-network\\nnetworks:\\npg-network:\\nname: pg-network\\nvolumes:\\npg-data:\\nname: ingest_pgdata\\npg-admin:\\nname: ingest_pgadmin',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'WSL Docker directory permissions error'},\n",
       "   {'text': 'Cause : If Running on git bash or vm in windows pgadmin doesnt work easily LIbraries like psycopg2 and libpq ar required still the error persists.\\nSolution- I use psql instead of pgadmin totally same\\nPip install psycopg2\\ndock',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'Docker - If pgadmin is not working for Querying in Postgres Use PSQL'},\n",
       "   {'text': 'Cause:\\nIt happens because the apps are not updated. To be specific, search for any pending updates for Windows Terminal, WSL and Windows Security updates.\\nSolution\\nfor updating Windows terminal which worked for me:\\nGo to Microsoft Store.\\nGo to the library of apps installed in your system.\\nSearch for Windows terminal.\\nUpdate the app and restart your system to  see the changes.\\nFor updating the Windows security updates:\\nGo to Windows updates and check if there are any pending updates from Windows, especially security updates.\\nDo restart your system once the updates are downloaded and installed successfully.',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'WSL - Insufficient system resources exist to complete the requested service.'},\n",
       "   {'text': 'Up restardoting the same issue appears. Happens out of the blue on windows.\\nSolution 1: Fixing DNS Issue (credit: reddit) this worked for me personally\\nreg add \"HKLM\\\\System\\\\CurrentControlSet\\\\Services\\\\Dnscache\" /v \"Start\" /t REG_DWORD /d \"4\" /f\\nRestart your computer and then enable it with the following\\nreg add \"HKLM\\\\System\\\\CurrentControlSet\\\\Services\\\\Dnscache\" /v \"Start\" /t REG_DWORD /d \"2\" /f\\nRestart your OS again. It should work.\\nSolution 2: right click on running Docker icon (next to clock) and chose \"Switch to Linux containers\"\\nbash: conda: command not found\\nDatabase is uninitialized and superuser password is not specified.\\nDatabase is uninitialized and superuser password is not specified.',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'WSL - WSL integration with distro Ubuntu unexpectedly stopped with exit code 1.'},\n",
       "   {'text': 'Issue when trying to run the GPC VM through SSH through WSL2,  probably because WSL2 isn’t looking for .ssh keys in the correct folder. My case I was trying to run this command in the terminal and getting an error\\nPC:/mnt/c/Users/User/.ssh$ ssh -i gpc [username]@[my external IP]\\nYou can try to use sudo before the command\\nSudo .ssh$ ssh -i gpc [username]@[my external IP]\\nYou can also try to cd to your folder and change the permissions for the private key SSH file.\\nchmod 600 gpc\\nIf that doesn’t work, create a .ssh folder in the home diretory of WSL2 and copy the content of windows .ssh folder to that new folder.\\ncd ~\\nmkdir .ssh\\ncp -r /mnt/c/Users/YourUsername/.ssh/* ~/.ssh/\\nYou might need to adjust the permissions of the files and folders in the .ssh directory.',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'WSL - Permissions too open at Windows'},\n",
       "   {'text': 'Such as the issue above, WSL2 may not be referencing the correct .ssh/config path from Windows. You can create a config file at the home directory of WSL2.\\ncd ~\\nmkdir .ssh\\nCreate a config file in this new .ssh/ folder referencing this folder:\\nHostName [GPC VM external IP]\\nUser [username]\\nIdentityFile ~/.ssh/[private key]',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'WSL - Could not resolve host name'},\n",
       "   {'text': 'Change TO Socket\\npgcli -h 127.0.0.1 -p 5432 -u root -d ny_taxi\\npgcli -h 127.0.0.1 -p 5432 -u root -d ny_taxi',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'PGCLI - connection failed: :1), port 5432 failed: could not receive data from server: Connection refused could not send SSL negotiation packet: Connection refused'},\n",
       "   {'text': 'probably some installation error, check out sy',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'PGCLI --help error'},\n",
       "   {'text': 'In this section of the course, the 5432 port of pgsql is mapped to your computer’s 5432 port. Which means you can access the postgres database via pgcli directly from your computer.\\nSo No, you don’t need to run it inside another container. Your local system will do.',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'PGCLI - INKhould we run pgcli inside another docker container?'},\n",
       "   {'text': 'FATAL:  password authentication failed for user \"root\"\\nobservations: Below in bold do not forget the folder that was created ny_taxi_postgres_data\\nThis happens if you have a local Postgres installation in your computer. To mitigate this, use a different port, like 5431, when creating the docker container, as in: -p 5431: 5432\\nThen, we need to use this port when connecting to pgcli, as shown below:\\npgcli -h localhost -p 5431 -u root -d ny_taxi\\nThis will connect you to your postgres docker container, which is mapped to your host’s 5431 port (though you might choose any port of your liking as long as it is not occupied).\\nFor a more visual and detailed explanation, feel free to check the video 1.4.2 - Port Mapping and Networks in Docker\\nIf you want to debug: the following can help (on a MacOS)\\nTo find out if something is blocking your port (on a MacOS):\\nYou can use the lsof command to find out which application is using a specific port on your local machine. `lsof -i :5432`wi\\nOr list the running postgres services on your local machine with launchctl\\nTo unload the running service on your local machine (on a MacOS):\\nunload the launch agent for the PostgreSQL service, which will stop the service and free up the port  \\n`launchctl unload -w ~/Library/LaunchAgents/homebrew.mxcl.postgresql.plist`\\nthis one to start it again\\n`launchctl load -w ~/Library/LaunchAgents/homebrew.mxcl.postgresql.plist`\\nChanging port from 5432:5432 to 5431:5432 helped me to avoid this error.',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'PGCLI - FATAL: password authentication failed for user \"root\" (You already have Postgres)'},\n",
       "   {'text': 'I get this error\\npgcli -h localhost -p 5432 -U root -d ny_taxi\\nTraceback (most recent call last):\\nFile \"/opt/anaconda3/bin/pgcli\", line 8, in <module>\\nsys.exit(cli())\\nFile \"/opt/anaconda3/lib/python3.9/site-packages/click/core.py\", line 1128, in __call__\\nreturn self.main(*args, **kwargs)\\nFile \"/opt/anaconda3/lib/python3.9/sitYe-packages/click/core.py\", line\\n1053, in main\\nrv = self.invoke(ctx)\\nFile \"/opt/anaconda3/lib/python3.9/site-packages/click/core.py\", line 1395, in invoke\\nreturn ctx.invoke(self.callback, **ctx.params)\\nFile \"/opt/anaconda3/lib/python3.9/site-packages/click/core.py\", line 754, in invoke\\nreturn __callback(*args, **kwargs)\\nFile \"/opt/anaconda3/lib/python3.9/site-packages/pgcli/main.py\", line 880, in cli\\nos.makedirs(config_dir)\\nFile \"/opt/anaconda3/lib/python3.9/os.py\", line 225, in makedirspython\\nmkdir(name, mode)PermissionError: [Errno 13] Permission denied: \\'/Users/vray/.config/pgcli\\'\\nMake sure you install pgcli without sudo.\\nThe recommended approach is to use conda/anaconda to make sure your system python is not affected.\\nIf conda install gets stuck at \"Solving environment\" try these alternatives: https://stackoverflow.com/questions/63734508/stuck-at-solving-environment-on-anaconda',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': \"PGCLI - PermissionError: [Errno 13] Permission denied: '/some/path/.config/pgcli'\"},\n",
       "   {'text': 'ImportError: no pq wrapper available.\\nAttempts made:\\n- couldn\\'t import \\\\dt\\nopg \\'c\\' implementation: No module named \\'psycopg_c\\'\\n- couldn\\'t import psycopg \\'binary\\' implementation: No module named \\'psycopg_binary\\'\\n- couldn\\'t import psycopg \\'python\\' implementation: libpq library not found\\nSolution:\\nFirst, make sure your Python is set to 3.9, at least.\\nAnd the reason for that is we have had cases of \\'psycopg2-binary\\' failing to install because of an old version of Python (3.7.3). \\n\\n0. You can check your current python version with: \\n$ python -V(the V must be capital)\\n1. Based on the previous output, if you\\'ve got a 3.9, skip to Step #2\\n   Otherwispye better off with a new environment with 3.9\\n$ conda create –name de-zoomcamp python=3.9\\n$ conda activate de-zoomcamp\\n2. Next, you should be able to install the lib for postgres like this:\\n```\\n$ e\\n$ pip install psycopg2_binary\\n```\\n3. Finally, make sure you\\'re also installing pgcli, but use conda for that:\\n```\\n$ pgcli -h localhost -U root -d ny_taxisudo\\n```\\nThere, you should be good to go now!\\nAnother solution:\\nRun this\\npip install \"psycopg[binary,pool]\"',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'PGCLI - no pq wrapper available.'},\n",
       "   {'text': 'If your Bash prompt is stuck on the password command for postgres\\nUse winpty:\\nwinpty pgcli -h localhost -p 5432 -u root -d ny_taxi\\nAlternatively, try using Windows terminal or terminal in VS code.\\nEditPGCLI -connection failed: FATAL:  password authentication failed for user \"root\"\\nThe error above was faced continually despite inputting the correct password\\nSolution\\nOption 1: Stop the PostgreSQL service on Windows\\nOption 2 (using WSL): Completely uninstall Protgres 12 from Windows and install postgresql-client on WSL (sudo apt install postgresql-client-common postgresql-client libpq-dev)\\nOption 3: Change the port of the docker container\\nNEW SOLUTION: 27/01/2024\\nPGCLI -connection failed: FATAL:  password authentication failed for user \"root\"\\nIf you’ve got the error above, it’s probably because you were just like me, closed the connection to the Postgres:13 image in the previous step of the tutorial, which is\\n\\ndocker run -it \\\\\\n-e POSTGRES_USER=root \\\\\\n-e POSTGRES_PASSWORD=root \\\\\\n-e POSTGRES_DB=ny_taxi \\\\\\n-v d:/git/data-engineering-zoomcamp/week_1/docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n-p 5432:5432 \\\\\\npostgres:13\\nSo keep the database connected and you will be able to implement all the next steps of the tutorial.',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'PGCLI -  stuck on password prompt'},\n",
       "   {'text': 'Problem: If you have already installed pgcli but bash doesn\\'t recognize pgcli\\nOn Git bash: bash: pgcli: command not found\\nOn Windows Terminal: pgcli: The term \\'pgcli\\' is not recognized…\\nSolution: Try adding a Python path C:\\\\Users\\\\...\\\\AppData\\\\Roaming\\\\Python\\\\Python39\\\\Scripts to Windows PATH\\nFor details:\\nGet the location: pip list -v\\nCopy C:\\\\Users\\\\...\\\\AppData\\\\Roaming\\\\Python\\\\Python39\\\\site-packages\\n3. Replace site-packages with Scripts: C:\\\\Users\\\\...\\\\AppData\\\\Roaming\\\\Python\\\\Python39\\\\Scripts\\nIt can also be that you have Python installed elsewhere.\\nFor me it was under c:\\\\python310\\\\lib\\\\site-packages\\nSo I had to add c:\\\\python310\\\\lib\\\\Scripts to PATH, as shown below.\\nPut the above path in \"Path\" (or \"PATH\") in System Variables\\nReference: https://stackoverflow.com/a/68233660',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'PGCLI - pgcli: command not found'},\n",
       "   {'text': 'In case running pgcli  locally causes issues or you do not want to install it locally you can use it running in a Docker container instead.\\nBelow the usage with values used in the videos of the course for:\\nnetwork name (docker network)\\npostgres related variables for pgcli\\nHostname\\nUsername\\nPort\\nDatabase name\\n$ docker run -it --rm --network pg-network ai2ys/dockerized-pgcli:4.0.1\\n175dd47cda07:/# pgcli -h pg-database -U root -p 5432 -d ny_taxi\\nPassword for root:\\nServer: PostgreSQL 16.1 (Debian 16.1-1.pgdg120+1)\\nVersion: 4.0.1\\nHome: http://pgcli.com\\nroot@pg-database:ny_taxi> \\\\dt\\n+--------+------------------+-------+-------+\\n| Schema | Name             | Type  | Owner |\\n|--------+------------------+-------+-------|\\n| public | yellow_taxi_data | table | root  |\\n+--------+------------------+-------+-------+\\nSELECT 1\\nTime: 0.009s\\nroot@pg-database:ny_taxi>',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'PGCLI - running in a Docker container'},\n",
       "   {'text': 'PULocationID will not be recognized but “PULocationID” will be. This is because unquoted \"Localidentifiers are case insensitive. See docs.',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'PGCLI - case sensitive use “Quotations” around columns with capital letters'},\n",
       "   {'text': 'When using the command `\\\\d <database name>` you get the error column `c.relhasoids does not exist`.\\nResolution:\\nUninstall pgcli\\nReinstall pgclidatabase \"ny_taxi\" does not exist\\nRestart pc',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'PGCLI - error column c.relhasoids does not exist'},\n",
       "   {'text': \"This happens while uploading data via the connection in jupyter notebook\\nengine = create_engine('postgresql://root:root@localhost:5432/ny_taxi')\\nThe port 5432 was taken by another postgres. We are not connecting to the port in docker, but to the port on our machine. Substitute 5431 or whatever port you mapped to for port 5432.\\nAlso if this error is still persistent , kindly check if you have a service in windows running postgres , Stopping that service will resolve the issue\",\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'Postgres - OperationalError: (psycopg2.OperationalError) connection to server at \"localhost\" (::1), port 5432 failed: FATAL:  password authentication failed for user \"root\"'},\n",
       "   {'text': 'Can happen when connecting via pgcli\\npgcli -h localhost -p 5432 -U root -d ny_taxi\\nOr while uploading data via the connection in jupyter notebook\\nengine = create_engine(\\'postgresql://root:root@localhost:5432/ny_taxi\\')\\nThis can happen when Postgres is already installed on your computer. Changing the port can resolve that (e.g. from 5432 to 5431).\\nTo check whether there even is a root user with the ability to login:\\nTry: docker exec -it <your_container_name> /bin/bash\\nAnd then run\\n???\\nAlso, you could change port from 5432:5432 to 5431:5432\\nOther solution that worked:\\nChanging `POSTGRES_USER=juroot` to `PGUSER=postgres`\\nBased on this: postgres with docker compose gives FATAL: role \"root\" does not exist error - Stack Overflow\\nAlso `docker compose down`, removing folder that had postgres volume, running `docker compose up` again.',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'Postgres - OperationalError: (psycopg2.OperationalError) connection to server at \"localhost\" (::1), port 5432 failed: FATAL:  role \"root\" does not exist'},\n",
       "   {'text': '~\\\\anaconda3\\\\lib\\\\site-packages\\\\psycopg2\\\\__init__.py in connect(dsn, connection_factory, cursor_factory, **kwargs)\\n120\\n121     dsn = _ext.make_dsn(dsn, **kwargs)\\n--> 122     conn = _connect(dsn, connection_factory=connection_factory, **kwasync)\\n123     if cursor_factory is not None:\\n124         conn.cursor_factory = cursor_factory\\nOperationalError: (psycopg2.OperationalError) connection to server at \"localhost\" (::1), port 5432 failed: FATAL:  database \"ny_taxi\" does not exist\\nMake sure postgres is running. You can check that by running `docker ps`\\n✅Solution: If you have postgres software installed on your computer before now, build your instance on a different port like 8080 instead of 5432',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'Postgres - OperationalError: (psycopg2.OperationalError) connection to server at \"localhost\" (::1), port 5432 failed: FATAL:  dodatabase \"ny_taxi\" does not exist'},\n",
       "   {'text': \"Issue:\\ne…\\nSolution:\\npip install psycopg2-binary\\nIf you already have it, you might need to update it:\\npip install psycopg2-binary --upgrade\\nOther methods, if the above fails:\\nif you are getting the “ ModuleNotFoundError: No module named 'psycopg2' “ error even after the above installation, then try updating conda using the command conda update -n base -c defaults conda. Or if you are using pip, then try updating it before installing the psycopg packages i.e\\nFirst uninstall the psycopg package\\nThen update conda or pip\\nThen install psycopg again using pip.\\nif you are still facing error with r pcycopg2 and showing pg_config not found then you will have to install postgresql. in MAC it is brew install postgresql\",\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': \"Postgres - ModuleNotFoundError: No module named 'psycopg2'\"},\n",
       "   {'text': 'In the join queries, if we mention the column name directly or enclosed in single quotes it’ll throw an error says “column does not exist”.\\n✅Solution: But if we enclose the column names in double quotes then it will work',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'Postgres - \"Column does not exist\" but it actually does (Pyscopg2 error in MacBook Pro M2)'},\n",
       "   {'text': 'pgAdmin has a new version. Create server dialog may not appear. Try using register-> server instead.',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'pgAdmin - Create server dialog does not appear'},\n",
       "   {'text': 'Using GitHub Codespaces in the browser resulted in a blank screen after the login to pgAdmin (running in a Docker container). The terminal of the pgAdmin container was showing the following error message:\\nCSRFError: 400 Bad Request: The referrer does not match the host.\\nSolution #1:\\nAs recommended in the following issue  https://github.com/pgadmin-org/pgadmin4/issues/5432 setting the following environment variable solved it.\\nPGADMIN_CONFIG_WTF_CSRF_ENABLED=\"False\"\\nModified “docker run” command\\ndocker run --rm -it \\\\\\n-e PGADMIN_DEFAULT_EMAIL=\"admin@admin.com\" \\\\\\n-e PGADMIN_DEFAULT_PASSWORD=\"root\" \\\\\\n-e PGADMIN_CONFIG_WTF_CSRF_ENABLED=\"False\" \\\\\\n-p \"8080:80\" \\\\\\n--name pgadmin \\\\\\n--network=pg-network \\\\\\ndpage/pgadmin4:8.2\\nSolution #2:\\nUsing the local installed VSCode to display GitHub Codespaces.\\nWhen using GitHub Codespaces in the locally installed VSCode (opening a Codespace or creating/starting one) this issue did not occur.',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'pgAdmin - Blank/white screen after login (browser)'},\n",
       "   {'text': 'I am using a Mac Pro device and connect to the GCP Compute Engine via Remote SSH - VSCode. But when I trying to run the PgAdmin container via docker run or docker compose command, I am failed to access the pgAdmin address via my browser. I have switched to another browser, but still can not access the pgAdmin address. So I modified a little bit the configuration from the previous DE Zoomcamp repository like below and can access the pgAdmin address:\\nSolution #1:\\nModified “docker run” command\\ndocker run --rm -it \\\\\\n-e PGADMIN_DEFAULT_EMAIL=\"admin@admin.com\" \\\\\\n-e PGADMIN_DEFAULT_PASSWORD=\"pgadmin\" \\\\\\n-e PGADMIN_CONFIG_WTF_CSRF_ENABLED=\"False\" \\\\\\n-e PGADMIN_LISTEN_ADDRESS=0.0.0.0 \\\\\\n-e PGADMIN_LISTEN_PORT=5050 \\\\\\n-p 5050:5050 \\\\\\n--network=de-zoomcamp-network \\\\\\n--name pgadmin-container \\\\\\n--link postgres-container \\\\\\n-t dpage/pgadmin4\\nSolution #2:\\nModified docker-compose.yaml configuration (via “docker compose up” command)\\npgadmin:\\nimage: dpage/pgadmin4\\ncontainer_name: pgadmin-conntainer\\nenvironment:\\n- PGADMIN_DEFAULT_EMAIL=admin@admin.com\\n- PGADMIN_DEFAULT_PASSWORD=pgadmin\\n- PGADMIN_CONFIG_WTF_CSRF_ENABLED=False\\n- PGADMIN_LISTEN_ADDRESS=0.0.0.0\\n- PGADMIN_LISTEN_PORT=5050\\nvolumes:\\n- \"./pgadmin_data:/var/lib/pgadmin/data\"\\nports:\\n- \"5050:5050\"\\nnetworks:\\n- de-zoomcamp-network\\ndepends_on:\\n- postgres-conntainer\\nPython - ModuleNotFoundError: No module named \\'pysqlite2\\'\\nImportError: DLL load failed while importing _sqlite3: The specified module could not be found. ModuleNotFoundError: No module named \\'pysqlite2\\'\\nThe issue seems to arise from the missing of sqlite3.dll in path \".\\\\Anaconda\\\\Dlls\\\\\".\\n✅I solved it by simply copying that .dll file from \\\\Anaconda3\\\\Library\\\\bin and put it under the path mentioned above. (if you are using anaconda)',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'pgAdmin - Can not access/open the PgAdmin address via browser'},\n",
       "   {'text': 'If you follow the video 1.2.2 - Ingesting NY Taxi Data to Postgres and you execute all the same\\nsteps as Alexey does, you will ingest all the data (~1.3 million rows) into the table yellow_taxi_data as expected.\\nHowever, if you try to run the whole script in the Jupyter notebook for a second time from top to bottom, you will be missing the first chunk of 100000 records. This is because there is a call to the iterator before the while loop that puts the data in the table. The while loop therefore starts by ingesting the second chunk, not the first.\\n✅Solution: remove the cell “df=next(df_iter)” that appears higher up in the notebook than the while loop. The first time w(df_iter) is called should be within the while loop.\\n📔Note: As this notebook is just used as a way to test the code, it was not intended to be run top to bottom, and the logic is tidied up in a later step when it is instead inserted into a .py file for the pipeline',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'Python - Ingestion with Jupyter notebook - missing 100000 records'},\n",
       "   {'text': '{t_end - t_start} seconds\")\\nimport pandas as pd\\ndf = pd.read_csv(\\'path/to/file.csv.gz\\', /app/ingest_data.py:1: DeprecationWarning:)\\nIf you prefer to keep the uncompressed csv (easier preview in vscode and similar), gzip files can be unzipped using gunzip (but not unzip). On a Ubuntu local or virtual machine, you may need to apt-get install gunzip first.',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'Python - Iteration csv without error'},\n",
       "   {'text': \"Pandas can interpret “string” column values as “datetime” directly when reading the CSV file using “pd.read_csv” using the parameter “parse_dates”, which for example can contain a list of column names or column indices. Then the conversion afterwards is not required anymore.\\npandas.read_csv — pandas 2.1.4 documentation (pydata.org)\\nExample from week 1\\nimport pandas as pd\\ndf = pd.read_csv(\\n'yellow_tripdata_2021-01.csv',\\nnrows=100,\\nparse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime'])\\ndf.info()\\nwhich will output\\n<class 'pandas.core.frame.DataFrame'>\\nRangeIndex: 100 entries, 0 to 99\\nData columns (total 18 columns):\\n#   Column                 Non-Null Count  Dtype\\n---  ------                 --------------  -----\\n0   VendorID               100 non-null    int64\\n1   tpep_pickup_datetime   100 non-null    datetime64[ns]\\n2   tpep_dropoff_datetime  100 non-null    datetime64[ns]\\n3   passenger_count        100 non-null    int64\\n4   trip_distance          100 non-null    float64\\n5   RatecodeID             100 non-null    int64\\n6   store_and_fwd_flag     100 non-null    object\\n7   PULocationID           100 non-null    int64\\n8   DOLocationID           100 non-null    int64\\n9   payment_type           100 non-null    int64\\n10  fare_amount            100 non-null    float64\\n11  extra                  100 non-null    float64\\n12  mta_tax                100 non-null    float64\\n13  tip_amount             100 non-null    float64\\n14  tolls_amount           100 non-null    float64\\n15  improvement_surcharge  100 non-null    float64\\n16  total_amount           100 non-null    float64\\n17  congestion_surcharge   100 non-null    float64\\ndtypes: datetime64[ns](2), float64(9), int64(6), object(1)\\nmemory usage: 14.2+ KB\",\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'iPython - Pandas parsing dates with ‘read_csv’'},\n",
       "   {'text': 'os.system(f\"curl -LO {url} -o {csv_name}\")',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'Python - Python cant ingest data from the github link provided using curl'},\n",
       "   {'text': 'When a CSV file is compressed using Gzip, it is saved with a \".csv.gz\" file extension. This file type is also known as a Gzip compressed CSV file. When you want to read a Gzip compressed CSV file using Pandas, you can use the read_csv() function, which is specifically designed to read CSV files. The read_csv() function accepts several parameters, including a file path or a file-like object. To read a Gzip compressed CSV file, you can pass the file path of the \".csv.gz\" file as an argument to the read_csv() function.\\nHere is an example of how to read a Gzip compressed CSV file using Pandas:\\ndf = pd.read_csv(\\'file.csv.gz\\'\\n, compression=\\'gzip\\'\\n, low_memory=False\\n)',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'Python - Pandas can read *.csv.gzip'},\n",
       "   {'text': \"Contrary to panda’s read_csv method there’s no such easy way to iterate through and set chunksize for parquet files. We can use PyArrow (Apache Arrow Python bindings) to resolve that.\\nimport pyarrow.parquet as pq\\noutput_name = “https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-01.parquet”\\nparquet_file = pq.ParquetFile(output_name)\\nparquet_size = parquet_file.metadata.num_rows\\nengine = create_engine(f'postgresql://{user}:{password}@{host}:{port}/{db}')\\ntable_name=”yellow_taxi_schema”\\n# Clear table if exists\\npq.read_table(output_name).to_pandas().head(n=0).to_sql(name=table_name, con=engine, if_exists='replace')\\n# default (and max) batch size\\nindex = 65536\\nfor i in parquet_file.iter_batches(use_threads=True):\\nt_start = time()\\nprint(f'Ingesting {index} out of {parquet_size} rows ({index / parquet_size:.0%})')\\ni.to_pandas().to_sql(name=table_name, con=engine, if_exists='append')\\nindex += 65536\\nt_end = time()\\nprint(f'\\\\t- it took %.1f seconds' % (t_end - t_start))\",\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'Python - How to iterate through and ingest parquet file'},\n",
       "   {'text': 'Error raised during the jupyter notebook’s cell execution:\\nfrom sqlalchemy import create_engine.\\nSolution: Version of Python module “typing_extensions” >= 4.6.0. Can be updated by Conda or pip.',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': \"Python - SQLAlchemy - ImportError: cannot import name 'TypeAliasType' from 'typing_extensions'.\"},\n",
       "   {'text': 'create_engine(\\'postgresql://root:root@localhost:5432/ny_taxi\\')  I get the error \"TypeError: \\'module\\' object is not callable\"\\nSolution:\\nconn_string = \"postgresql+psycopg://root:root@localhost:5432/ny_taxi\"\\nengine = create_engine(conn_string)',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': \"Python - SQLALchemy - TypeError 'module' object is not callable\"},\n",
       "   {'text': \"Error raised during the jupyter notebook’s cell execution:\\nengine = create_engine('postgresql://root:root@localhost:5432/ny_taxi').\\nSolution: Need to install Python module “psycopg2”. Can be installed by Conda or pip.\",\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': \"Python - SQLAlchemy - ModuleNotFoundError: No module named 'psycopg2'.\"},\n",
       "   {'text': 'Unable to add Google Cloud SDK PATH to Windows\\nWindows error: The installer is unable to automatically update your system PATH. Please add  C:\\\\tools\\\\google-cloud-sdk\\\\bin\\nif you are constantly getting this feedback. Might be that you needed to add Gitbash to your Windows path:\\nOne way of doing that is to use conda: ‘If you are not already using it\\nDownload the Anaconda Navigator\\nMake sure to check the box (add conda to the path when installing navigator: although not recommended do it anyway)\\nYou might also need to install git bash if you are not already using it(or you might need to uninstall it to reinstall it properly)\\nMake sure to check the following boxes while you install Gitbash\\nAdd a GitBash to Windows Terminal\\nUse Git and optional Unix tools from the command prompt\\nNow open up git bash and type conda init bash This should modify your bash profile\\nAdditionally, you might want to use Gitbash as your default terminal.\\nOpen your Windows terminal and go to settings, on the default profile change Windows power shell to git bash',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'GCP - Unable to add Google Cloud SDK PATH to Windows'},\n",
       "   {'text': 'It asked me to create a project. This should be done from the cloud console. So maybe we don’t need this FAQ.\\nWARNING: Project creation failed: HttpError accessing <https://cloudresourcemanager.googleapis.com/v1/projects?alt=json>: response: <{\\'vtpep_pickup_datetimeary\\': \\'Origin, X-Origin, Referer\\', \\'content-type\\': \\'application/json; charset=UTF-8\\', \\'content-encoding\\': \\'gzip\\', \\'date\\': \\'Mon, 24 Jan 2022 19:29:12 GMT\\', \\'server\\': \\'ESF\\', \\'cache-control\\': \\'private\\', \\'x-xss-protection\\': \\'0\\', \\'x-frame-options\\': \\'SAMEORIGIN\\', \\'x-content-type-options\\': \\'nosniff\\', \\'server-timing\\': \\'gfet4t7; dur=189\\', \\'alt-svc\\': \\'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000,h3-Q050=\":443\"; ma=2592000,h3-Q046=\":443\"; ma=2592000,h3-Q043=\":443\"; ma=2592000,quic=\":443\"; ma=2592000; v=\"46,43\"\\', \\'transfer-encoding\\': \\'chunked\\', \\'status\\': 409}>, content <{\\n\"error\": {\\n\"code\": 409,\\n\"message\": \"Requested entity alreadytpep_pickup_datetime exists\",\\n\"status\": \"ALREADY_EXISTS\"\\n}\\n}\\nFrom Stackoverflow: https://stackoverflow.com/questions/52561383/gcloud-cli-cannot-create-project-the-project-id-you-specified-is-already-in-us?rq=1\\nProject IDs are unique across all projects. That means if any user ever had a project with that ID, you cannot use it. testproject is pretty common, so it\\'s not surprising it\\'s already taken.',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'GCP - Project creation failed: HttpError accessing … Requested entity alreadytpep_pickup_datetime exists'},\n",
       "   {'text': 'If you receive the error: “Error 403: The project to be billed is associated with an absent billing account., accountDisabled” It is most likely because you did not enter YOUR project ID. The snip below is from video 1.3.2\\nThe value you enter here will be unique to each student. You can find this value on your GCP Dashboard when you login.\\nAshish Agrawal\\nAnother possibility is that you have not linked your billing account to your current project',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'GCP - The project to be billed is associated with an absent billing account'},\n",
       "   {'text': 'GCP Account Suspension Inquiry\\nIf Google refuses your credit/debit card, try another - I’ve got an issue with Kaspi (Kazakhstan) but it worked with TBC (Georgia).\\nUnfortunately, there’s small hope that support will help.\\nIt seems that Pyypl web-card should work too.',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'GCP - OR-CBAT-15 ERROR Google cloud free trial account'},\n",
       "   {'text': 'The ny-rides.json is your private file in Google Cloud Platform (GCP). \\n\\nAnd here’s the way to find it:\\nGCP -> Select project with your  instance -> IAM & Admin -> Service Accounts Keys tab -> add key, JSON as key type, then click create\\nNote: Once you go into Service Accounts Keys tab, click the email, then you can see the “KEYS” tab where you can add key as a JSON as its key type',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'GCP - Where can I find the “ny-rides.json” file?'},\n",
       "   {'text': 'In this lecture, Alexey deleted his instance in Google Cloud. Do I have to do it?\\nNope. Do not delete your instance in Google Cloud platform. Otherwise, you have to do this twice for the week 1 readings.',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'GCP - Do I need to delete my instance in Google Cloud?'},\n",
       "   {'text': 'System Resource Usage:\\ntop or htop: Shows real-time information about system resource usage, including CPU, memory, and processes.\\nfree -h: Displays information about system memory usage and availability.\\ndf -h: Shows disk space usage of file systems.\\ndu -h <directory>: Displays disk usage of a specific directory.\\nRunning Processes:\\nps aux: Lists all running processes along with detailed information.\\nNetwork:\\nifconfig or ip addr show: Shows network interface configuration.\\nnetstat -tuln: Displays active network connections and listening ports.\\nHardware Information:\\nlscpu: Displays CPU information.\\nlsblk: Lists block devices (disks and partitions).\\nlshw: Lists hardware configuration.\\nUser and Permissions:\\nwho: Shows who is logged on and their activities.\\nw: Displays information about currently logged-in users and their processes.\\nPackage Management:\\napt list --installed: Lists installed packages (for Ubuntu and Debian-based systems)',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'Commands to inspect the health of your VM:'},\n",
       "   {'text': 'if you’ve got the error\\n│ Error: Error updating Dataset \"projects/<your-project-id>/datasets/demo_dataset\": googleapi: Error 403: Billing has not been enabled for this project. Enable billing at https://console.cloud.google.com/billing. The default table expiration time must be less than 60 days, billingNotEnabled\\nbut you’ve set your billing account indeed, then try to disable billing for the project and enable it again. It worked for ME!',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'Billing account has not been enabled for this project. But you’ve done it indeed!'},\n",
       "   {'text': 'for windows if you having trouble install SDK try follow these steps on the link, if you getting this error:\\nThese credentials will be used by any library that requests Application Default Credentials (ADC).\\nWARNING:\\nCannot find a quota project to add to ADC. You might receive a \"quota exceeded\" or \"API not enabled\" error. Run $ gcloud auth application-default set-quota-project to add a quota project.\\nFor me:\\nI reinstalled the sdk using unzip file “install.bat”,\\nafter successfully checking gcloud version,\\nrun gcloud init to set up project before\\nyou run gcloud auth application-default login\\nhttps://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_1_basics_n_setup/1_terraform_gcp/windows.md\\nGCP VM - I cannot get my Virtual Machine to start because GCP has no resources.\\nClick on your VM\\nCreate an image of your VM\\nOn the page of the image, tell GCP to create a new VM instance via the image\\nOn the settings page, change the location',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'GCP - Windows Google Cloud SDK install issue:gcp'},\n",
       "   {'text': 'The reason this video about the GCP VM exists is that many students had problems configuring their env. You can use your own env if it works for you.\\nAnd the advantage of using your own environment is that if you are working in a Github repo where you can commit, you will be able to commit the changes that you do. In the VM the repo is cloned via HTTPS so it is not possible to directly commit, even if you are the owner of the repo.',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'GCP VM - Is it necessary to use a GCP VM? When is it useful?'},\n",
       "   {'text': \"I am trying to create a directory but it won't let me do it\\nUser1@DESKTOP-PD6UM8A MINGW64 /\\n$ mkdir .ssh\\nmkdir: cannot create directory ‘.ssh’: Permission denied\\nYou should do it in your home directory. Should be your home (~)\\nLocal. But it seems you're trying to do it in the root folder (/). Should be your home (~)\\nLink to Video 1.4.1\",\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'GCP VM - mkdir: cannot create directory ‘.ssh’: Permission denied'},\n",
       "   {'text': \"Failed to save '<file>': Unable to write file 'vscode-remote://ssh-remote+de-zoomcamp/home/<user>/data_engineering_course/week_2/airflow/dags/<file>' (NoPermissions (FileSystemError): Error: EACCES: permission denied, open '/home/<user>/data_engineering_course/week_2/airflow/dags/<file>')\\nYou need to change the owner of the files you are trying to edit via VS Code. You can run the following command to change the ownership.\\nssh\\nsudo chown -R <user> <path to your directory>\",\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'GCP VM - Error while saving the file in VM via VS Code'},\n",
       "   {'text': 'Question: I connected to my VM perfectly fine last week (ssh) but when I tried again this week, the connection request keeps timing out.\\n✅Answer: Start your VM. Once the VM is running, copy its External IP and paste that into your config file within the ~/.ssh folder.\\ncd ~/.ssh\\ncode config ← this opens the config file in VSCode',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': '. GCP VM - VM connection request timeout'},\n",
       "   {'text': '(reference: https://serverfault.com/questions/953290/google-compute-engine-ssh-connect-to-host-ip-port-22-operation-timed-out)Go to edit your VM.\\nGo to section Automation\\nAdd Startup script\\n```\\n#!/bin/bash\\nsudo ufw allow ssh\\n```\\nStop and Start VM.',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'GCP VM -  connect to host port 22 no route to host'},\n",
       "   {'text': 'You can easily forward the ports of pgAdmin, postgres and Jupyter Notebook using the built-in tools in Ubuntu and without any additional client:\\nFirst, in the VM machine, launch docker-compose up -d and jupyter notebook in the correct folder.\\nFrom the local machine, execute: ssh -i ~/.ssh/gcp -L 5432:localhost:5432 username@external_ip_of_vm\\nExecute the same command but with ports 8080 and 8888.\\nNow you can access pgAdmin on local machine in browser typing localhost:8080\\nFor Jupyter Notebook, type localhost:8888 in the browser of your local machine. If you have problems with the credentials, it is possible that you have to copy the link with the access token provided in the logs of the terminal of the VM machine when you launched the jupyter notebook command.\\nTo forward both pgAdmin and postgres use, ssh -i ~/.ssh/gcp -L 5432:localhost:5432 -L 8080:localhost:8080 modito@35.197.218.128',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'GCP VM - Port forwarding from GCP without using VS Code'},\n",
       "   {'text': 'If you are using MS VS Code and running gcloud in WSL2, when you first try to login to gcp via the gcloud cli gcloud auth application-default login, you will see a message like this, and nothing will happen\\nAnd there might be a prompt to ask if you want to open it via browser, if you click on it, it will open up a page with error message\\nSolution : you should instead hover on the long link, and ctrl + click the long link\\n\\nClick configure Trusted Domains here\\n\\nPopup will appear, pick first or second entry\\nNext time you gcloud auth, the login page should popup via default browser without issues',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'GCP gcloud + MS VS Code - gcloud auth hangs'},\n",
       "   {'text': 'It is an internet connectivity error, terraform is somehow not able to access the online registry. Check your VPN/Firewall settings (or just clear cookies or restart your network). Try terraform init again after this, it should work.',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'Terraform - Error: Failed to query available provider packages │ Could not retrieve the list of available versions for provider hashicorp/google: could not query │ provider registry for registry.terrafogorm.io/hashicorp/google: the request failed after 2 attempts, │ please try again later'},\n",
       "   {'text': \"The issue was with the network. Google is not accessible in my country, I am using a VPN. And The terminal program does not automatically follow the system proxy and requires separate proxy configuration settings.I opened a Enhanced Mode in Clash, which is a VPN app, and 'terraform apply' works! So if you encounter the same issue, you can ask help for your vpn provider.\",\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'Terraform - Error:Post \"https://storage.googleapis.com/storage/v1/b?alt=json&prettyPrint=false&project=coherent-ascent-379901\": oauth2: cannot fetch token: Post \"https://oauth2.googleapis.com/token\": dial tcp 172.217.163.42:443: i/o timeout'},\n",
       "   {'text': 'https://techcommunity.microsoft.com/t5/azure-developer-community-blog/configuring-terraform-on-windows-10-linux-sub-system/ba-p/393845',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'Terraform - Install for WSL'},\n",
       "   {'text': 'https://github.com/hashicorp/terraform/issues/14513',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'Terraform - Error acquiring the state lock'},\n",
       "   {'text': 'When running\\nterraform apply\\non wsl2 I\\'ve got this error:\\n│ Error: Post \"https://storage.googleapis.com/storage/v1/b?alt=json&prettyPrint=false&project=<your-project-id>\": oauth2: cannot fetch token: 400 Bad Request\\n│ Response: {\"error\":\"invalid_grant\",\"error_description\":\"Invalid JWT: Token must be a short-lived token (60 minutes) and in a reasonable timeframe. Check your iat and exp values in the JWT claim.\"}\\nIT happens because there may be time desync on your machine which affects computing JWT\\nTo fix this, run the command\\nsudo hwclock -s\\nwhich fixes your system time.\\nReference',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'Terraform - Error 400 Bad Request.  Invalid JWT Token  on WSL.'},\n",
       "   {'text': '│ Error: googleapi: Error 403: Access denied., forbidden\\nYour $GOOGLE_APPLICATION_CREDENTIALS might not be pointing to the correct file \\nrun = export GOOGLE_APPLICATION_CREDENTIALS=~/.gc/YOUR_JSON.json\\nAnd then = gcloud auth activate-service-account --key-file $GOOGLE_APPLICATION_CREDENTIALS',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'Terraform - Error 403 : Access denied'},\n",
       "   {'text': \"One service account is enough for all the services/resources you'll use in this course. After you get the file with your credentials and set your environment variable, you should be good to go.\",\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'Terraform - Do I need to make another service account for terraform before I get the keys (.json file)?'},\n",
       "   {'text': 'Here: https://releases.hashicorp.com/terraform/1.1.3/terraform_1.1.3_linux_amd64.zip',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'Terraform - Where can I find the Terraform 1.1.3 Linux (AMD 64)?'},\n",
       "   {'text': 'You get this error because I run the command terraform init outside the working directory, and this is wrong.You need first to navigate to the working directory that contains terraform configuration files, and and then run the command.',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'Terraform - Terraform initialized in an empty directory! The directory has no Terraform configuration files. You may begin working with Terraform immediately by creating Terraform configuration files.g'},\n",
       "   {'text': 'The error:\\nError: googleapi: Error 403: Access denied., forbidden\\n│\\nand\\n│ Error: Error creating Dataset: googleapi: Error 403: Request had insufficient authentication scopes.\\nFor this solution make sure to run:\\necho $GOOGLE_APPLICATION_CREDENTIALS\\necho $?\\nSolution:\\nYou have to set again the GOOGLE_APPLICATION_CREDENTIALS as Alexey did in the environment set-up video in week1:\\nexport GOOGLE_APPLICATION_CREDENTIALS=\"<path/to/your/service-account-authkeys>.json',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'Terraform - Error creating Dataset: googleapi: Error 403: Request had insufficient authentication scopes'},\n",
       "   {'text': \"The error:\\nError: googleapi: Error 403: terraform-trans-campus@trans-campus-410115.iam.gserviceaccount.com does not have storage.buckets.create access to the Google Cloud project. Permission 'storage.buckets.create' denied on resource (or it may not exist)., forbidden\\nThe solution:\\nYou have to declare the project name as your Project ID, and not your Project name, available on GCP console Dashboard.\",\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'Terraform - Error creating Bucket: googleapi: Error 403: Permission denied to access ‘storage.buckets.create’'},\n",
       "   {'text': 'provider \"google\" {\\nproject     = var.projectId\\ncredentials = file(\"${var.gcpkey}\")\\n#region      = var.region\\nzone = var.zone\\n}',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'To ensure the sensitivity of the credentials file, I had to spend lot of time to input that as a file.'},\n",
       "   {'text': 'For the HW1 I encountered this issue. The solution is\\nSELECT * FROM zones AS z WHERE z.\"Zone\" = \\'Astoria Zone\\';\\nI think columns which start with uppercase need to go between “Column”. I ran into a lot of issues like this and “ ” made it work out.\\nAddition to the above point, for me, there is no ‘Astoria Zone’, only ‘Astoria’ is existing in the dataset.\\nSELECT * FROM zones AS z WHERE z.\"Zone\" = \\'Astoria’;',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': \"SQL - SELECT * FROM zones_taxi WHERE Zone='Astoria Zone'; Error Column Zone doesn't exist\"},\n",
       "   {'text': 'It is inconvenient to use quotation marks all the time, so it is better to put the data to the database all in lowercase, so in Pandas after\\ndf = pd.read_csv(‘taxi+_zone_lookup.csv’)\\nAdd the row:\\ndf.columns = df.columns.str.lower()',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': \"SQL - SELECT Zone FROM taxi_zones Error Column Zone doesn't exist\"},\n",
       "   {'text': 'Solution (for mac users): os.system(f\"curl {url} --output {csv_name}\")',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'CURL - curl: (6) Could not resolve host: output.csv'},\n",
       "   {'text': 'To resolve this, ensure that your config file is in C/User/Username/.ssh/config',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'SSH Error: ssh: Could not resolve hostname linux: Name or service not known'},\n",
       "   {'text': 'If you use Anaconda (recommended for the course), it comes with pip, so the issues is probably that the anaconda’s Python is not on the PATH.\\nAdding it to the PATH is different for each operation system.\\nFor Linux and MacOS:\\nOpen a terminal.\\nFind the path to your Anaconda installation. This is typically `~/anaconda3` or `~/opt/anaconda3`.\\nAdd Anaconda to your PATH with the command: `export PATH=\"/path/to/anaconda3/bin:$PATH\"`.\\nTo make this change permanent, add the command to your `.bashrc` (Linux) or `.bash_profile` (MacOS) file.\\nOn Windows, python and pip are in different locations (python is in the anaconda root, and pip is in Scripts). With GitBash:\\nLocate your Anaconda installation. The default path is usually `C:\\\\Users\\\\[YourUsername]\\\\Anaconda3`.\\nDetermine the correct path format for Git Bash. Paths in Git Bash follow the Unix-style, so convert the Windows path to a Unix-style path. For example, `C:\\\\Users\\\\[YourUsername]\\\\Anaconda3` becomes `/c/Users/[YourUsername]/Anaconda3`.\\nAdd Anaconda to your PATH with the command: `export PATH=\"/c/Users/[YourUsername]/Anaconda3/:/c/Users/[YourUsername]/Anaconda3/Scripts/$PATH\"`.\\nTo make this change permanent, add the command to your `.bashrc` file in your home directory.\\nRefresh your environment with the command: `source ~/.bashrc`.\\nFor Windows (without Git Bash):\\nRight-click on \\'This PC\\' or \\'My Computer\\' and select \\'Properties\\'.\\nClick on \\'Advanced system settings\\'.\\nIn the System Properties window, click on \\'Environment Variables\\'.\\nIn the Environment Variables window, select the \\'Path\\' variable in the \\'System variables\\' section and click \\'Edit\\'.\\nIn the Edit Environment Variable window, click \\'New\\' and add the path to your Anaconda installation (typically `C:\\\\Users\\\\[YourUsername]\\\\Anaconda3` and C:\\\\Users\\\\[YourUsername]\\\\Anaconda3\\\\Scripts`).\\nClick \\'OK\\' in all windows to apply the changes.\\nAfter adding Anaconda to the PATH, you should be able to use `pip` from the command line. Remember to restart your terminal (or command prompt in Windows) to apply these changes.',\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': \"'pip' is not recognized as an internal or external command, operable program or batch file.\"},\n",
       "   {'text': \"Resolution: You need to stop the services which is using the port.\\nRun the following:\\n```\\nsudo kill -9 `sudo lsof -t -i:<port>`\\n```\\n<port> being 8080 in this case. This will free up the port for use.\\n~ Abhijit Chakraborty\\nError: error response from daemon: cannot stop container: 1afaf8f7d52277318b71eef8f7a7f238c777045e769dd832426219d6c4b8dfb4: permission denied\\nResolution: In my case, I had to stop docker and restart the service to get it running properly\\nUse the following command:\\n```\\nsudo systemctl restart docker.socket docker.service\\n```\\n~ Abhijit Chakraborty\\nError: cannot import module psycopg2\\nResolution: Run the following command in linux:\\n```\\nsudo apt-get install libpq-dev\\npip install psycopg2\\n```\\n~ Abhijit Chakraborty\\nError: docker build Error checking context: 'can't stat '<path-to-file>'\\nResolution: This happens due to insufficient permission for docker to access a certain file within the directory which hosts the Dockerfile.\\n1. You can create a .dockerignore file and add the directory/file which you want Dockerfile to ignore while build.\\n2. If the above does not work, then put the dockerfile and corresponding script, `\\t1.py` in our case to a subfolder. and run `docker build ...`\\nfrom inside the new folder.\\n~ Abhijit Chakraborty\",\n",
       "    'section': 'Module 1: Docker and Terraform',\n",
       "    'question': 'Error: error starting userland proxy: listen tcp4 0.0.0.0:8080: bind: address already in use'},\n",
       "   {'text': 'To get a pip-friendly requirements.txt file file from Anaconda use\\nconda install pip then `pip list –format=freeze > requirements.txt`.\\n`conda list -d > requirements.txt` will not work and `pip freeze > requirements.txt` may give odd pathing.',\n",
       "    'section': 'Module 2: Workflow Orchestration',\n",
       "    'question': 'Anaconda to PIP'},\n",
       "   {'text': 'Prefect: https://docs.google.com/document/d/1K_LJ9RhAORQk3z4Qf_tfGQCDbu8wUWzru62IUscgiGU/edit?usp=sharing\\nAirflow: https://docs.google.com/document/d/1-BwPAsyDH_mAsn8HH5z_eNYVyBMAtawJRjHHsjEKHyY/edit?usp=sharing',\n",
       "    'section': 'Module 2: Workflow Orchestration',\n",
       "    'question': 'Where are the FAQ questions from the previous cohorts for the orchestration module?'},\n",
       "   {'text': 'Issue : Docker containers exit instantly with code 132, upon docker compose up\\nMage documentation has it listing the cause as \"older architecture\" .\\nThis might be a hardware issue, so unless you have another computer, you can\\'t solve it without purchasing a new one, so the next best solution is a VM.\\nThis is from a student running on a VirtualBox VM, Ubuntu 22.04.3 LTS, Docker version 25.0.2. So not having the context on how the vbox was spin up with (CPU, RAM, network, etc), it’s really inconclusive at this time.',\n",
       "    'section': 'Module 2: Workflow Orchestration',\n",
       "    'question': 'Docker - 2.2.2 Configure Mage'},\n",
       "   {'text': 'This issue was occurring with Windows WSL 2\\nFor me this was because WSL 2 was not dedicating enough cpu cores to Docker.The load seems to take up at least one cpu core so I recommend dedicating at least two.\\nOpen Bash and run the following code:\\n$ cd ~\\n$ ls -la\\nLook for the .wsl config file:\\n-rw-r--r-- 1 ~1049089       31 Jan 25 12:54  .wslconfig\\nUsing a text editing tool of your choice edit or create your .wslconfig file:\\n$ nano .wslconfig\\nPaste the following into the new file/ edit the existing file in this format and save:\\n*** Note - for memory– this is the RAM on your machine you can dedicate to Docker, your situation may be different than mine ***\\n[wsl2]\\nprocessors=<Number of Processors - at least 2!> example: 4\\nmemory=<memory> example:4GB\\nExample:\\nOnce you do that run:\\n$ wsl --shutdown\\nThis shuts down WSL\\nThen Restart Docker Desktop - You should now be able to load the .csv.gz file without the error into a pandas dataframe',\n",
       "    'section': 'Module 2: Workflow Orchestration',\n",
       "    'question': 'WSL - 2.2.3 Mage - Unexpected Kernel Restarts; Kernel Running out of memory:'},\n",
       "   {'text': 'The issue and solution on the link:\\nhttps://datatalks-club.slack.com/archives/C01FABYF2RG/p1706817366764269?thread_ts=1706815324.993529&cid=C01FABYF2RG',\n",
       "    'section': 'Module 2: Workflow Orchestration',\n",
       "    'question': '2.2.3 Configuring Postgres'},\n",
       "   {'text': 'Check that the POSTGRES_PORT variable in the io_config.yml  file is set to port 5432, which is the default postgres port. The POSTGRES_PORT variable is the mage container port, not the host port. Hence, there’s no need to set the POSTGRES_PORT to 5431 just because you already have a conflicting postgres installation in your host machine.',\n",
       "    'section': 'Module 2: Workflow Orchestration',\n",
       "    'question': 'MAGE - 2.2.3 OperationalError: (psycopg2.OperationalError) connection to server at \"localhost\" (::1), port 5431 failed: Connection refused'},\n",
       "   {'text': 'You forgot to select ‘dev’ profile in the dropdown menu next to where you select ‘PostgreSQL’ in the connection drop down.',\n",
       "    'section': 'Module 2: Workflow Orchestration',\n",
       "    'question': 'MAGE - 2.2.4 executing SELECT 1; results in KeyError'},\n",
       "   {'text': 'If you are getting this error. Update your mage io_config.yaml file, and specify a timeout value set to 600 like this.\\nMake sure to save your changes.\\nMAGE - 2.2.4 Testing BigQuery connection using SQL 404 error:\\nNotFound: 404 Not found: Dataset ny-rides-diegogutierrez:None was not found in location northamerica-northeast1\\nIf you get this error even with all roles/permissions given to the service account check if you have ticked the box where it says “Use raw SQL”, just like the image below.',\n",
       "    'section': 'Module 2: Workflow Orchestration',\n",
       "    'question': \"MAGE -2.2.4 ConnectionError: ('Connection aborted.', TimeoutError('The write operation timed out'))\"},\n",
       "   {'text': 'Solution: https://stackoverflow.com/questions/48056381/google-client-invalid-jwt-token-must-be-a-short-lived-token',\n",
       "    'section': 'Module 2: Workflow Orchestration',\n",
       "    'question': \"Problem: RefreshError: ('invalid_grant: Invalid JWT: Token must be a short-lived token (60 minutes) and in a reasonable timeframe. Check your iat and exp values in the JWT claim.', {'error': 'invalid_grant', 'error_description': 'Invalid JWT: Token must be a short-lived token (60 minutes) and in a reasonable timeframe. Check your iat and exp values in the JWT claim.'})\"},\n",
       "   {'text': \"Origin of Solution (Mage Slack-Channel): https://mageai.slack.com/archives/C03HTTWFEKE/p1706543947795599\\nProblem: This error can often be seen after solving the error mentioned in 2.2.4. The error can be found in Mage version 0.9.61 and is a side-effect of the update of the code for data-loader blocks.\\nNote: Mage 0.9.62 has been released, as of Feb 5 2024. Please recheck. Solution below may be obsolete\\nSolution: Using a “fixed” version of the docker container\\nPull updated docker image from docker-hub\\nmageai/mageaidocker pull:alpha\\nUpdate docker-compose.yaml\\nversion: '3'\\nservices:\\nmagic:\\nimage: mageai/mageai:alpha  <--- instead of “latest”-tag\\ndocker-compose up\\nThe original Error is still present, but the SQL-query will return the desired result:\\n--------------------------------------------------------------------------------------\",\n",
       "    'section': 'Module 2: Workflow Orchestration',\n",
       "    'question': 'Mage - 2.2.4 IndexError: list index out of range'},\n",
       "   {'text': 'Add\\nif not path.parent.is_dir():\\npath.parent.mkdir(parents=True)\\npath = Path(path).as_posix()\\nsee:\\nhttps://datatalks-club.slack.com/archives/C01FABYF2RG/p1675774214591809?thread_ts=1675768839.028879&cid=C01FABYF2RG',\n",
       "    'section': 'Module 2: Workflow Orchestration',\n",
       "    'question': '2.2.6 OSError: Cannot save file into a non-existent directory: \\'..\\\\\\\\..\\\\\\\\data\\\\\\\\yellow\\'\\\\n\")'},\n",
       "   {'text': 'The video DE Zoomcamp 2.2.7 is missing  the actual deployment of Mage using Terraform to GCP. The steps for the deployment were not covered in the video.\\nI successfully deployed it and wanted to share some key points:\\nIn variables.tf, set the project_id default value to your GCP project ID.\\nEnable the Cloud Filestore API:\\nVisit the Google Cloud Console.to\\nNavigate to \"APIs & Services\" > \"Library.\"\\nSearch for \"Cloud Filestore API.\"\\nClick on the API and enable it.\\nTo perform the deployment:\\nterraform init\\nterraform apply\\nPlease note that during the terraform apply step, Terraform will prompt you to enter the PostgreSQL password. After that, it will ask for confirmation to proceed with the deployment. Review the changes, type \\'yes\\' when prompted, and press Enter.',\n",
       "    'section': 'Module 2: Workflow Orchestration',\n",
       "    'question': 'GCP - 2.2.7d Deploying Mage to GCP'},\n",
       "   {'text': 'If you want to rune multiple docker containers from different directories. Then make sure to change the port mappings in the docker-compose.yml file.\\nports:\\n- 8088:6789\\nThe 8088 port in above case is hostport, where mage will run on your local machine. You can customize this as long as the port is available. If you are running on VM, make sure to forward the port too. You need to keep the container port to 6789 as this is the port where mage is running.\\nGCP - 2.2.7d Deploying Mage to Google Cloud\\nWhile terraforming all the resources inside a VM created in GCS the following error is shown.\\nError log:\\nmodule.lb-http.google_compute_backend_service.default[\"default\"]: Creating...\\n╷\\n│ Error: Error creating GlobalAddress: googleapi: Error 403: Request had insufficient authentication scopes.\\n│ Details:\\n│ [\\n│   {\\n│     \"@type\": \"type.googleapis.com/google.rpc.ErrorInfo\",\\n│     \"domain\": \"googleapis.com\",\\n│     \"metadatas\": {\\n│       \"method\": \"compute.beta.GlobalAddressesService.Insert\",\\n│       \"service\": \"compute.googleapis.com\"\\n│     },\\n│     \"reason\": \"ACCESS_TOKEN_SCOPE_INSUFFICIENT\"\\n│   }\\n│ ]\\n│\\n│ More details:\\n│ Reason: insufficientPermissions, Message: Insufficient Permission\\nThis error might happen when you are using a VM inside GCS. To use the Google APIs from a GCP virtual machine you need to add the cloud platform scope (\"https://www.googleapis.com/auth/cloud-platform\") to your VM when it is created.\\nSince ours is already created you can just stop it and change the permissions. You can do it in the console, just go to \"EDIT\", g99o all the way down until you find \"Cloud API access scopes\". There you can \"Allow full access to all Cloud APIs\". I did this and all went smoothly generating all the resources needed. Hope it helps if you encounter this same error.\\nResources: https://stackoverflow.com/questions/35928534/403-request-had-insufficient-authentication-scopes-during-gcloud-container-clu',\n",
       "    'section': 'Module 2: Workflow Orchestration',\n",
       "    'question': 'Ruuning Multiple Mage instances in Docker from different directories'},\n",
       "   {'text': 'If you are on the free trial account on GCP you will face this issue when trying to deploy the infrastructures with terraform. This service is not available for this kind of account.\\nThe solution I found was to delete the load_balancer.tf file and to comment or delete the rows that differentiate it on the main.tf file. After this just do terraform destroy to delete any infrastructure created on the fail attempts and re-run the terraform apply.\\nCode on main.tf to comment/delete:\\nLine 166, 167, 168',\n",
       "    'section': 'Module 2: Workflow Orchestration',\n",
       "    'question': 'GCP - 2.2.7d Load Balancer Problem (Security Policies quota)'},\n",
       "   {'text': \"If you get the following error\\nYou have to edit variables.tf on the gcp folder, set your project-id and region and zones properly. Then, run terraform apply again.\\nYou can find correct regions/zones here: https://cloud.google.com/compute/docs/regions-zones\\nDeploying MAGE to GCP  with Terraform via the VM (2.2.7)\\nFYI - It can take up to 20 minutes to deploy the MAGE Terraform files if you are using a GCP Virtual Machine. It is normal, so don’t interrupt the process or think it’s taking too long. If you have, make sure you run a terraform destroy before trying again as you will have likely partially created resources which will cause errors next time you run `terraform apply`.\\n`terraform destroy` may not completely delete partial resources - go to Google Cloud Console and use the search bar at the top to search for the ‘app.name’ you declared in your variables.tf file; this will list all resources with that name - make sure you delete them all before running `terraform apply` again.\\nWhy are my GCP free credits going so fast? MAGE .tf files - Terraform Destroy not destroying all Resources\\nI checked my GCP billing last night & the MAGE Terraform IaC didn't destroy a GCP Resource called Filestore as ‘mage-data-prep- it has been costing £5.01 of my free credits each day  I now have £151 left - Alexey has assured me that This amount WILL BE SUFFICIENT funds to finish the course. Note to anyone who had issues deploying the MAGE terraform code: check your billing account to see what you're being charged for (main menu - billing) (even if it's your free credits) and run a search for 'mage-data-prep' in the top bar just to be sure that your resources have been destroyed - if any come up delete them.\",\n",
       "    'section': 'Module 2: Workflow Orchestration',\n",
       "    'question': 'GCP - 2.2.7d Part 2 - Getting error when you run terraform apply'},\n",
       "   {'text': '```\\n│ Error: Error creating Connector: googleapi: Error 403: Permission \\'vpcaccess.connectors.create\\' denied on resource \\'//vpcaccess.googleapis.com/projects/<ommit>/locations/us-west1\\' (or it may not exist).\\n│ Details:\\n│ [\\n│   {\\n│     \"@type\": \"type.googleapis.com/google.rpc.ErrorInfo\",\\n│     \"domain\": \"vpcaccess.googleapis.com\",\\n│     \"metadata\": {\\n│       \"permission\": \"vpcaccess.connectors.create\",\\n│       \"resource\": \"projects/<ommit>/locations/us-west1\"\\n│     },\\n│     \"reason\": \"IAM_PERMISSION_DENIED\"\\n│   }\\n│ ]\\n│\\n│   with google_vpc_access_connector.connector,\\n│   on fs.tf line 19, in resource \"google_vpc_access_connector\" \"connector\":\\n│   19: resource \"google_vpc_access_connector\" \"connector\" {\\n│\\n```\\nSolution: Add Serverless VPC Access Admin to Service Account.\\nLine 148',\n",
       "    'section': 'Module 2: Workflow Orchestration',\n",
       "    'question': \"Question: Permission 'vpcaccess.connectors.create'\"},\n",
       "   {'text': 'Git won’t push an empty folder to GitHub, so if you put a file in that folder and then push, then you should be good to go.\\nOr - in your code- make the folder if it doesn’t exist using Pathlib as shown here: https://stackoverflow.com/a/273227/4590385.\\nFor some reason, when using github storage, the relative path for writing locally no longer works. Try using two separate paths, one full path for the local write, and the original relative path for GCS bucket upload.',\n",
       "    'section': 'Module 2: Workflow Orchestration',\n",
       "    'question': \"File Path: Cannot save file into a non-existent directory: 'data/green'\"},\n",
       "   {'text': 'The green dataset contains lpep_pickup_datetime while the yellow contains tpep_pickup_datetime. Modify the script(s) depending on  the dataset as required.',\n",
       "    'section': 'Module 2: Workflow Orchestration',\n",
       "    'question': 'No column name lpep_pickup_datetime / tpep_pickup_datetime'},\n",
       "   {'text': 'pd.read_csv\\ndf_iter = pd.read_csv(dataset_url, iterator=True, chunksize=100000)\\nThe data needs to be appended to the parquet file using the fastparquet engine\\ndf.to_parquet(path, compression=\"gzip\", engine=\\'fastparquet\\', append=True)',\n",
       "    'section': 'Module 2: Workflow Orchestration',\n",
       "    'question': 'Process to download the VSC using Pandas is killed right away'},\n",
       "   {'text': 'denied: requested access to the resource is denied\\nThis can happen when you\\nHaven\\'t logged in properly to Docker Desktop (use docker login -u \"myusername\")\\nHave used the wrong username when pushing to docker images. Use the same one as your username and as the one you build on\\ndocker image build -t <myusername>/<imagename>:<tag>\\ndocker image push <myusername>/<imagename>:<tag>',\n",
       "    'section': 'Module 2: Workflow Orchestration',\n",
       "    'question': 'Push to docker image failure'},\n",
       "   {'text': \"16:21:35.607 | INFO    | Flow run 'singing-malkoha' - Executing 'write_bq-b366772c-0' immediately...\\nKilled\\nSolution:  You probably are running out of memory on your VM and need to add more.  For example, if you have 8 gigs of RAM on your VM, you may want to expand that to 16 gigs.\",\n",
       "    'section': 'Module 2: Workflow Orchestration',\n",
       "    'question': 'Flow script fails with “killed” message:'},\n",
       "   {'text': 'After playing around with prefect for a while this can happen.\\nSsh to your VM and run sudo du -h --block-size=G | sort -n -r | head -n 30 to see which directory needs the most space.\\nMost likely it will be …/.prefect/storage, where your cached flows are stored. You can delete older flows from there. You also have to delete the corresponding flow in the UI, otherwise it will throw you an error, when you try to run your next flow.\\nSSL Certificate Verify: (I got it when trying to run flows on MAC): urllib.error.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED]\\npip install certifi\\n/Applications/Python\\\\ {ver}/Install\\\\ Certificates.command\\nor\\nrunning the “Install Certificate.command” inside of the python{ver} folder',\n",
       "    'section': 'Module 2: Workflow Orchestration',\n",
       "    'question': 'GCP VM: Disk Space is full'},\n",
       "   {'text': 'It means your container consumed all available RAM allocated to it. It can happen in particular when working on Question#3 in the homework as the dataset is relatively large and containers eat a lot of memory in general.\\nI would recommend restarting your computer and only starting the necessary processes to run the container. If that doesn’t work, allocate more resources to docker. If also that doesn’t work because your workstation is a potato, you can use an online compute environment service like GitPod, which is free under under 50 hours / month of use.',\n",
       "    'section': 'Module 2: Workflow Orchestration',\n",
       "    'question': 'Docker: container crashed with status code 137.'},\n",
       "   {'text': 'In Q3 there was a task to run the etl script from web to GCS. The problem was, it wasn’t really an ETL straight from web to GCS, but it was actually a web to local storage to local memory to GCS over network ETL. Yellow data is about 100 MB each per month compressed and ~700 MB after uncompressed on memory\\nThis leads to a problem where i either got a network type error because my not so good 3rd world internet or i got my WSL2 crashed/hanged because out of memory error and/or 100% resource usage hang.\\nSolution:\\nif you have a lot of time at hand, try compressing it to parquet and writing it to GCS with the timeout argument set to a really high number (the default os 60 seconds)\\nthe yellow taxi data for feb 2019 is about 100MB as parquet file\\ngcp_cloud_storage_bucket_block.upload_from_path(\\nfrom_path=f\"{path}\",\\nto_path=path,\\ntimeout=600\\n)',\n",
       "    'section': 'Module 2: Workflow Orchestration',\n",
       "    'question': 'Timeout due to slow upload internet'},\n",
       "   {'text': 'This error occurs when you try to re-run the export block, of the transformed green_taxi data to PostgreSQL.\\nWhat you’ll need to do is to drop the table using SQL in Mage (screenshot below).\\nYou should be able to re-run the block successfully after dropping the table.',\n",
       "    'section': 'Module 2: Workflow Orchestration',\n",
       "    'question': 'UndefinedColumn: column \"ratecode_id\", \"rate_code_id\" “vendor_id”, “pu_location_id”, “do_location_id” of relation \"green_taxi\" does not exist - Export transformed green_taxi data to PostgreSQL'},\n",
       "   {'text': 'SettingWithCopyWarning:\\nA value is trying to be set on a copy of a slice from a DataFrame.\\nUse the data.loc[] = value syntax instead of df[] = value to ensure that the new column is being assigned to the original dataframe instead of a copy of a dataframe or a series.',\n",
       "    'section': 'Module 2: Workflow Orchestration',\n",
       "    'question': 'Homework - Q3 SettingWithCopyWarning Error:'},\n",
       "   {'text': 'CSV Files are very big in nyc data, so we instead of using Pandas/Python kernel , we can try Pyspark Kernel\\nDocumentation of Mage for using pyspark kernel: https://docs.mage.ai/integrations/spark-pyspark\\n?',\n",
       "    'section': 'Module 2: Workflow Orchestration',\n",
       "    'question': 'Since I was using slow laptop, and we have so big csv files, I used pyspark kernel in mage instead of python, How to do it?'},\n",
       "   {'text': 'So we will first delete the connection between blocks then we can remove the connection.',\n",
       "    'section': 'Module 2: Workflow Orchestration',\n",
       "    'question': 'I got an error when I was deleting  BLOCK IN A PIPELINE'},\n",
       "   {'text': 'While Editing the Pipeline Name It throws permission denied error.\\n(Work around)In that case proceed with the work and save later on revisit it will let you edit.',\n",
       "    'section': 'Module 2: Workflow Orchestration',\n",
       "    'question': 'Mage UI won’t let you edit the Pipeline name?'},\n",
       "   {'text': 'Solution n°1 if you want to download everything :\\n```\\nimport pyarrow as pa\\nimport pyarrow.parquet as pq\\nfrom pyarrow.fs import GcsFileSystem\\n…\\n@data_loader\\ndef load_data(*args, **kwargs):\\n    bucket_name = YOUR_BUCKET_NAME_HERE\\'\\n    blob_prefix = \\'PATH / TO / WHERE / THE / PARTITIONS / ARE\\'\\n    root_path = f\"{bucket_name}/{blob_prefix}\"\\npa_table = pq.read_table(\\n        source=root_path,\\n        filesystem=GcsFileSystem(),        \\n    )\\n\\n    return pa_table.to_pandas()\\nSolution n°2 if you want to download only some dates :\\n@data_loader\\ndef load_data(*args, **kwargs):\\ngcs = pa.fs.GcsFileSystem()\\nbucket_name = \\'YOUR_BUCKET_NAME_HERE\\'\\nblob_prefix = \\'\\'PATH / TO / WHERE / THE / PARTITIONS / ARE\\'\\'\\nroot_path = f\"{bucket_name}/{blob_prefix}\"\\npa_dataset = pq.ParquetDataset(\\npath_or_paths=root_path,\\nfilesystem=gcs,\\nfilters=[(\\'lpep_pickup_date\\', \\'>=\\', \\'2020-10-01\\'), (\\'lpep_pickup_date\\', \\'<=\\', \\'2020-10-31\\')]\\n)\\nreturn pa_dataset.read().to_pandas()\\n# More information about the pq.Parquet.Dataset : Encapsulates details of reading a complete Parquet dataset possibly consisting of multiple files and partitions in subdirectories. Documentation here :\\nhttps://arrow.apache.org/docs/python/generated/pyarrow.parquet.ParquetDataset.html#pyarrow.parquet.ParquetDataset\\nERROR: UndefinedColumn: column \"vendor_id\" of relation \"green_taxi\" does not exist\\nTwo possible solutions both of them work in the same way.\\nOpen up a Data Loader connect using SQL - RUN the command \\n`DROP TABLE mage.green_taxi`\\nElse, Open up a Data Extractor of SQL  - increase the rows to above the number of rows in the dataframe (you can find that in the bottom of the transformer block) change the Write Policy to `Replace` and run the SELECT statement',\n",
       "    'section': 'Module 2: Workflow Orchestration',\n",
       "    'question': 'How do I make Mage load the partitioned files that we created on 2.2.4, to load them into BigQuery ?'},\n",
       "   {'text': \"All mage files are in your /home/src/folder where you saved your credentials.json so you should be able to access them locally. You will see a folder for ‘Pipelines’,  'data loaders', 'data transformers' & 'data exporters' - inside these will be the .py or .sql files for the blocks you created in your pipeline.\\nRight click & ‘download’ the pipeline itself to your local machine (which gives you metadata, pycache and other files)\\nAs above, download each .py/.sql file that corresponds to each block you created for the pipeline. You'll find these under 'data loaders', 'data transformers' 'data exporters'\\nMove the downloaded files to your GitHub repo folder & commit your changes.\",\n",
       "    'section': 'Module 2: Workflow Orchestration',\n",
       "    'question': 'Git - What Files Should I Submit for Homework 2 & How do I get them out of MAGE:'},\n",
       "   {'text': 'Assuming you downloaded the Mage repo in the week 2 folder of the Data Engineering Zoomcamp, you might want to include your mage copy, demo pipelines and homework within your personal copy of the Data Engineering Zoomcamp repo. This will not work by default, because GitHub sees them as two separate repositories, and one does not track the other. To add the Mage files to your main DE Zoomcamp repo, you will need to:\\nMove the contents of the .gitignore file in your main .gitignore.\\nUse the terminal to cd into the Mage folder and:\\nrun “git remote remove origin” to de-couple the Mage repo,\\nrun “rm -rf .git” to delete local git files,\\nrun “git add .” to add the current folder as changes to stage, commit and push.',\n",
       "    'section': 'Module 2: Workflow Orchestration',\n",
       "    'question': 'Git - How do I include the files in the Mage repo (including exercise files and homework) in a personal copy of the Data Engineering Zoomcamp repo?'},\n",
       "   {'text': \"When try to add three assertions:\\nvendor_id is one of the existing values in the column (currently)\\npassenger_count is greater than 0\\ntrip_distance is greater than 0\\nto test_output, I got ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all(). Below is my code:\\ndata_filter = (data['passenger_count'] > 0) and (data['trip_distance'] > 0)\\nAfter looking for solutions at Stackoverflow, I found great discussion about it. So I changed my code into:\\ndata_filter = (data['passenger_count'] > 0) & (data['trip_distance'] > 0)\",\n",
       "    'section': 'Module 2: Workflow Orchestration',\n",
       "    'question': 'Got ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()'},\n",
       "   {'text': 'This happened when I just booted up my PC, continuing from the progress I was doing from yesterday.\\nAfter cd-ing into your directory, and running docker compose up , the web interface for the Mage shows, but the files that I had yesterday was gone.\\nIf your files are gone, go ahead and close the web interface, and properly shutting down the mage docker compose by doing Ctrl + C once. Try running it again. This worked for me more than once (yes the issue persisted with my PC twice)\\nAlso, you should check if you’re in the correct repository before doing docker compose up . This was discussed in the Slack #course-data-engineering channel',\n",
       "    'section': 'Module 2: Workflow Orchestration',\n",
       "    'question': 'Mage AI Files are Gone/disappearing'},\n",
       "   {'text': 'The above errors due to “ at the trailing side and it need to be modified with ‘ quotes at both ends\\nKrishna Anand',\n",
       "    'section': 'Module 2: Workflow Orchestration',\n",
       "    'question': 'Mage - Errors in io.config.yaml file'},\n",
       "   {'text': \"Problem: The following error occurs when attempting to export data from Mage to a GCS bucket using pyarrow suggesting Mage doesn’t have the necessary permissions to access the specified GCP credentials .json file.\\nArrowException: Unknown error: google::cloud::Status(UNKNOWN: Permanent error GetBucketMetadata: Could not create a OAuth2 access token to authenticate the request. The request was not sent, as such an access token is required to complete the request successfully. Learn more about Google Cloud authentication at https://cloud.google.com/docs/authentication. The underlying error message was: Cannot open credentials file /home/src/...\\nSolution: Inside the Mage app:\\nCreate a credentials folder (e.g. gcp-creds) within the magic-zoomcamp folder\\nIn the credentials folder create a .json key file (e.g. mage-gcp-creds.json)\\nCopy/paste GCP service account credentials into the .json key file and save\\nUpdate code to point to this file. E.g.\\nenviron['GOOGLE_APPLICATION_CREDENTIALS'] = '/home/src/magic-zoomcamp/gcp-creds/mage-gcp-creds.json'\",\n",
       "    'section': 'Module 2: Workflow Orchestration',\n",
       "    'question': 'Mage - ArrowException Cannot open credentials file'},\n",
       "   {'text': \"Oserror: google::cloud::status(unavailable: retry policy exhausted getbucketmetadata: could not create a OAuth2 access token to authenticate the request. the request was not sent, as such an access token is required to complete the request successfully. learn more about google cloud authentication at https://cloud.google.com/docs/authentication. the underlying error message was: performwork() - curl error [6]=couldn't resolve host name)\",\n",
       "    'section': 'Module 2: Workflow Orchestration',\n",
       "    'question': 'Mage - OSError'},\n",
       "   {'text': \"Problem: The following error occurs when attempting to export data from Mage to a GCS bucket. Assigned service account doesn’t have the necessary permissions access Google Cloud Storage Bucket\\nPermissionError: [Errno 13] google::cloud::Status(PERMISSION_DENIED: Permanent error GetBucketMetadata:... .iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket. Permission 'storage.buckets.get' denied on resource (or it may not exist). error_info={reason=forbidden, domain=global, metadata={http_status_code=403}}). Detail: [errno 13] Permission denied\\nSolution: Add Cloud Storage Admin role to the service account:\\nGo to project in Google Cloud Console>IAM & Admin>IAM\\nClick Edit principal (pencil symbol) to the right of the service account you are using\\nClick + ADD ANOTHER ROLE\\nSelect Cloud Storage>Storage Admin\\nClick Save\",\n",
       "    'section': 'Module 2: Workflow Orchestration',\n",
       "    'question': 'Mage - PermissionError service account does not have storage.buckets.get access to the Google Cloud Storage bucket'},\n",
       "   {'text': '1. Make sure your pyspark script is ready to be send to Dataproc cluster\\n2. Create a Dataproc Cluster in GCP Console\\n3. Make sure to edit the service account and add new role - Dataproc Editor\\n4. Copy the python script ./notebooks/pyspark_script.py and place it under GCS bucket path\\n5. Make sure gcloud cli is installed either in Mage manually or  via your Dockerfile and docker-compose files. This is needed to let Mage access google Dataproc and the script it needs to execute. Refer - Installing the latest gcloud CLI\\n6. Use the Bigquery/Dataproc script mentioned here - https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/05-batch/code/cloud.md . Use Mage to trigger the query',\n",
       "    'section': 'Module 3: Data Warehousing',\n",
       "    'question': 'Trigger Dataproc from Mage'},\n",
       "   {'text': 'A:\\n1 solution) Add -Y flag, so that apt-get automatically agrees to install additional packages\\n2) Use python ZipFile package, which is included in all modern python distributions',\n",
       "    'section': 'Module 3: Data Warehousing',\n",
       "    'question': 'Docker-compose takes infinitely long to install zip unzip packages for linux, which are required to unpack datasets'},\n",
       "   {'text': 'Make sure to use Nullable dataTypes, such as Int64 when appliable.',\n",
       "    'section': 'Module 3: Data Warehousing',\n",
       "    'question': 'GCS Bucket - error when writing data from web to GCS:'},\n",
       "   {'text': 'Ultimately, when trying to ingest data into a BigQuery table, all files within a given directory must have the same schema.\\nWhen dealing for example with the FHV Datasets from 2019, however (see image below), one can see that the files for \\'2019-05\\', and 2019-06, have the columns \"PUlocationID\" and \"DOlocationID\" as Integers, while for the period of \\'2019-01\\' through \\'2019-04\\', the same column is defined as FLOAT.\\nSo while importing these files as parquet to BigQuery, the first one will be used to define the schema of the table, while all files following that will be used to append data on the existing table. Which means, they must all follow the very same schema of the file that created the table.\\nSo, in order to prevent errors like that, make sure to enforce the data types for the columns on the DataFrame before you serialize/upload them to BigQuery. Like this:\\npd.read_csv(\"path_or_url\").astype({\\n\\t\"col1_name\": \"datatype\",\\t\\n\\t\"col2_name\": \"datatype\",\\t\\n\\t...\\t\\t\\t\\t\\t\\n\\t\"colN_name\": \"datatype\" \\t\\n})',\n",
       "    'section': 'Module 3: Data Warehousing',\n",
       "    'question': \"GCS Bucket - Failed to create table: Error while reading data, error message: Parquet column 'XYZ' has type INT which does not match the target cpp_type DOUBLE. File: gs://path/to/some/blob.parquet\"},\n",
       "   {'text': \"If you receive the error gzip.BadGzipFile: Not a gzipped file (b'\\\\n\\\\n'), this is because you have specified the wrong URL to the FHV dataset. Make sure to use https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/{dataset_file}.csv.gz\\nEmphasising the ‘/releases/download’ part of the URL.\",\n",
       "    'section': 'Module 3: Data Warehousing',\n",
       "    'question': 'GCS Bucket - Fix Error when importing FHV data to GCS'},\n",
       "   {'text': 'Krishna Anand',\n",
       "    'section': 'Module 3: Data Warehousing',\n",
       "    'question': 'GCS Bucket - Load Data From URL list in to GCP Bucket'},\n",
       "   {'text': 'Check the Schema\\nYou might have a wrong formatting\\nTry to upload the CSV.GZ files without formatting or going through pandas via wget\\nSee this Slack conversation for helpful tips',\n",
       "    'section': 'Module 3: Data Warehousing',\n",
       "    'question': 'GCS Bucket - I query my dataset and get a Bad character (ASCII 0) error?'},\n",
       "   {'text': 'Run the following command to check if “BigQuery Command Line Tool” is installed or not: gcloud components list\\nYou can also use bq.cmd instead of bq to make it work.',\n",
       "    'section': 'Module 3: Data Warehousing',\n",
       "    'question': 'GCP BQ - “bq: command not found”'},\n",
       "   {'text': 'Use big queries carefully,\\nI created by bigquery dataset on an account where my free trial was exhausted, and got a bill of $80.\\nUse big query in free credits and destroy all the datasets after creation.\\nCheck your Billing daily! Especially if you’ve spinned up a VM.',\n",
       "    'section': 'Module 3: Data Warehousing',\n",
       "    'question': 'GCP BQ - Caution in using bigquery:no'},\n",
       "   {'text': 'Be careful when you create your resources on GCP, all of them have to share the same Region in order to allow load data from GCS Bucket to BigQuery. If you forgot it when you created them, you can create a new dataset on BigQuery using the same Region which you used on your GCS Bucket.\\nThis means that your GCS Bucket and the BigQuery dataset are placed in different regions. You have to create a new dataset inside BigQuery in the same region with your GCS bucket and store the data in the newly created dataset.',\n",
       "    'section': 'Module 3: Data Warehousing',\n",
       "    'question': 'GCP BQ - Cannot read and write in different locations: source: EU, destination: US - Loading data from GCS into BigQuery (different Region):'},\n",
       "   {'text': \"Make sure to create the BigQuery dataset in the very same location that you've created the GCS Bucket. For instance, if your GCS Bucket was created in `us-central1`, then BigQuery dataset must be created in the same region (us-central1, in this example)\",\n",
       "    'section': 'Module 3: Data Warehousing',\n",
       "    'question': 'GCP BQ - Cannot read and write in different locations: source: <REGION_HERE>, destination: <ANOTHER_REGION_HERE>'},\n",
       "   {'text': 'By the way, this isn’t a problem/solution, but a useful hint:\\nPlease, remember to save your progress in BigQuery SQL Editor.\\nI was almost finishing the homework, when my Chrome Tab froze and I had to reload it. Then I lost my entire SQL script.\\nSave your script from time to time. Just click on the button at the top bar. Your saved file will be available on the left panel.\\nAlternatively, you can copy paste your queries into an .sql file in your preferred editor (Notepad++, VS Code, etc.). Using the .sql extension will provide convenient color formatting.',\n",
       "    'section': 'Module 3: Data Warehousing',\n",
       "    'question': 'GCP BQ - Remember to save your queries'},\n",
       "   {'text': 'Ans :  While real-time analytics might not be explicitly mentioned, BigQuery has real-time data streaming capabilities, allowing for potential integration in future project iterations.',\n",
       "    'section': 'Module 3: Data Warehousing',\n",
       "    'question': 'GCP BQ - Can I use BigQuery for real-time analytics in this project?'},\n",
       "   {'text': \"could not parse 'pickup_datetime' as timestamp for field pickup_datetime (position 2)\\nThis error is caused by invalid data in the timestamp column. A way to identify the problem is to define the schema from the external table using string datatype. This enables the queries to work at which point we can filter out the invalid rows from the import to the materialised table and insert the fields with the timestamp data type.\",\n",
       "    'section': 'Module 3: Data Warehousing',\n",
       "    'question': 'GCP BQ - Unable to load data from external tables into a materialized table in BigQuery due to an invalid timestamp error that are added while appending data to the file in Google Cloud Storage'},\n",
       "   {'text': 'Background:\\n`pd.read_parquet`\\n`pd.to_datetime`\\n`pq.write_to_dataset`\\nReference:\\nhttps://stackoverflow.com/questions/48314880/are-parquet-file-created-with-pyarrow-vs-pyspark-compatible\\nhttps://stackoverflow.com/questions/57798479/editing-parquet-files-with-python-causes-errors-to-datetime-format\\nhttps://www.reddit.com/r/bigquery/comments/16aoq0u/parquet_timestamp_to_bq_coming_across_as_int/?share_id=YXqCs5Jl6hQcw-kg6-VgF&utm_content=1&utm_medium=ios_app&utm_name=ioscss&utm_source=share&utm_term=1\\nSolution:\\nAdd `use_deprecated_int96_timestamps=True` to `pq.write_to_dataset` function, like below\\npq.write_to_dataset(\\ntable,\\nroot_path=root_path,\\nfilesystem=gcs,\\nuse_deprecated_int96_timestamps=True\\n# Write timestamps to INT96 Parquet format\\n)',\n",
       "    'section': 'Module 3: Data Warehousing',\n",
       "    'question': 'GCP BQ - Error Message in BigQuery: annotated as a valid Timestamp, please annotate it as TimestampType(MICROS) or TimestampType(MILLIS)'},\n",
       "   {'text': 'Solution:\\nIf you’re using Mage, in the last Data Exporter that writes to Google Cloud Storage use PyArrow to generate the Parquet file with the correct logical type for the datetime columns, otherwise they won\\'t be converted to timestamp when loaded by BigQuery later on.\\nimport pyarrow as pa\\nimport pyarrow.parquet as pq\\nimport os\\nif \\'data_exporter\\' not in globals():\\nfrom mage_ai.data_preparation.decorators import data_exporter\\n# Replace with the location of your service account key JSON file.\\nos.environ[\\'GOOGLE_APPLICATION_CREDENTIALS\\'] = \\'/home/src/personal-gcp.json\\'\\nbucket_name = \"<YOUR_BUCKET_NAME>\"\\nobject_key = \\'nyc_taxi_data_2022.parquet\\'\\nwhere = f\\'{bucket_name}/{object_key}\\'\\n@data_exporter\\ndef export_data(data, *args, **kwargs):\\ntable = pa.Table.from_pandas(data, preserve_index=False)\\ngcs = pa.fs.GcsFileSystem()\\npq.write_table(\\ntable,\\nwhere,\\n# Convert integer columns in Epoch milliseconds\\n# to Timestamp columns in microseconds (\\'us\\') so\\n# they can be loaded into BigQuery with the right\\n# data type\\ncoerce_timestamps=\\'us\\',\\nfilesystem=gcs\\n)\\nSolution 2:\\nIf you’re using Mage, in the last Data Exporter that writes to Google Cloud Storage, provide PyArrow with explicit schema to generate the Parquet file with the correct logical type for the datetime columns, otherwise they won\\'t be converted to timestamp when loaded by BigQuery later on.\\nschema = pa.schema([\\n(\\'vendor_id\\', pa.int64()),\\n(\\'lpep_pickup_datetime\\', pa.timestamp(\\'ns\\')),\\n(\\'lpep_dropoff_datetime\\', pa.timestamp(\\'ns\\')),\\n(\\'store_and_fwd_flag\\', pa.string()),\\n(\\'ratecode_id\\', pa.int64()),\\n(\\'pu_location_id\\', pa.int64()),\\n(\\'do_location_id\\', pa.int64()),\\n(\\'passenger_count\\', pa.int64()),\\n(\\'trip_distance\\', pa.float64()),\\n(\\'fare_amount\\', pa.float64()),\\n(\\'extra\\', pa.float64()),\\n(\\'mta_tax\\', pa.float64()),\\n(\\'tip_amount\\', pa.float64()),\\n(\\'tolls_amount\\', pa.float64()),\\n(\\'improvement_surcharge\\', pa.float64()),\\n(\\'total_amount\\', pa.float64()),\\n(\\'payment_type\\', pa.int64()),\\n(\\'trip_type\\', pa.int64()),\\n(\\'congestion_surcharge\\', pa.float64()),\\n(\\'lpep_pickup_month\\', pa.int64())\\n])\\ntable = pa.Table.from_pandas(data, schema=schema)',\n",
       "    'section': 'Module 3: Data Warehousing',\n",
       "    'question': 'GCP BQ - Datetime columns in Parquet files created from Pandas show up as integer columns in BigQuery'},\n",
       "   {'text': 'Reference:\\nhttps://cloud.google.com/bigquery/docs/external-data-cloud-storage\\nSolution:\\nfrom google.cloud import bigquery\\n# Set table_id to the ID of the table to create\\ntable_id = f\"{project_id}.{dataset_name}.{table_name}\"\\n# Construct a BigQuery client object\\nclient = bigquery.Client()\\n# Set the external source format of your table\\nexternal_source_format = \"PARQUET\"\\n# Set the source_uris to point to your data in Google Cloud\\nsource_uris = [ f\\'gs://{bucket_name}/{object_key}/*\\']\\n# Create ExternalConfig object with external source format\\nexternal_config = bigquery.ExternalConfig(external_source_format)\\n# Set source_uris that point to your data in Google Cloud\\nexternal_config.source_uris = source_uris\\nexternal_config.autodetect = True\\ntable = bigquery.Table(table_id)\\n# Set the external data configuration of the table\\ntable.external_data_configuration = external_config\\ntable = client.create_table(table)  # Make an API request.\\nprint(f\\'Created table with external source: {table_id}\\')\\nprint(f\\'Format: {table.external_data_configuration.source_format}\\')',\n",
       "    'section': 'Module 3: Data Warehousing',\n",
       "    'question': 'GCP BQ - Create External Table using Python'},\n",
       "   {'text': 'Reference:\\nhttps://stackoverflow.com/questions/60941726/can-bigquery-api-overwrite-existing-table-view-with-create-table-tables-inser\\nSolution:\\nCombine with “Create External Table using Python”, use it before “client.create_table” function.\\ndef tableExists(tableID, client):\\n\"\"\"\\nCheck if a table already exists using the tableID.\\nreturn : (Boolean)\\n\"\"\"\\ntry:\\ntable = client.get_table(tableID)\\nreturn True\\nexcept Exception as e: # NotFound:\\nreturn False',\n",
       "    'section': 'Module 3: Data Warehousing',\n",
       "    'question': 'GCP BQ - Check BigQuery Table Exist And Delete'},\n",
       "   {'text': 'To avoid this error you can upload data from Google Cloud Storage to BigQuery through BigQuery Cloud Shell using the command:\\n$ bq load  --autodetect --allow_quoted_newlines --source_format=CSV dataset_name.table_name \"gs://dtc-data-lake-bucketname/fhv/fhv_tripdata_2019-*.csv.gz\"',\n",
       "    'section': 'Module 3: Data Warehousing',\n",
       "    'question': 'GCP BQ - Error: Missing close double quote (\") character'},\n",
       "   {'text': 'Solution: This problem arises if your gcs and bigquery storage is in different regions.\\nOne potential way to solve it:\\nGo to your google cloud bucket and check the region in field named “Location”\\nNow in bigquery, click on three dot icon near your project name and select create dataset.\\nIn region filed choose the same regions as you saw in your google cloud bucket',\n",
       "    'section': 'Module 3: Data Warehousing',\n",
       "    'question': 'GCP BQ - Cannot read and write in different locations: source: asia-south2, destination: US'},\n",
       "   {'text': 'There are multiple benefits of using Cloud Functions to automate tasks in Google Cloud.\\nUse below Cloud Function python script to load files directly to BigQuery. Use your project id, dataset id & table id as defined by you.\\nimport tempfile\\nimport requests\\nimport logging\\nfrom google.cloud import bigquery\\ndef hello_world(request):\\n# table_id = <project_id.dataset_id.table_id>\\ntable_id = \\'de-zoomcap-project.dezoomcamp.fhv-2019\\'\\n# Create a new BigQuery client\\nclient = bigquery.Client()\\nfor month in range(4, 13):\\n# Define the schema for the data in the CSV.gz files\\nurl = \\'https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-{:02d}.csv.gz\\'.format(month)\\n# Download the CSV.gz file from Github\\nresponse = requests.get(url)\\n# Create new table if loading first month data else append\\nwrite_disposition_string = \"WRITE_APPEND\" if month > 1 else \"WRITE_TRUNCATE\"\\n# Defining LoadJobConfig with schema of table to prevent it from changing with every table\\njob_config = bigquery.LoadJobConfig(\\nschema=[\\nbigquery.SchemaField(\"dispatching_base_num\", \"STRING\"),\\nbigquery.SchemaField(\"pickup_datetime\", \"TIMESTAMP\"),\\nbigquery.SchemaField(\"dropOff_datetime\", \"TIMESTAMP\"),\\nbigquery.SchemaField(\"PUlocationID\", \"STRING\"),\\nbigquery.SchemaField(\"DOlocationID\", \"STRING\"),\\nbigquery.SchemaField(\"SR_Flag\", \"STRING\"),\\nbigquery.SchemaField(\"Affiliated_base_number\", \"STRING\"),\\n],\\nskip_leading_rows=1,\\nwrite_disposition=write_disposition_string,\\nautodetect=True,\\nsource_format=\"CSV\",\\n)\\n# Load the data into BigQuery\\n# Create a temporary file to prevent the exception- AttributeError: \\'bytes\\' object has no attribute \\'tell\\'\"\\nwith tempfile.NamedTemporaryFile() as f:\\nf.write(response.content)\\nf.seek(0)\\njob = client.load_table_from_file(\\nf,\\ntable_id,\\nlocation=\"US\",\\njob_config=job_config,\\n)\\njob.result()\\nlogging.info(\"Data for month %d successfully loaded into table %s.\", month, table_id)\\nreturn \\'Data loaded into table {}.\\'.format(table_id)',\n",
       "    'section': 'Module 3: Data Warehousing',\n",
       "    'question': 'GCP BQ - Tip: Using Cloud Function to read csv.gz files from github directly to BigQuery in Google Cloud:'},\n",
       "   {'text': 'You need to uncheck cache preferences in query settings',\n",
       "    'section': 'Module 3: Data Warehousing',\n",
       "    'question': 'GCP BQ - When querying two different tables external and materialized you get the same result when count(distinct(*))'},\n",
       "   {'text': 'Problem: When you inject data into GCS using Pandas, there is a chance that some dataset has missing values on  DOlocationID and PUlocationID. Pandas by default will cast these columns as float data type, causing inconsistent data type between parquet in GCS and schema defined in big query. You will see something like this:\\nSolution:\\nFix the data type issue in data pipeline\\nBefore injecting data into GCS, use astype and Int64 (which is different from int64 and accept both missing value and integer exist in the column) to cast the columns.\\nSomething like:\\ndf[\"PUlocationID\"] = df.PUlocationID.astype(\"Int64\")\\ndf[\"DOlocationID\"] = df.DOlocationID.astype(\"Int64\")\\nNOTE: It is best to define the data type of all the columns in the Transformation section of the ETL pipeline before loading to BigQuery',\n",
       "    'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\",\n",
       "    'question': 'GCP BQ - How to handle type error from big query and parquet data?'},\n",
       "   {'text': 'Problem occurs when misplacing content after fro``m clause in BigQuery SQLs.\\nCheck to remove any extra apaces or any other symbols, keep in lowercases, digits and dashes only',\n",
       "    'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\",\n",
       "    'question': 'GCP BQ - Invalid project ID . Project IDs must contain 6-63 lowercase letters, digits, or dashes. Some project'},\n",
       "   {'text': 'No. Based on the documentation for Bigquery, it does not support more than 1 column to be partitioned.\\n[source]',\n",
       "    'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\",\n",
       "    'question': 'GCP BQ - Does BigQuery support multiple columns partition?'},\n",
       "   {'text': 'Error Message:\\nPARTITION BY expression must be DATE(<timestamp_column>), DATE(<datetime_column>), DATETIME_TRUNC(<datetime_column>, DAY/HOUR/MONTH/YEAR), a DATE column, TIMESTAMP_TRUNC(<timestamp_column>, DAY/HOUR/MONTH/YEAR), DATE_TRUNC(<date_column>, MONTH/YEAR), or RANGE_BUCKET(<int64_column>, GENERATE_ARRAY(<int64_value>, <int64_value>[, <int64_value>]))\\nSolution:\\nConvert the column to datetime first.\\ndf[\"pickup_datetime\"] = pd.to_datetime(df[\"pickup_datetime\"])\\ndf[\"dropOff_datetime\"] = pd.to_datetime(df[\"dropOff_datetime\"])',\n",
       "    'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\",\n",
       "    'question': 'GCP BQ - DATE() Error in BigQuery'},\n",
       "   {'text': 'Native tables are tables where the data is stored in BigQuery.  External tables store the data outside BigQuery, with BigQuery storing metadata about that external table.\\nResources:\\nhttps://cloud.google.com/bigquery/docs/external-tables\\nhttps://cloud.google.com/bigquery/docs/tables-intro',\n",
       "    'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\",\n",
       "    'question': 'GCP BQ - Native tables vs External tables in BigQuery?'},\n",
       "   {'text': 'Issue: Tried running command to export ML model from BQ to GCS from Week 3\\nbq --project_id taxi-rides-ny extract -m nytaxi.tip_model gs://taxi_ml_model/tip_model\\nIt is failing on following error:\\nBigQuery error in extract operation: Error processing job Not found: Dataset was not found in location US\\nI verified the BQ data set and gcs bucket are in the same region- us-west1. Not sure how it gets location US. I couldn’t find the solution yet.\\nSolution:  Please enter correct project_id and gcs_bucket folder address. My gcs_bucket folder address is\\ngs://dtc_data_lake_optimum-airfoil-376815/tip_model',\n",
       "    'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\",\n",
       "    'question': 'GCP BQ ML - Unable to run command (shown in video) to export ML model from BQ to GCS'},\n",
       "   {'text': \"To solve this error mention the location = US when creating the dim_zones table\\n{{ config(\\nmaterialized='table',\\nlocation='US'\\n) }}\\nJust Update this part to solve the issue and run the dim_zones again and then run the fact_trips\",\n",
       "    'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\",\n",
       "    'question': 'Dim_zones.sql Dataset was not found in location US When Running fact_trips.sql'},\n",
       "   {'text': 'Solution: proceed with setting up serving_dir on your computer as in the extract_model.md file. Then instead of\\ndocker pull tensorflow/serving\\nuse\\ndocker pull emacski/tensorflow-serving\\nThen\\ndocker run -p 8500:8500 -p 8501:8501 --mount type=bind,source=`pwd`/serving_dir/tip_model,target=/models/tip_model -e MODEL_NAME=tip_model -t emacski/tensorflow-serving\\nThen run the curl command as written, and you should get a prediction.',\n",
       "    'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\",\n",
       "    'question': 'GCP BQ ML - Export ML model to make predictions does not work for MacBook with Apple M1 chip (arm architecture).'},\n",
       "   {'text': 'Try deleting data you’ve saved to your VM locally during ETLs\\nKill processes related to deleted files\\nDownload ncdu and look for large files (pay particular attention to files related to Prefect)\\nIf you delete any files related to Prefect, eliminate caching from your flow code',\n",
       "    'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\",\n",
       "    'question': 'VMs - What do I do if my VM runs out of space?'},\n",
       "   {'text': \"Ans: What they mean is that they don't want you to do anything more than that. You should load the files into the bucket and create an external table based on those files (but nothing like cleaning the data and putting it in parquet format)\",\n",
       "    'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\",\n",
       "    'question': \"Homework - What does it mean “Stop with loading the files into a bucket.' Stop with loading the files into a bucket?”\"},\n",
       "   {'text': 'If for whatever reason you try to read parquets directly from nyc.gov’s cloudfront into pandas, you might run into this error:\\npyarrow.lib.ArrowInvalid: Casting from timestamp[us] to timestamp[ns] would result in out of bounds\\nCause:\\nthere is one errant data record where the dropOff_datetime was set to year 3019 instead of 2019.\\npandas uses “timestamp[ns]” (as noted above), and int64 only allows a ~580 year range, centered on 2000. See `pd.Timestamp.max` and `pd.Timestamp.min`\\nThis becomes out of bounds when pandas tries to read it because 3019 > 2300 (approx value of pd.Timestamp.Max\\nFix:\\nUse pyarrow to read it:\\nimport pyarrow.parquet as pq df = pq.read_table(\\'fhv_tripdata_2019-02.parquet\\').to_pandas(safe=False)\\nHowever this results in weird timestamps for the offending record\\nRead the datetime columns separately using pq.read_table\\n\\ntable = pq.read_table(‘taxi.parquet’)\\ndatetimes = [‘list of datetime column names’]\\ndf_dts = pd.DataFrame()\\nfor col in datetimes:\\ndf_dts[col] = pd.to_datetime(table .column(col), errors=\\'coerce\\')\\n\\nThe `errors=’coerce’` parameter will convert the out of bounds timestamps into either the max or the min\\nUse parquet.compute.filter to remove the offending rows\\n\\nimport pyarrow.compute as pc\\ntable = pq.read_table(\"‘taxi.parquet\")\\ndf = table.filter(\\npc.less_equal(table[\"dropOff_datetime\"], pa.scalar(pd.Timestamp.max))\\n).to_pandas()',\n",
       "    'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\",\n",
       "    'question': 'Homework - Reading parquets from nyc.gov directly into pandas returns Out of bounds error'},\n",
       "   {'text': 'Answer: The 2022 NYC taxi data parquet files are available for each month separately. Therefore, you need to add all 12 files to your GCS bucket and then refer to them using the URIs option when creating an external table in BigQuery. You can use the wildcard \"*\" to refer to all 12 files using a single string.',\n",
       "    'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\",\n",
       "    'question': 'Question: for homework 3 , we need all 12 parquet files for green taxi 2022 right ?'},\n",
       "   {'text': 'This can help avoid schema issues in the homework. \\nDownload files locally and use the ‘upload files’ button in GCS at the desired path. You can upload many files at once. You can also choose to upload a folder.',\n",
       "    'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\",\n",
       "    'question': 'Homework - Uploading files to GCS via GUI'},\n",
       "   {'text': 'Ans: Take a careful look at the format of the dates in the question.',\n",
       "    'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\",\n",
       "    'question': 'Homework - Qn 5: The partitioned/clustered table isn’t giving me the prediction I expected'},\n",
       "   {'text': 'Many people aren’t getting an exact match, but are very close to one of the options. As per Alexey said to choose the closest option.',\n",
       "    'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\",\n",
       "    'question': 'Homework - Qn 6: Did anyone get an exact match for one of the options given in Module 3 homework Q6?'},\n",
       "   {'text': 'UnicodeDecodeError: \\'utf-8\\' codec can\\'t decode byte 0xa0 in position 41721: invalid start byte\\nSolution:\\nStep 1: When reading the data from the web into the pandas dataframe mention the encoding as follows:\\npd.read_csv(dataset_url, low_memory=False, encoding=\\'latin1\\')\\nStep 2: When writing the dataframe from the local system to GCS as a csv mention the encoding as follows:\\ndf.to_csv(path_on_gsc, compression=\"gzip\", encoding=\\'utf-8\\')\\nAlternative: use pd.read_parquet(url)',\n",
       "    'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\",\n",
       "    'question': 'Python - invalid start byte Error Message'},\n",
       "   {'text': 'A generator is a function in python that returns an iterator using the yield keyword.\\nA generator is a special type of iterable, similar to a list or a tuple, but with a crucial difference. Instead of creating and storing all the values in memory at once, a generator generates values on-the-fly as you iterate over it. This makes generators memory-efficient, particularly when dealing with large datasets.',\n",
       "    'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\",\n",
       "    'question': 'Python - Generators in python'},\n",
       "   {'text': 'The read_parquet function supports a list of files as an argument. The list of files will be merged into a single result table.',\n",
       "    'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\",\n",
       "    'question': 'Python - Easiest way to read multiple files at the same time?'},\n",
       "   {'text': \"Incorrect:\\ndf['DOlocationID'] = pd.to_numeric(df['DOlocationID'], downcast=integer) or\\ndf['DOlocationID'] = df['DOlocationID'].astype(int)\\nCorrect:\\ndf['DOlocationID'] = df['DOlocationID'].astype('Int64')\",\n",
       "    'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\",\n",
       "    'question': \"Python - These won't work. You need to make sure you use Int64:\"},\n",
       "   {'text': \"ValueError: Path /Users/kt/.prefect/storage/44ccce0813ed4f24ab2d3783de7a9c3a does not exist.\\nRemove ```cache_key_fn=task_input_hash ``` as it’s in argument in your function & run your flow again.\\nNote: catche key is beneficial if you happen to run the code multiple times, it won't repeat the process which you have finished running in the previous run.  That means, if you have this ```cache_key``` in your initial run, this might cause the error.\",\n",
       "    'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\",\n",
       "    'question': 'Prefect - Error on Running Prefect Flow to Load data to GCS'},\n",
       "   {'text': '@task\\ndef download_file(url: str, file_path: str):\\nresponse = requests.get(url)\\nopen(file_path, \"wb\").write(response.content)\\nreturn file_path\\n@flow\\ndef extract_from_web() -> None:\\nfile_path = download_file(url=f\\'{url-filename}.csv.gz\\',file_path=f\\'{filename}.csv.gz\\')',\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': 'Prefect - Tip: Downloading csv.gz from a url in a prefect environment (sample snippet).'},\n",
       "   {'text': 'Update the seed column types in the dbt_project.yaml file\\nfor using double : float\\nfor using int : numeric\\nDBT Cloud production error: prod dataset not available in location EU\\nProblem: I am trying to deploy my DBT  models to production, using DBT Cloud. The data should live in BigQuery.  The dataset location is EU.  However, when I am running the model in production, a prod dataset is being create in BigQuery with a location US and the dbt invoke build is failing giving me \"ERROR 404: porject.dataset:prod not available in location EU\". I tried different ways to fix this. I am not sure if there is a more simple solution then creating my project or buckets in location US. Hope anyone can help here.\\nNote: Everything is working fine in development mode, the issue is just happening when scheduling and running job in production\\nSolution: I created the prod dataset manually in BQ and specified EU, then I ran the job.',\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': 'If you are getting not found in location us error.'},\n",
       "   {'text': 'Error: This project does not have a development environment configured. Please create a development environment and configure your development credentials to use the dbt IDE.\\nThe error itself tells us how to solve this issue, the guide is here. And from videos @1:42 and also slack chat',\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': 'Setup - No development environment'},\n",
       "   {'text': \"Runtime Error\\ndbt was unable to connect to the specified database.\\nThe database returned the following error:\\n>Database Error\\nAccess Denied: Project <project_name>: User does not have bigquery.jobs.create permission in project <project_name>.\\nCheck your database credentials and try again. For more information, visit:\\nhttps://docs.getdbt.com/docs/configure-your-profile\\nSteps to resolve error in Google Cloud:\\n1. Navigate to IAM & Admin and select IAM\\n2. Click Grant Access if your newly created dbt service account isn't listed\\n3. In New principals field, add your service account\\n4. Select a Role and search for BigQuery Job User to add\\n5. Go back to dbt cloud project setup and Test your connection\\n6. Note: Also add BigQuery Data Owner, Storage Object Admin, & Storage Admin to prevent permission issues later in the course\",\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': 'Setup - Connecting dbt Cloud with BigQuery Error'},\n",
       "   {'text': 'error: This dbt Cloud run was cancelled because a valid dbt project was not found. Please check that the repository contains a proper dbt_project.yml config file. If your dbt project is located in a subdirectory of the connected repository, be sure to specify its location on the Project settings page in dbt Cloud',\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': 'Dbt build error'},\n",
       "   {'text': \"Error: Failed to clone repository.\\ngit clone git@github.com:DataTalksClub/data-engineering-zoomcamp.git /usr/src/develop/…\\nCloning into '/usr/src/develop/...\\nWarning: Permanently added 'github.com,140.82.114.4' (ECDSA) to the list of known hosts.\\ngit@github.com: Permission denied (publickey).\\nfatal: Could not read from remote repository.\\nIssue: You don’t have permissions to write to DataTalksClub/data-engineering-zoomcamp.git\\nSolution 1: Clone the repository and use this forked repo, which contains your github username. Then, proceed to specify the path, as in:\\n[your github username]/data-engineering-zoomcamp.git\\nSolution 2: create a fresh repo for dbt-lessons. We’d need to do branching and PRs in this lesson, so it might be a good idea to also not mess up your whole other repo. Then you don’t have to create a subfolder for the dbt project files\\nSolution 3: Use https link\",\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': 'Setup - Failed to clone repository.'},\n",
       "   {'text': \"Solution:\\nCheck if you’re on the Developer Plan. As per the prerequisites, you'll need to be enrolled in the Team Plan or Enterprise Plan to set up a CI Job in dbt Cloud.\\nSo If you're on the Developer Plan, you'll need to upgrade to utilise CI Jobs.\\nNote from another user: I’m in the Team Plan (trial period) but the option is still disabled. What worked for me instead was this. It works for the Developer (free) plan.\",\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': 'dbt job - Triggered by pull requests is disabled when I try to create a new Continuous Integration job in dbt cloud.'},\n",
       "   {'text': 'Issue: If the DBT cloud IDE loading indefinitely then giving you this error\\nSolution: check the dbt_cloud_setup.md  file and make a SSH Key and use gitclone to import repo into dbt project, copy and paste deploy key back in your repo setting.',\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': 'Setup - Your IDE session was unable to start. Please contact support.'},\n",
       "   {'text': 'Issue: If you don’t define the column format while converting from csv to parquet Python will “choose” based on the first rows.\\n✅Solution: Defined the schema while running web_to_gcp.py pipeline.\\nSebastian adapted the script:\\nhttps://github.com/sebastian2296/data-engineering-zoomcamp/blob/main/week_4_analytics_engineering/web_to_gcs.py\\nNeed a quick change to make the file work with gz files, added the following lines (and don’t forget to delete the file at the end of each iteration of the loop to avoid any problem of disk space)\\nfile_name_gz = f\"{service}_tripdata_{year}-{month}.csv.gz\"\\nopen(file_name_gz, \\'wb\\').write(r.content)\\nos.system(f\"gzip -d {file_name_gz}\")\\nos.system(f\"rm {file_name_init}.*\")\\nSame ERROR - When running dbt run for fact_trips.sql, the task failed with error:\\n“Parquet column \\'ehail_fee\\' has type DOUBLE which does not match the target cpp_type INT64”\\n开启屏幕阅读器支持\\n要启用屏幕阅读器支持，请按Ctrl+Alt+Z。要了解键盘快捷键，请按Ctrl+斜杠。\\n查找和替换\\nReason: Parquet files have their own schema. Some parquet files for green data have records with decimals in ehail_fee column.\\nThere are some possible fixes:\\nDrop ehail_feel column since it is not really used. For instance when creating a partitioned table from the external table in BigQuery\\nSELECT * EXCEPT (ehail_fee) FROM…\\nModify stg_green_tripdata.sql model using this line cast(0 as numeric) as ehail_fee.\\nModify Airflow dag to make the conversion and avoid the error.\\npv.read_csv(src_file, convert_options=pv.ConvertOptions(column_types = {\\'ehail_fee\\': \\'float64\\'}))\\nSame type of ERROR - parquet files with different data types - Fix it with pandas\\nHere is another possibility that could be interesting:\\nYou can specify the dtypes when importing the file from csv to a dataframe with pandas\\npd.from_csv(..., dtype=type_dict)\\nOne obstacle is that the regular int64 pandas use (I think this is from the numpy library) does not accept null values (NaN, not a number). But you can use the pandas Int64 instead, notice capital ‘I’. The type_dict is a python dictionary mapping the column names to the dtypes.\\nSources:\\nhttps://pandas.pydata.org/docs/reference/api/pandas.read_csv.html\\nNullable integer data type — pandas 1.5.3 documentation',\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': 'DBT - I am having problems with columns datatype while running DBT/BigQuery'},\n",
       "   {'text': 'If the provided URL isn’t working for you (https://nyc-tlc.s3.amazonaws.com/trip+data/):\\nWe can use the GitHub CLI to easily download the needed trip data from https://github.com/DataTalksClub/nyc-tlc-data, and manually upload to a GCS bucket.\\nInstructions on how to download the CLI here: https://github.com/cli/cli\\nCommands to use:\\ngh auth login\\ngh release list -R DataTalksClub/nyc-tlc-data\\ngh release download yellow -R DataTalksClub/nyc-tlc-data\\ngh release download green -R DataTalksClub/nyc-tlc-data\\netc.\\nNow you can upload the files to a GCS bucket using the GUI.',\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': 'Ingestion: When attempting to use the provided quick script to load trip data into GCS, you receive error Access Denied from the S3 bucket'},\n",
       "   {'text': \"R: This conversion is needed for the question 3 of homework, in order to process files for fhv data. The error is:\\npyarrow.lib.ArrowInvalid: CSV parse error: Expected 7 columns, got 1: B02765\\nCause: Some random line breaks in this particular file.\\nFixed by opening a bash in the container executing the dag and manually running the following command that deletes all \\\\n not preceded by \\\\r.\\nperl -i -pe 's/(?<!\\\\r)\\\\n/\\\\1/g' fhv_tripdata_2020-01.csv\\nAfter that, clear the failed task in Airflow to force re-execution.\",\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': 'Ingestion - Error thrown by format_to_parquet_task when converting fhv_tripdata_2020-01.csv using Airflow'},\n",
       "   {'text': 'I initially followed data-engineering-zoomcamp/03-data-warehouse/extras/web_to_gcs.py at main · DataTalksClub/data-engineering-bootcamp (github.com)\\nBut it was taking forever for the yellow trip data and when I tried to download and upload the parquet files directly to GCS, that works fine but when creating the Bigquery table, there was a schema inconsistency issue\\nThen I found another hack shared in the slack which was suggested by Victoria.\\n[Optional] Hack for loading data to BigQuery for Week 4 - YouTube\\nPlease watch until the end as there is few schema changes required to be done',\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': 'Hack to load yellow and green trip data for 2019 and 2020'},\n",
       "   {'text': '“gs\\\\storage_link\\\\*.parquet” need to be added in destination folder',\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': 'Move many files (more than one) from Google cloud storage bucket to Big query'},\n",
       "   {'text': 'One common cause experienced is lack of space after running prefect several times. When running prefect, check the folder ‘.prefect/storage’ and delete the logs now and then to avoid the problem.',\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': 'GCP VM - All of sudden ssh stopped working for my VM after my last restart'},\n",
       "   {'text': 'You can try to do this steps:',\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': 'GCP VM - If you have lost SSH access to your machine due to lack of space. Permission denied (publickey)'},\n",
       "   {'text': 'R: Go to BigQuery, and check the location of BOTH\\nThe source dataset (trips_data_all), and\\nThe schema you’re trying to write to (name should be \\tdbt_<first initial><last name> (if you didn’t change the default settings at the end when setting up your project))\\nLikely, your source data will be in your region, but the write location will be a multi-regional location (US in this example). Delete these datasets, and recreate them with your specified region and the correct naming format.\\nAlternatively, instead of removing datasets, you can specify the single-region location you are using. E.g. instead of ‘location: US’, specify the region, so ‘location: US-east1’. See this Github comment for more detail. Additionally please see this post of Sandy\\nIn DBT cloud you can actually specify the location using the following steps:\\nGPo to your profile page (top right drop-down --> profile)\\nThen go to under Credentials --> Analytics (you may have customised this name)\\nClick on Bigquery >\\nHit Edit\\nUpdate your location, you may need to re-upload your service account JSON to re-fetch your private key, and save. (NOTE: be sure to exactly copy the region BigQuery specifies your dataset is in.)',\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': '404 Not found: Dataset eighth-zenith-372015:trip_data_all was not found in location us-west1'},\n",
       "   {'text': 'Error: `dbt_utils.surrogate_key` has been replaced by `dbt_utils.generate_surrogate_key`\\nFix:\\nReplace dbt_utils.surrogate_key  with dbt_utils.generate_surrogate_key in stg_green_tripdata.sql\\nWhen executing dbt run after fact_trips.sql has been created, the task failed with error:\\nR: “Access Denied: BigQuery BigQuery: Permission denied while globbing file pattern.”\\n1. Fixed by adding the Storage Object Viewer role to the service account in use in BigQuery.\\n2. Add the related roles to the service account in use in GCS.',\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': 'When executing dbt run after installing dbt-utils latest version i.e., 1.0.0 warning has generated'},\n",
       "   {'text': 'You need to create packages.yml file in main project directory and add packages’ meta data:\\npackages:\\n- package: dbt-labs/dbt_utils\\nversion: 0.8.0\\nAfter creating file run:\\nAnd hit enter.',\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': 'When You are getting error dbt_utils not found'},\n",
       "   {'text': \"Ensure you properly format your yml file. Check the build logs if the run was completed successfully. You can expand the command history console (where you type the --vars '{'is_test_run': 'false'}')  and click on any stage’s logs to expand and read errors messages or warnings.\",\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': 'Lineage is currently unavailable. Check that your project does not contain compilation errors or contact support if this error persists.'},\n",
       "   {'text': \"Make sure you use:\\ndbt run --var ‘is_test_run: false’ or\\ndbt build --var ‘is_test_run: false’\\n(watch out for formatted text from this document: re-type the single quotes). If that does not work, use --vars '{'is_test_run': 'false'}' with each phrase separately quoted.\",\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': 'Build - Why do my Fact_trips only contain a few days of data?'},\n",
       "   {'text': 'Check if you specified if_exists argument correctly when writing data from GCS to BigQuery. When I wrote my automated flow for each month of the years 2019 and 2020 for green and yellow data I had specified if_exists=\"replace\" while I was experimenting with the flow setup. Once you want to run the flow for all months in 2019 and 2020 make sure to set if_exists=\"append\"\\nif_exists=\"replace\" will replace the whole table with only the month data that you are writing into BigQuery in that one iteration -> you end up with only one month in BigQuery (the last one you inserted)\\nif_exists=\"append\" will append the new monthly data -> you end up with data from all months',\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': 'Build - Why do my fact_trips only contain one month of data?'},\n",
       "   {'text': \"R: After the second SELECT, change this line:\\ndate_trunc('month', pickup_datetime) as revenue_month,\\nTo this line:\\ndate_trunc(pickup_datetime, month) as revenue_month,\\nMake sure that “month” isn’t surrounded by quotes!\",\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': 'BigQuery returns an error when I try to run the dm_monthly_zone_revenue.sql model.'},\n",
       "   {'text': 'For this instead:\\n{{ dbt_utils.generate_surrogate_key([ \\n     field_a, \\n     field_b, \\n     field_c,\\n     …,\\n     field_z\\n]) }}',\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': 'Replace: \\n{{ dbt_utils.surrogate_key([ \\n     field_a, \\n     field_b, \\n     field_c,\\n     …,\\n     field_z     \\n]) }}'},\n",
       "   {'text': 'Remove the dataset from BigQuery which was created by dbt and run dbt run again so that it will recreate the dataset in BigQuery with the correct location',\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': 'I changed location in dbt, but dbt run still gives me an error'},\n",
       "   {'text': 'Remove the dataset from BigQuery created by dbt and run again (with test disabled) to ensure the dataset created has all the rows.\\nDBT - Why am I getting a new dataset after running my CI/CD Job? / What is this new dbt dataset in BigQuery?\\nAnswer: when you create the CI/CD job, under ‘Compare Changes against an environment (Deferral) make sure that you select ‘ No; do not defer to another environment’ - otherwise dbt won’t merge your dev models into production models; it will create a new environment called ‘dbt_cloud_pr_number of pull request’',\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': 'I ran dbt run without specifying variable which gave me a table of 100 rows. I ran again with the variable value specified but my table still has 100 rows in BQ.'},\n",
       "   {'text': \"Vic created three different datasets in the videos.. dbt_<name> was used for development and you used a production dataset for the production environment. What was the use for the staging dataset?\\nR: Staging, as the name suggests, is like an intermediate between the raw datasets and the fact and dim tables, which are the finished product, so to speak. You'll notice that the datasets in staging are materialised as views and not tables.\\nVic didn't use it for the project, you just need to create production and dbt_name + trips_data_all that you had already.\",\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': 'Why do we need the Staging dataset?'},\n",
       "   {'text': 'Try removing the “network: host” line in docker-compose.',\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': 'DBT Docs Served but Not Accessible via Browser'},\n",
       "   {'text': 'Go to Account settings >> Project >> Analytics >> Click on your connection >> go all the way down to Location and type in the GCP location just as displayed in GCP (e.g. europe-west6). You might need to reupload your GCP key.\\nDelete your dataset in GBQ\\nRebuild project: dbt build\\nNewly built dataset should be in the correct location',\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': 'BigQuery adapter: 404 Not found: Dataset was not found in location europe-west6'},\n",
       "   {'text': 'Create a new branch to edit. More on this can be found here in the dbt docs.',\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': 'Dbt+git - Main branch is “read-only”'},\n",
       "   {'text': 'Create a new branch for development, then you can merge it to the main branch\\nCreate a new branch and switch to this branch. It allows you to make changes. Then you can commit and push the changes to the “main” branch.',\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': \"Dbt+git - It appears that I can't edit the files because I'm in read-only mode. Does anyone know how I can change that?\"},\n",
       "   {'text': \"Error:\\nTriggered by pull requests\\nThis feature is only available for dbt repositories connected through dbt Cloud's native integration with Github, Gitlab, or Azure DevOps\\nSolution: Contrary to the guide on DTC repo, don’t use the Git Clone option. Use the Github one instead. Step-by-step guide to UN-LINK Git Clone and RE-LINK with Github in the next entry below\",\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': 'Dbt deploy + Git CI - cannot create CI checks job for deployment to Production. See more discussion in slack chat'},\n",
       "   {'text': 'If you’re trying to configure CI with Github and on the job’s options you can’t see Run on Pull Requests? on triggers, you have to reconnect with Github using native connection instead clone by SSH. Follow these steps:\\nOn Profile Settings > Linked Accounts connect your Github account with dbt project allowing the permissions asked. More info at https://docs.getdbt.com/docs/collaborate/git/connect-gith\\nDisconnect your current Github’s configuration from Account Settings > Projects (analytics) > Github connection. At the bottom left appears the button Disconnect, press it.\\nOnce we have confirmed the change, we can configure it again. This time, choose Github and it will appear in all repositories which you have allowed to work with dbt. Select your repository and it’s ready.\\nGo to the Deploy > job configuration’s page and go down until Triggers and now you can see the option Run on Pull Requests:',\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': 'Dbt deploy + Git CI - Unable to configure Continuous Integration (CI) with Github'},\n",
       "   {'text': \"If you're following video DE Zoomcamp 4.3.1 - Building the First DBT Models, you may have encountered an issue at 14:25 where the Lineage graph isn't displayed and a Compilation Error occurs, as shown in the attached image. Don't worry - a quick fix for this is to simply save your schema.yml file. Once you've done this, you should be able to view your Lineage graph without any further issues.\",\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': \"Compilation Error (Model 'model.my_new_project.stg_green_tripdata' (models/staging/stg_green_tripdata.sql) depends on a source named 'staging.green_trip_external' which was not found)\"},\n",
       "   {'text': '> in macro test_accepted_values (tests/generic/builtin.sql)\\n> called by test accepted_values_stg_green_tripdata_Payment_type__False___var_payment_type_values_ (models/staging/schema.yml)\\nRemember that you have to add to dbt_project.yml the vars:\\nvars:\\npayment_type_values: [1, 2, 3, 4, 5, 6]',\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': \"'NoneType' object is not iterable\"},\n",
       "   {'text': \"You will face this issue if you copied and pasted the exact macro directly from data-engineering-zoomcamp repo.\\nBigQuery adapter: Retry attempt 1 of 1 after error: BadRequest('No matching signature for operator CASE for argument types: STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, NULL at [35:5]; reason: invalidQuery, location: query, message: No matching signature for operator CASE for argument types: STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, NULL at [35:5]')\\nWhat you’d have to do is to change the data type of the numbers (1, 2, 3 etc.) to text by inserting ‘’, as the initial ‘payment_type’ data type should be string (Note: I extracted and loaded the green trips data using Google BQ Marketplace)\\n{#\\nThis macro returns the description of the payment_type\\n#}\\n{% macro get_payment_type_description(payment_type) -%}\\ncase {{ payment_type }}\\nwhen '1' then 'Credit card'\\nwhen '2' then 'Cash'\\nwhen '3' then 'No charge'\\nwhen '4' then 'Dispute'\\nwhen '5' then 'Unknown'\\nwhen '6' then 'Voided trip'\\nend\\n{%- endmacro %}\",\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': 'dbt macro errors with get_payment_type_description(payment_type)'},\n",
       "   {'text': 'The dbt error  log contains a link to BigQuery. When you follow it you will see your query and the problematic line will be highlighted.',\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': 'Troubleshooting in dbt:'},\n",
       "   {'text': 'It is a default behaviour of dbt to append custom schema to initial schema. To override this behaviour simply create a macro named “generate_schema_name.sql”:\\n{% macro generate_schema_name(custom_schema_name, node) -%}\\n{%- set default_schema = target.schema -%}\\n{%- if custom_schema_name is none -%}\\n{{ default_schema }}\\n{%- else -%}\\n{{ custom_schema_name | trim }}\\n{%- endif -%}\\n{%- endmacro %}\\nNow you can override default custom schema in “dbt_project.yml”:',\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': 'Why changing the target schema to “marts” actually creates a schema named “dbt_marts” instead?'},\n",
       "   {'text': 'There is a project setting which allows you to set `Project subdirectory` in dbt cloud:',\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': 'How to set subdirectory of the github repository as the dbt project root'},\n",
       "   {'text': \"Remember that you should modify accordingly your .sql models, to read from existing table names in BigQuery/postgres db\\nExample: select * from {{ source('staging',<your table name in the database>') }}\",\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': \"Compilation Error : Model 'model.XXX' (models/<model_path>/XXX.sql) depends on a source named '<a table name>' which was not found\"},\n",
       "   {'text': 'Make sure that you create a pull request from your Development branch to the Production branch (main by default). After that, check in your ‘seeds’ folder if the seed file is inside it.\\nAnother thing to check is your .gitignore file. Make sure that the .csv extension is not included.',\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': \"Compilation Error : Model '<model_name>' (<model_path>) depends on a node named '<seed_name>' which was not found   (Production Environment)\"},\n",
       "   {'text': '1. Go to your dbt cloud service account\\n1. Adding the  [Storage Object Admin,Storage Admin] role in addition tco BigQuery Admin.',\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': 'When executing dbt run after using fhv_tripdata as an external table: you get “Access Denied: BigQuery BigQuery: Permission denied”'},\n",
       "   {'text': 'Problem: when injecting data to bigquery, you may face the type error. This is because pandas by default will parse integer columns with missing value as float type.\\nSolution:\\nOne way to solve this problem is to specify/ cast data type Int64 during the data transformation stage.\\nHowever, you may be lazy to type all the int columns. If that is the case, you can simply use convert_dtypes to infer the data type\\n# Make pandas to infer correct data type (as pandas parse int with missing as float)\\ndf.fillna(-999999, inplace=True)\\ndf = df.convert_dtypes()\\ndf = df.replace(-999999, None)',\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': 'How to automatically infer the column data type (pandas missing value issues)?'},\n",
       "   {'text': 'Seed files loaded from directory with name ‘seed’, that’s why you should rename dir with name ‘data’ to ‘seed’',\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': 'When loading github repo raise exception that ‘taxi_zone_lookup’ not found'},\n",
       "   {'text': 'Check the .gitignore file and make sure you don’t have *.csv in it\\n\\nDbt error 404 was not found in location\\nMy specific error:\\nRuntime Error in rpc request (from remote system.sql) 404 Not found: Table dtc-de-0315:trips_data_all.green_tripdata_partitioned was not found in location europe-west6 Location: europe-west6 Job ID: 168ee9bd-07cd-4ca4-9ee0-4f6b0f33897c\\nMake sure all of your datasets have the correct region and not a generalised region:\\nEurope-west6 as opposed to EU\\n\\nMatch this in dbt settings:\\ndbt -> projects -> optional settings -> manually set location to match',\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': '‘taxi_zone_lookup’ not found'},\n",
       "   {'text': \"The easiest way to avoid these errors is by ingesting the relevant data in a .csv.gz file type. Then, do:\\nCREATE OR REPLACE EXTERNAL TABLE `dtc-de.trips_data_all.fhv_tripdata`\\nOPTIONS (\\nformat = 'CSV',\\nuris = ['gs://dtc_data_lake_dtc-de-updated/data/fhv_all/fhv_tripdata_2019-*.csv.gz']\\n);\\nAs an example. You should no longer have any data type issues for week 4.\",\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': 'Data type errors when ingesting with parquet files'},\n",
       "   {'text': 'This is due to the way the deduplication is done in the two staging files.\\nSolution: add order by in the partition by part of both staging files. Keep adding columns to order by until the number of rows in the fact_trips table is consistent when re-running the fact_trips model.\\nExplanation (a bit convoluted, feel free to clarify, correct etc.)\\nWe partition by vendor id and pickup_datetime and choose the first row (rn=1) from all these partitions. These partitions are not ordered, so every time we run this, the first row might be a different one. Since the first row is different between runs, it might or might not contain an unknown borough. Then, in the fact_trips model we will discard a different number of rows when we discard all values with an unknown borough.',\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': 'Inconsistent number of rows when re-running fact_trips model'},\n",
       "   {'text': 'If you encounter data type error on trip_type column, it may due to some nan values that isn’t null in bigquery.\\nSolution: try casting it to FLOAT datatype instead of NUMERIC',\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': 'Data Type Error when running fact table'},\n",
       "   {'text': \"This error could result if you are using some select * query without mentioning the name of table for ex:\\nwith dim_zones as (\\nselect * from `engaged-cosine-374921`.`dbt_victoria_mola`.`dim_zones`\\nwhere borough != 'Unknown'\\n),\\nfhv as (\\nselect * from `engaged-cosine-374921`.`dbt_victoria_mola`.`stg_fhv_tripdata`\\n)\\nselect * from fhv\\ninner join dim_zones as pickup_zone\\non fhv.PUlocationID = pickup_zone.locationid\\ninner join dim_zones as dropoff_zone\\non fhv.DOlocationID = dropoff_zone.locationid\\n);\\nTo resolve just replace use : select fhv.* from fhv\",\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': 'CREATE TABLE has columns with duplicate name locationid.'},\n",
       "   {'text': 'Some ehail fees are null and casting them to integer gives Bad int64 value: 0.0 error,\\nSolution:\\nUsing safe_cast returns NULL instead of throwing an error. So use safe_cast from dbt_utils function in the jinja code for casting into integer as follows:\\n{{ dbt_utils.safe_cast(\\'ehail_fee\\',  api.Column.translate_type(\"integer\"))}} as ehail_fee,\\nCan also just use safe_cast(ehail_fee as integer) without relying on dbt_utils.',\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': 'Bad int64 value: 0.0 error'},\n",
       "   {'text': \"You might encounter this when building the fact_trips.sql model. The issue may be with the payment_type_description field.\\nUsing safe_cast as above, would cause the entire field to become null. A better approach is to drop the offending decimal place, then cast to integer.\\ncast(replace({{ payment_type }},'.0','') as integer)\\nBad int64 value: 1.0 error (again)\\n\\nI found that there are more columns causing the bad INT64: ratecodeid and trip_type on Green_tripdata table.\\nYou can use the queries below to address them:\\nCAST(\\nREGEXP_REPLACE(CAST(rate_code AS STRING), r'\\\\.0', '') AS INT64\\n) AS ratecodeid,\\nCAST(\\nCASE\\nWHEN REGEXP_CONTAINS(CAST(trip_type AS STRING), r'\\\\.\\\\d+') THEN NULL\\nELSE CAST(trip_type AS INT64)\\nEND AS INT64\\n) AS trip_type,\",\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': 'Bad int64 value: 2.0/1.0 error'},\n",
       "   {'text': 'The two solution above don’t work for me - I used the line below in `stg_green_trips.sql` to replace the original ehail_fee line:\\n`{{ dbt.safe_cast(\\'ehail_fee\\',  api.Column.translate_type(\"numeric\"))}} as ehail_fee,`',\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': 'DBT - Error on building fact_trips.sql: Parquet column \\'ehail_fee\\' has type DOUBLE which does not match the target cpp_type INT64. File: gs://<gcs bucket>/<table>/green_taxi_2019-01.parquet\")'},\n",
       "   {'text': \"Remember to add a space between the variable and the value. Otherwise, it won't be interpreted as a dictionary.\\nIt should be:\\ndbt run --var 'is_test_run: false'\",\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': 'The - vars argument must be a YAML dictionary, but was of type str'},\n",
       "   {'text': \"You don't need to change the environment type. If you are following the videos, you are creating a Production Deployment, so the only available option is the correct one.'\",\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': 'Not able to change Environment Type as it is greyed out and inaccessible'},\n",
       "   {'text': 'Database Error in model stg_yellow_tripdata (models/staging/stg_yellow_tripdata.sql)\\nAccess Denied: Table taxi-rides-ny-339813-412521:trips_data_all.yellow_tripdata: User does not have permission to query table taxi-rides-ny-339813-412521:trips_data_all.yellow_tripdata, or perhaps it does not exist in location US.\\ncompiled Code at target/run/taxi_rides_ny/models/staging/stg_yellow_tripdata.sql\\nIn my case, I was set up in a different branch, so always check the branch you are working on. Change the 04-analytics-engineering/taxi_rides_ny/models/staging/schema.yml file in the\\nsources:\\n- name: staging\\ndatabase: your_database_name\\nIf this error will continue when running dbt job, As for changing the branch for your job, you can use the ‘Custom Branch’ settings in your dbt Cloud environment. This allows you to run your job on a different branch than the default one (usually main). To do this, you need to:\\nGo to an environment and select Settings to edit it\\nSelect Only run on a custom branch in General settings\\nEnter the name of your custom branch (e.g. HW)\\nClick Save\\nCould not parse the dbt project. please check that the repository contains a valid dbt project\\nRunning the Environment on the master branch causes this error, you must activate “Only run on a custom branch” checkbox and specify the branch you are  working when Environment is setup.',\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': 'Access Denied: Table taxi-rides-ny-339813-412521:trips_data_all.yellow_tripdata: User does not have permission to query table taxi-rides-ny-339813-412521:trips_data_all.yellow_tripdata, or perhaps it does not exist in location US.'},\n",
       "   {'text': 'Change to main branch, make a pull request from the development branch.\\nNote: this will take you to github.\\nApprove the merging and rerun you job, it would work as planned now',\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': 'Made change to your modelling files and commit the your development branch, but Job still runs on old file?'},\n",
       "   {'text': 'Before you can develop some data model on dbt, you should create development environment and set some parameter on it. After the model being developed, we should also create deployment environment to create and run some jobs.',\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': 'Setup - I’ve set Github and Bigquery to dbt successfully. Why nothing showed in my Develop tab?'},\n",
       "   {'text': 'Error Message:\\nInvestigate Sentry error: ProtocolError \"Invalid input ConnectionInputs.SEND_HEADERS in state ConnectionState.CLOSED\"\\nSolution:\\nreference\\nRun it again because it happens sometimes. Or wait a few minutes, it will continue.',\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': 'Prefect Agent retrieving runs from queue sometimes fails with httpx.LocalProtocolError'},\n",
       "   {'text': \"My taxi data was loaded into gcs with etl_web_to_gcs.py script that converts csv data into parquet. Then I placed raw data trips into external tables and when I executed dbt run I got an error message: Parquet column 'passenger_count' has type INT64 which does not match the target cpp_type DOUBLE. It is because several columns in files have different formats of data.\\nWhen I added df[col] = df[col].astype('Int64') transformation to the columns: passenger_count, payment_type, RatecodeID, VendorID, trip_type it went ok. Several people also faced this error and more about it you can read on the slack channel.\",\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': 'BigQuery returns an error when i try to run ‘dbt run’:'},\n",
       "   {'text': 'Use the syntax below instead if the code in the tutorial is not working.\\ndbt run --select stg_green_tripdata --vars \\'{\"is_test_run\": false}\\'',\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': \"Running dbt run --models stg_green_tripdata --var 'is_test_run: false' is not returning anything:\"},\n",
       "   {'text': \"Following dbt with BigQuery on Docker readme.md, after `docker-compose build` and `docker-compose run dbt-bq-dtc init`, encountered error `ModuleNotFoundError: No module named 'pytz'`\\nSolution:\\nAdd `RUN python -m pip install --no-cache pytz` in the Dockerfile under `FROM --platform=$build_for python:3.9.9-slim-bullseye as base`\",\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': \"DBT - Error: No module named 'pytz' while setting up dbt with docker\"},\n",
       "   {'text': \"If you have problems editing dbt_project.yml when using Docker after ‘docker-compose run dbt-bq-dtc init’, to change profile ‘taxi_rides_ny’ to 'bq-dbt-workshop’, just run:\\nsudo chown -R username path\\nDBT - Internal Error: Profile should not be None if loading is completed\\nWhen  running dbt debug, change the directory to the newly created subdirectory (e.g: the newly created `taxi_rides_ny` directory, which contains the dbt project).\",\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': '\\u200b\\u200bVS Code: NoPermissions (FileSystemError): Error: EACCES: permission denied (linux)'},\n",
       "   {'text': 'When running a query on BigQuery sometimes could appear a this table is not on the specified location error.\\nFor this problem there is not a straightforward solution, you need to dig a little, but the problem could be one of these:\\nCheck the locations of your bucket, datasets and tables. Make sure they are all on the same one.\\nChange the query settings to the location you are in: on the query window select more -> query settings -> select the location\\nCheck if all the paths you are using in your query to your tables are correct: you can click on the table -> details -> and copy the path.',\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': 'Google Cloud BigQuery Location Problems'},\n",
       "   {'text': 'This happens because we have moved the dbt project to another directory on our repo.\\nOr might be that you’re on a different branch than is expected to be merged from / to.\\nSolution:\\nGo to the projects window on dbt cloud -> settings -> edit -> and add directory (the extra path to the dbt project)\\nFor example:\\n/week5/taxi_rides_ny\\nMake sure your file explorer path and this Project settings path matches and there’s no files waiting to be committed to github if you’re running the job to deploy to PROD.\\nAnd that you had setup the PROD environment to check in the main branch, or whichever you specified.\\nIn the picture below, I had set it to ella2024 to be checked as “production-ready” by the “freshness” check mark at the PROD environment settings. So each time I merge a branch from something else into ella2024 and then trigger the PR, the CI check job would kick-in. But we still do need to Merge and close the PR manually, I believe, that part is not automated.\\nYou set up the PROD custom branch (if not default main) in the Environment setup screen.',\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': 'DBT Deploy - This dbt Cloud run was cancelled because a valid dbt project was not found.'},\n",
       "   {'text': 'When you are creating the pull request and running the CI, dbt is creating a new schema on BIgQuery. By default that new schema will be created on ‘US’ location, if you have your dataset, schemas and tables on ‘EU’ that will generate an error and the pull request will not be accepted. To change that location to ‘EU’ on the connection to BigQuery from dbt we need to add the location ‘EU’ on the connection optional settings:\\nDbt -> project -> settings -> connection BIgQuery -> OPtional Settings -> Location -> EU',\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': 'DBT Deploy + CI - Location Problems on BigQuery'},\n",
       "   {'text': 'When running trying to run the dbt project on prod there is some things you need to do and check on your own:\\nFirst Make the pull request and Merge the branch into the main.\\nMake sure you have the latest version, if you made changes to the repo in another place.\\nCheck if the dbt_project.yml file is accessible to the project, if not check this solution (Dbt: This dbt Cloud run was cancelled because a valid dbt project was not found.).\\nCheck if the name you gave to the dataset on BigQuery is the same you put on the dataset spot on the production environment created on dbt cloud.',\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': 'DBT Deploy - Error When trying to run the dbt project on Prod'},\n",
       "   {'text': 'In the step in this video (DE Zoomcamp 4.3.1 - Build the First dbt Models), after creating `stg_green_tripdata.sql` and clicking `build`, I encountered an error saying dataset not found in location EU. The default location for dbt Bigquery is the US, so when generating the new Bigquery schema for dbt, unless specified, the schema locates in the US.\\nSolution:\\nTurns out I forgot to specify Location to be `EU` when adding connection details.\\nDevelop -> Configure Cloud CLI -> Projects -> taxi_rides_ny -> (connection) Bigquery -> Edit -> Location (Optional) -> type `EU` -> Save',\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': 'DBT - Error: “404 Not found: Dataset <dataset_name>:<dbt_schema_name> was not found in location EU” after building from stg_green_tripdata.sql'},\n",
       "   {'text': 'Issue: If you’re having problems loading the FHV_20?? data from the github repo into GCS and then into BQ (input file not of type parquet), you need to do two things. First, append the URL Template link with ‘?raw=true’ like so:\\nURL_TEMPLATE = URL_PREFIX + \"/fhv_tripdata_{{ execution_date.strftime(\\\\\\'%Y-%m\\\\\\') }}.parquet?raw=true\"\\nSecond, update make sure the URL_PREFIX is set to the following value:\\n\\nURL_PREFIX = \"https://github.com/alexeygrigorev/datasets/blob/master/nyc-tlc/fhv\"\\nIt is critical that you use this link with the keyword blob. If your link has ‘tree’ here, replace it. Everything else can stay the same, including the curl -sSLf command. ‘',\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': 'Homework - Ingesting FHV_20?? data'},\n",
       "   {'text': 'I found out that the easies way to upload datasets form github for the homework is utilising this script git_csv_to_gcs.py. Thank you Lidia!!\\nIt is similar to a script that Alexey provided us in 03-data-warehouse/extras/web_to_gcs.py',\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': 'Homework - Ingesting NYC TLC Data'},\n",
       "   {'text': 'If you have to securely put your credentials for a project and, probably, push it to a git repository then the best option is to use an environment variable\\nFor example for web_to_gcs.py or git_csv_to_gcs.py we have to set these variables:\\nGOOGLE_APPLICATION_CREDENTIALS\\nGCP_GCS_BUCKET\\nThe easises option to do it  is to use .env  (dotenv).\\nInstall it and add a few lines of code that inject these variables for your project\\npip install python-dotenv\\nfrom dotenv import load_dotenv\\nimport os\\n# Load environment variables from .env file\\nload_dotenv()\\n# Now you can access environment variables like GCP_GCS_BUCKET and GOOGLE_APPLICATION_CREDENTIALS\\ncredentials_path = os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\\nBUCKET = os.environ.get(\"GCP_GCS_BUCKET\")',\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': 'How to set environment variable easily for any credentials'},\n",
       "   {'text': \"If you uploaded manually the fvh 2019 csv files, you may face errors regarding date types. Try to create an the external table in bigquery but define the pickup_datetime and dropoff_datetime to be strings\\nCREATE OR REPLACE EXTERNAL TABLE `gcp_project.trips_data_all.fhv_tripdata`  (\\ndispatching_base_num STRING,\\npickup_datetime STRING,\\ndropoff_datetime STRING,\\nPUlocationID STRING,\\nDOlocationID STRING,\\nSR_Flag STRING,\\nAffiliated_base_number STRING\\n)\\nOPTIONS (\\nformat = 'csv',\\nuris = ['gs://bucket/*.csv']\\n);\\nThen when creating the fhv core model in dbt, use TIMESTAMP(CAST(()) to ensure it first parses as a string and then convert it to timestamp.\\nwith fhv_tripdata as (\\nselect * from {{ ref('stg_fhv_tripdata') }}\\n),\\ndim_zones as (\\nselect * from {{ ref('dim_zones') }}\\nwhere borough != 'Unknown'\\n)\\nselect fhv_tripdata.dispatching_base_num,\\nTIMESTAMP(CAST(fhv_tripdata.pickup_datetime AS STRING)) AS pickup_datetime,\\nTIMESTAMP(CAST(fhv_tripdata.dropoff_datetime AS STRING)) AS dropoff_datetime,\",\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': \"Invalid date types after Ingesting FHV data through CSV files: Could not parse 'pickup_datetime' as a timestamp\"},\n",
       "   {'text': \"If you uploaded manually the fvh 2019 parquet files manually after downloading from https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2019-*.parquet you may face errors regarding date types while loading the data in a landing table (say fhv_tripdata). Try to create an the external table with the schema defines as following and load each month in a loop.\\n-----Correct load with schema defination----will not throw error----------------------\\nCREATE OR REPLACE EXTERNAL TABLE `dw-bigquery-week-3.trips_data_all.external_tlc_fhv_trips_2019` (\\ndispatching_base_num STRING,\\npickup_datetime TIMESTAMP,\\ndropoff_datetime TIMESTAMP,\\nPUlocationID FLOAT64,\\nDOlocationID FLOAT64,\\nSR_Flag FLOAT64,\\nAffiliated_base_number STRING\\n)\\nOPTIONS (\\nformat = 'PARQUET',\\nuris = ['gs://project id/fhv_2019_8.parquet']\\n);\\nCan Also USE  uris = ['gs://project id/fhv_2019_*.parquet'] (THIS WILL remove the need for the loop and can be done for all month in single RUN )\\n– THANKYOU FOR THIS –\",\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': 'Invalid data types after Ingesting FHV data through parquet files: Could not parse SR_Flag as Float64,Couldn’t parse datetime column as timestamp,couldn’t handle NULL values in PULocationID,DOLocationID'},\n",
       "   {'text': 'When accessing Looker Studio through the Google Cloud Project console, you may be prompted to subscribe to the Pro version and receive the following errors:\\nInstead, navigate to https://lookerstudio.google.com/navigation/reporting which will take you to the free version.',\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': 'Google Looker Studio - you have used up your 30-day trial'},\n",
       "   {'text': 'Ans: Dbt provides a mechanism called \"ref\" to manage dependencies between models. By referencing other models using the \"ref\" keyword in SQL, dbt automatically understands the dependencies and ensures the correct execution order.\\nLoading FHV Data goes into slumber using Mage?\\nTry loading the data using jupyter notebooks in a local environment. There might be bandwidth issues with Mage.\\nLoad the data into a pandas dataframe using the urls, make necessary transformations, upload the gcp bucket / alternatively download the parquet/csv files locally and then upload to GCP manually.\\nRegion Mismatch in DBT and BigQuery\\nIf you are using the datasets copied into BigQuery from BigQuery public datasets, the region will be set as US by default and hence it is much easier to set your dbt profile location as US while transforming the tables and views. \\nYou can change the location as follows:',\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': 'How does dbt handle dependencies between models?'},\n",
       "   {'text': \"Use the PostgreSQL COPY FROM feature that is compatible with csv files\\nCOPY table_name [ ( column_name [, ...] ) ]\\nFROM { 'filename' | PROGRAM 'command' | STDIN }\\n[ [ WITH ] ( option [, ...] ) ]\\n[ WHERE condition ]\",\n",
       "    'section': 'Module 4: analytics engineering with dbt',\n",
       "    'question': 'What is the fastest way to upload taxi data to dbt-postgres?'},\n",
       "   {'text': 'Update the line:\\nWith:',\n",
       "    'section': 'Module 5: pyspark',\n",
       "    'question': 'When configuring the profiles.yml file for dbt-postgres with jinja templates with environment variables, I\\'m getting \"Credentials in profile \"PROFILE_NAME\", target: \\'dev\\', invalid: \\'5432\\'is not of type \\'integer\\''},\n",
       "   {'text': 'Install SDKMAN:\\ncurl -s \"https://get.sdkman.io\" | bash\\nsource \"$HOME/.sdkman/bin/sdkman-init.sh\"\\nUsing SDKMAN, install Java 11 and Spark 3.3.2:\\nsdk install java 11.0.22-tem\\nsdk install spark 3.3.2\\nOpen a new terminal or run the following in the same shell:\\nsource \"$HOME/.sdkman/bin/sdkman-init.sh\"\\nVerify the locations and versions of Java and Spark that were installed:\\necho $JAVA_HOME\\njava -version\\necho $SPARK_HOME\\nspark-submit --version',\n",
       "    'section': 'Module 5: pyspark',\n",
       "    'question': 'Setting up Java and Spark (with PySpark) on Linux (Alternative option using SDKMAN)'},\n",
       "   {'text': 'If you’re seriously struggling to set things up \"locally\" (here locally meaning non/partly-managed environment like own laptop, a VM or Codespaces) you can use the following guide to use Spark in Google Colab:\\nhttps://medium.com/gitconnected/launch-spark-on-google-colab-and-connect-to-sparkui-342cad19b304\\nStarter notebook:\\nhttps://github.com/aaalexlit/medium_articles/blob/main/Spark_in_Colab.ipynb\\nIt’s advisable to spend some time setting things up locally rather than jumping right into this solution.',\n",
       "    'section': 'Module 5: pyspark',\n",
       "    'question': 'PySpark - Setting Spark up in Google Colab'},\n",
       "   {'text': 'If after installing Java (either jdk or openjdk), Hadoop and Spark, and setting the corresponding environment variables you find the following error when spark-shell is run at CMD:\\njava.lang.IllegalAccessError: class org.apache.spark.storage.StorageUtils$ (in unnamed module @0x3c947bc5) cannot access class sun.nio.ch.DirectBuffer (in module java.base) because module java.base does not export sun.nio.ch to unnamed\\nmodule @0x3c947bc5\\nSolution: Java 17 or 19 is not supported by Spark. Spark 3.x: requires Java 8/11/16. Install Java 11 from the website provided in the windows.md setup file.',\n",
       "    'section': 'Module 5: pyspark',\n",
       "    'question': 'Spark-shell: unable to load native-hadoop library for platform - Windows'},\n",
       "   {'text': 'I found this error while executing the user defined function in Spark (crazy_stuff_udf). I am working on Windows and using conda. After following the setup instructions, I found that the PYSPARK_PYTHON environment variable was not set correctly, given that conda has different python paths for each environment.\\nSolution:\\npip install findspark on the command line inside proper environment\\nAdd to the top of the script\\nimport findspark\\nfindspark.init()',\n",
       "    'section': 'Module 5: pyspark',\n",
       "    'question': 'PySpark - Python was not found; run without arguments to install from the Microsoft Store, or disable this shortcut from Settings > Manage App Execution Aliases.'},\n",
       "   {'text': 'This is because Python 3.11 has some inconsistencies with such an old version of Spark. The solution is a downgrade in the Python version. Python 3.9 using a conda environment takes care of it. Or install newer PySpark >= 3.5.1 works for me (Ella) [source].',\n",
       "    'section': 'Module 5: pyspark',\n",
       "    'question': 'PySpark - TypeError: code() argument 13 must be str, not int  , while executing `import pyspark`  (Windows/ Spark 3.0.3 - Python 3.11)'},\n",
       "   {'text': 'If anyone is a Pythonista or becoming one (which you will essentially be one along this journey), and desires to have all python dependencies under same virtual environment (e.g. conda) as done with prefect and previous exercises, simply follow these steps\\nInstall OpenJDK 11,\\non MacOS: $ brew install java11\\nAdd export PATH=\"/opt/homebrew/opt/openjdk@11/bin:$PATH\"\\nto ~/.bashrc or ~/zshrc\\nActivate working environment (by pipenv / poetry / conda)\\nRun $ pip install pyspark\\nWork with exercises as normal\\nAll default commands of spark will be also available at shell session under activated enviroment.\\nHope this can help!\\nP.s. you won’t need findspark to firstly initialize.\\nPy4J - Py4JJavaError: An error occurred while calling (...)  java.net.ConnectException: Connection refused: no further information;\\nIf you\\'re getting `Py4JavaError` with a generic root cause, such as the described above (Connection refused: no further information). You\\'re most likely using incompatible versions of the JDK or Python with Spark.\\nAs of the current latest Spark version (3.5.0), it supports JDK 8 / 11 / 17. All of which can be easily installed with SDKMan! on macOS or Linux environments\\n\\n$ sdk install java 17.0.10-librca\\n$ sdk install spark 3.5.0\\n$ sdk install hadoop 3.3.5\\nAs PySpark 3.5.0 supports Python 3.8+ make sure you\\'re setting up your virtualenv with either 3.8 / 3.9 / 3.10 / 3.11 (Most importantly avoid using 3.12 for now as not all libs in the data-science/engineering ecosystem are fully package for that)\\n\\n\\n$ conda create -n ENV_NAME python=3.11\\n$ conda activate ENV_NAME\\n$ pip install pyspark==3.5.0\\nThis setup makes installing `findspark` and the likes of it unnecessary. Happy coding.\\nPy4J - Py4JJavaError: An error occurred while calling o54.parquet. Or any kind of Py4JJavaError that show up after run df.write.parquet(\\'zones\\')(On window)\\nThis assume you already correctly set up the PATH in the nano ~/.bashrc\\nHere my\\nexport JAVA_HOME=\"/c/tools/jdk-11.0.21\"\\nexport PATH=\"${JAVA_HOME}/bin:${PATH}\"\\nexport HADOOP_HOME=\"/c/tools/hadoop-3.2.0\"\\nexport PATH=\"${HADOOP_HOME}/bin:${PATH}\"\\nexport SPARK_HOME=\"/c/tools/spark-3.3.2-bin-hadoop3\"\\nexport PATH=\"${SPARK_HOME}/bin:${PATH}\"\\nexport PYTHONPATH=\"${SPARK_HOME}/python/:$PYTHONPATH\"\\nexport PYTHONPATH=\"${SPARK_HOME}spark-3.5.1-bin-hadoop3py4j-0.10.9.5-src.zip:$PYTHONPATH\"\\nYou also need to add environment variables correctly which paths to java jdk, spark and hadoop through\\nGo to Stephenlaye2/winutils3.3.0: winutils.exe hadoop.dll and hdfs.dll binaries for hadoop windows (github.com), download the right winutils for hadoop-3.2.0. Then create a new folder,bin and put every thing in side to make a /c/tools/hadoop-3.2.0/bin(You might not need to do this, but after testing it without the /bin I could not make it to work)\\nThen follow the solution in this video: How To Resolve Issue with Writing DataFrame to Local File | winutils | msvcp100.dll (youtube.com)\\nRemember to restart IDE and computer, After the error An error occurred while calling o54.parquet.  is fixed but new errors like o31.parquet. Or o35.parquet. appear.',\n",
       "    'section': 'Module 5: pyspark',\n",
       "    'question': 'Java+Spark - Easy setup with miniconda env (worked on MacOS)'},\n",
       "   {'text': 'After installing all including pyspark (and it is successfully imported), but then running this script on the jupyter notebook\\nimport pyspark\\nfrom pyspark.sql import SparkSession\\nspark = SparkSession.builder \\\\\\n.master(\"local[*]\") \\\\\\n.appName(\\'test\\') \\\\\\n.getOrCreate()\\ndf = spark.read \\\\\\n.option(\"header\", \"true\") \\\\\\n.csv(\\'taxi+_zone_lookup.csv\\')\\ndf.show()\\nit gives the error:\\nRuntimeError: Java gateway process exited before sending its port number\\n✅The solution (for me) was:\\npip install findspark on the command line and then\\nAdd\\nimport findspark\\nfindspark.init()\\nto the top of the script.\\nAnother possible solution is:\\nCheck that pyspark is pointing to the correct location.\\nRun pyspark.__file__. It should be list /home/<your user name>/spark/spark-3.0.3-bin-hadoop3.2/python/pyspark/__init__.py if you followed the videos.\\nIf it is pointing to your python site-packages remove the pyspark directory there and check that you have added the correct exports to you .bashrc file and that there are not any other exports which might supersede the ones provided in the course content.\\nTo add to the solution above, if the errors persist in regards to setting the correct path for spark,  an alternative solution for permanent path setting solve the error is  to set environment variables on system and user environment variables following this tutorial: Install Apache PySpark on Windows PC | Apache Spark Installation Guide\\nOnce everything is installed, skip to 7:14 to set up environment variables. This allows for the environment variables to be set permanently.',\n",
       "    'section': 'Module 5: pyspark',\n",
       "    'question': 'lsRuntimeError: Java gateway process exited before sending its port number'},\n",
       "   {'text': 'Even after installing pyspark correctly on linux machine (VM ) as per course instructions, faced a module not found error in jupyter notebook .\\nThe solution which worked for me(use following in jupyter notebook) :\\n!pip install findspark\\nimport findspark\\nfindspark.init()\\nThereafter , import pyspark and create spark contex<<t as usual\\nNone of the solutions above worked for me till I ran !pip3 install pyspark instead !pip install pyspark.\\nFilter based on conditions based on multiple columns\\nfrom pyspark.sql.functions import col\\nnew_final.filter((new_final.a_zone==\"Murray Hill\") & (new_final.b_zone==\"Midwood\")).show()\\nKrishna Anand',\n",
       "    'section': 'Module 5: pyspark',\n",
       "    'question': 'Module Not Found Error in Jupyter Notebook .'},\n",
       "   {'text': 'You need to look for the Py4J file and note the version of the filename. Once you know the version, you can update the export command accordingly, this is how you check yours:\\n` ls ${SPARK_HOME}/python/lib/ ` and then you add it in the export command, mine was:\\nexport PYTHONPATH=”${SPARK_HOME}/python/lib/Py4J-0.10.9.5-src.zip:${PYTHONPATH}”\\nMake sure that the version under `${SPARK_HOME}/python/lib/` matches the filename of py4j or you will encounter `ModuleNotFoundError: No module named \\'py4j\\'` while executing `import pyspark`.\\nFor instance, if the file under `${SPARK_HOME}/python/lib/` was `py4j-0.10.9.3-src.zip`.\\nThen the export PYTHONPATH statement above should be changed to `export PYTHONPATH=\"${SPARK_HOME}/python/lib/py4j-0.10.9.3-src.zip:$PYTHONPATH\"` appropriately.\\nAdditionally, you can check for the version of ‘py4j’ of the spark you’re using from here and update as mentioned above.\\n~ Abhijit Chakraborty: Sometimes, even with adding the correct version of py4j might not solve the problem. Simply run pip install py4j and problem should be resolved.',\n",
       "    'section': 'Module 5: pyspark',\n",
       "    'question': \"Py4JJavaError - ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`\"},\n",
       "   {'text': 'If below does not work, then download the latest available py4j version with\\nconda install -c conda-forge py4j\\nTake care of the latest version number in the website to replace appropriately.\\nNow add\\nexport PYTHONPATH=\"${SPARK_HOME}/python/:$PYTHONPATH\"\\nexport PYTHONPATH=\"${SPARK_HOME}/python/lib/py4j-0.10.9.7-src.zip:$PYTHONPATH\"\\nin your  .bashrc file.',\n",
       "    'section': 'Module 5: pyspark',\n",
       "    'question': \"Py4J Error - ModuleNotFoundError: No module named 'py4j' (Solve with latest version)\"},\n",
       "   {'text': 'Even after we have exported our paths correctly you may find that  even though Jupyter is installed you might not have Jupyter Noteboopgak for one reason or another. Full instructions are found here (for my walkthrough) or here (where I got the original instructions from) but are included below. These instructions include setting up a virtual environment (handy if you are on your own machine doing this and not a VM):\\nFull steps:\\nUpdate and upgrade packages:\\nsudo apt update && sudo apt -y upgrade\\nInstall Python:\\nsudo apt install python3-pip python3-dev\\nInstall Python virtualenv:\\nsudo -H pip3 install --upgrade pip\\nsudo -H pip3 install virtualenv\\nCreate a Python Virtual Environment:\\nmkdir notebook\\ncd notebook\\nvirtualenv jupyterenv\\nsource jupyterenv/bin/activate\\nInstall Jupyter Notebook:\\npip install jupyter\\nRun Jupyter Notebook:\\njupyter notebook',\n",
       "    'section': 'Module 5: pyspark',\n",
       "    'question': 'Exception: Jupyter command `jupyter-notebook` not found.'},\n",
       "   {'text': 'Code executed:\\ndf = spark.read.parquet(pq_path)\\n… some operations on df …\\ndf.write.parquet(pq_path, mode=\"overwrite\")\\njava.io.FileNotFoundException: File file:/home/xxx/code/data/pq/fhvhv/2021/02/part-00021-523f9ad5-14af-4332-9434-bdcb0831f2b7-c000.snappy.parquet does not exist\\nThe problem is that Sparks performs lazy transformations, so the actual action that trigger the job is df.write, which does delete the parquet files that is trying to read (mode=”overwrite”)\\n✅Solution: Write to a different directorydf\\ndf.write.parquet(pq_path_temp, mode=\"overwrite\")',\n",
       "    'section': 'Module 5: pyspark',\n",
       "    'question': 'Error java.io.FileNotFoundException'},\n",
       "   {'text': 'You need to create the Hadoop /bin directory manually and add the downloaded files in there, since the shell script provided for Windows installation just puts them in /c/tools/hadoop-3.2.0/ .',\n",
       "    'section': 'Module 5: pyspark',\n",
       "    'question': 'Hadoop - FileNotFoundException: Hadoop bin directory does not exist , when trying to write (Windows)'},\n",
       "   {'text': 'Actually Spark SQL is one independent “type” of SQL - Spark SQL.\\nThe several SQL providers are very similar:\\nSELECT [attributes]\\nFROM [table]\\nWHERE [filter]\\nGROUP BY [grouping attributes]\\nHAVING [filtering the groups]\\nORDER BY [attribute to order]\\n(INNER/FULL/LEFT/RIGHT) JOIN [table2]\\nON [attributes table joining table2] (...)\\nWhat differs the most between several SQL providers are built-in functions.\\nFor Built-in Spark SQL function check this link: https://spark.apache.org/docs/latest/api/sql/index.html\\nExtra information on SPARK SQL :\\nhttps://databricks.com/glossary/what-is-spark-sql#:~:text=Spark%20SQL%20is%20a%20Spark,on%20existing%20deployments%20and%20data.',\n",
       "    'section': 'Module 5: pyspark',\n",
       "    'question': 'Which type of SQL is used in Spark? Postgres? MySQL? SQL Server?'},\n",
       "   {'text': \"✅Solution: I had two notebooks running, and the one I wanted to look at had opened a port on localhost:4041.\\nIf a port is in use, then Spark uses the next available port number. It can be even 4044. Clean up after yourself when a port does not work or a container does not run.\\nYou can run spark.sparkContext.uiWebUrl\\nand result will be some like\\n'http://172.19.10.61:4041'\",\n",
       "    'section': 'Module 5: pyspark',\n",
       "    'question': 'The spark viewer on localhost:4040 was not showing the current run'},\n",
       "   {'text': '✅Solution: replace Java Developer Kit 11 with Java Developer Kit 8.\\nJava - RuntimeError: Java gateway process exited before sending its port number\\nShows java_home is not set on the notebook log\\nhttps://sparkbyexamples.com/pyspark/pyspark-exception-java-gateway-process-exited-before-sending-the-driver-its-port-number/\\nhttps://twitter.com/drkrishnaanand/status/1765423415878463839',\n",
       "    'section': 'Module 5: pyspark',\n",
       "    'question': 'Java - java.lang.NoSuchMethodError: sun.nio.ch.DirectBuffer.cleaner()Lsun/misc/Cleaner Error during repartition call (conda pyspark installation)'},\n",
       "   {'text': '✅I got it working using `gcs-connector-hadoop-2.2.5-shaded.jar` and Spark 3.1\\nI also added the google_credentials.json and .p12 to auth with gcs. These files are downloadable from GCP Service account.\\nTo create the SparkSession:\\nspark = SparkSession.builder.master(\\'local[*]\\') \\\\\\n.appName(\\'spark-read-from-bigquery\\') \\\\\\n.config(\\'BigQueryProjectId\\',\\'razor-project-xxxxxxx) \\\\\\n.config(\\'BigQueryDatasetLocation\\',\\'de_final_data\\') \\\\\\n.config(\\'parentProject\\',\\'razor-project-xxxxxxx) \\\\\\n.config(\"google.cloud.auth.service.account.enable\", \"true\") \\\\\\n.config(\"credentialsFile\", \"google_credentials.json\") \\\\\\n.config(\"GcpJsonKeyFile\", \"google_credentials.json\") \\\\\\n.config(\"spark.driver.memory\", \"4g\") \\\\\\n.config(\"spark.executor.memory\", \"2g\") \\\\\\n.config(\"spark.memory.offHeap.enabled\",True) \\\\\\n.config(\"spark.memory.offHeap.size\",\"5g\") \\\\\\n.config(\\'google.cloud.auth.service.account.json.keyfile\\', \"google_credentials.json\") \\\\\\n.config(\"fs.gs.project.id\", \"razor-project-xxxxxxx\") \\\\\\n.config(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\\\\n.config(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\") \\\\\\n.getOrCreate()',\n",
       "    'section': 'Module 5: pyspark',\n",
       "    'question': 'Spark fails when reading from BigQuery and using `.show()` on `SELECT` queries'},\n",
       "   {'text': 'While creating a SparkSession using the config spark.jars.packages as com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.23.2\\nspark = SparkSession.builder.master(\\'local\\').appName(\\'bq\\').config(\"spark.jars.packages\", \"com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.23.2\").getOrCreate()\\nautomatically downloads the required dependency jars and configures the connector, removing the need to manage this dependency. More details available here',\n",
       "    'section': 'Module 5: pyspark',\n",
       "    'question': 'Spark BigQuery connector Automatic configuration'},\n",
       "   {'text': 'Link to Slack Thread : has anyone figured out how to read from GCP data lake instead of downloading all the taxi data again?\\nThere’s a few extra steps to go into reading from GCS with PySpark\\n1.)  IMPORTANT: Download the Cloud Storage connector for Hadoop here: https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage#clusters\\nAs the name implies, this .jar file is what essentially connects PySpark with your GCS\\n2.) Move the .jar file to your Spark file directory. I installed Spark using homebrew on my MacOS machine and I had to create a /jars directory under \"/opt/homebrew/Cellar/apache-spark/3.2.1/ (where my spark dir is located)\\n3.) In your Python script, there are a few extra classes you’ll have to import:\\nimport pyspark\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.conf import SparkConf\\nfrom pyspark.context import SparkContext\\n4.) You must set up your configurations before building your SparkSession. Here’s my code snippet:\\nconf = SparkConf() \\\\\\n.setMaster(\\'local[*]\\') \\\\\\n.setAppName(\\'test\\') \\\\\\n.set(\"spark.jars\", \"/opt/homebrew/Cellar/apache-spark/3.2.1/jars/gcs-connector-hadoop3-latest.jar\") \\\\\\n.set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\\\\n.set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", \"path/to/google_credentials.json\")\\nsc = SparkContext(conf=conf)\\nsc._jsc.hadoopConfiguration().set(\"fs.AbstractFileSystem.gs.impl\",  \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\\nsc._jsc.hadoopConfiguration().set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\\nsc._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.json.keyfile\", \"path/to/google_credentials.json\")\\nsc._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.enable\", \"true\")\\n5.) Once you run that, build your SparkSession with the new parameters we’d just instantiated in the previous step:\\nspark = SparkSession.builder \\\\\\n.config(conf=sc.getConf()) \\\\\\n.getOrCreate()\\n6.) Finally, you’re able to read your files straight from GCS!\\ndf_green = spark.read.parquet(\"gs://{BUCKET}/green/202*/\")',\n",
       "    'section': 'Module 5: pyspark',\n",
       "    'question': 'Spark Cloud Storage connector'},\n",
       "   {'text': 'from pyarrow.parquet import ParquetFile\\npf = ParquetFile(\\'fhvhv_tripdata_2021-01.parquet\\')\\n#pyarrow builds tables, not dataframes\\ntbl_small = next(pf.iter_batches(batch_size = 1000))\\n#this function converts the table to a dataframe of manageable size\\ndf = tbl_small.to_pandas()\\nAlternatively without PyArrow:\\ndf = spark.read.parquet(\\'fhvhv_tripdata_2021-01.parquet\\')\\ndf1 = df.sort(\\'DOLocationID\\').limit(1000)\\npdf = df1.select(\"*\").toPandas()\\ngcsu',\n",
       "    'section': 'Module 5: pyspark',\n",
       "    'question': 'How can I read a small number of rows from the parquet file directly?'},\n",
       "   {'text': 'Probably you’ll encounter this if you followed the video ‘5.3.1 - First Look at Spark/PySpark’ and used the parquet file from the TLC website (csv was used in the video).\\nWhen defining the schema, the PULocation and DOLocationID are defined as IntegerType. This will cause an error because the Parquet file is INT64 and you’ll get an error like:\\nParquet column cannot be converted in file [...] Column [...] Expected: int, Found: INT64\\nChange the schema definition from IntegerType to LongType and it should work',\n",
       "    'section': 'Module 5: pyspark',\n",
       "    'question': 'DataType error when creating Spark DataFrame with a specified schema?'},\n",
       "   {'text': 'df_finalx=df_finalw.select([col(x).alias(x.replace(\" \",\"\")) for x in df_finalw.columns])\\nKrishna Anand',\n",
       "    'section': 'Module 5: pyspark',\n",
       "    'question': 'Remove white spaces from column names in Pyspark'},\n",
       "   {'text': 'This error comes up on the Spark video 5.3.1 - First Look at Spark/PySpark,\\nbecause as at the creation of the video, 2021 data was the most recent which utilised csv files but as at now its parquet.\\nSo when you run the command spark.createDataFrame(df1_pandas).show(),\\nYou get the Attribute error. This is caused by the pandas version 2.0.0 which seems incompatible with Spark 3.3.2, so to fix it you have to downgrade pandas to 1.5.3 using the command pip install -U pandas==1.5.3\\nAnother option is adding the following after importing pandas, if one does not want to downgrade pandas version (source) :\\npd.DataFrame.iteritems = pd.DataFrame.items\\nNote that this problem is solved with Spark versions from 3.4.1',\n",
       "    'section': 'Module 5: pyspark',\n",
       "    'question': \"AttributeError: 'DataFrame' object has no attribute 'iteritems'\"},\n",
       "   {'text': 'Another alternative is to install pandas 2.0.1 (it worked well as at the time of writing this), and it is compatible with Pyspark 3.5.1. Make sure to add or edit your environment variable like this:\\nexport SPARK_HOME=\"${HOME}/spark/spark-3.5.1-bin-hadoop3\"\\nexport PATH=\"${SPARK_HOME}/bin:${PATH}\"',\n",
       "    'section': 'Module 5: pyspark',\n",
       "    'question': \"AttributeError: 'DataFrame' object has no attribute 'iteritems'\"},\n",
       "   {'text': 'Open a CMD terminal in administrator mode\\ncd %SPARK_HOME%\\nStart a master node: bin\\\\spark-class org.apache.spark.deploy.master.Master\\nStart a worker node: bin\\\\spark-class org.apache.spark.deploy.worker.Worker spark://<master_ip>:<port> --host <IP_ADDR>\\nbin/spark-class org.apache.spark.deploy.worker.Worker spark://localhost:7077 --host <IP_ADDR>\\nspark://<master_ip>:<port>: copy the address from the previous command, in my case it was spark://localhost:7077\\nUse --host <IP_ADDR> if you want to run the worker on a different machine. For now leave it empty.\\nNow you can access Spark UI through localhost:8080\\nHomework for Module 5:\\nDo not refer to the homework file located under /05-batch/code/. The correct file is located under\\nhttps://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/cohorts/2024/05-batch/homework.md',\n",
       "    'section': 'Module 5: pyspark',\n",
       "    'question': 'Spark Standalone Mode on Windows'},\n",
       "   {'text': 'You can either type the export command every time you run a new session, add it to the .bashrc/ which you can find in /home or run this command at the beginning of your homebook:\\nimport findspark\\nfindspark.init()',\n",
       "    'section': 'Module 5: pyspark',\n",
       "    'question': 'Export PYTHONPATH command in linux is temporary'},\n",
       "   {'text': 'I solved this issue: unzip the file with:\\nf\\nbefore creating head.csv',\n",
       "    'section': 'Module 5: pyspark',\n",
       "    'question': 'Compressed file ended before the end-of-stream marker was reached'},\n",
       "   {'text': 'In the code along from Video 5.3.3 Alexey downloads the CSV files from the NYT website and gzips them in their bash script. If we now (2023) follow along but download the data from the GH course Repo, it will already be zippes as csv.gz files. Therefore we zip it again if we follow the code from the video exactly. This then leads to gibberish outcome when we then try to cat the contents or count the lines with zcat, because the file is zipped twitch and zcat only unzips it once.\\n✅solution: do not gzip the files downloaded from the course repo. Just wget them and save them as they are as csv.gz files. Then the zcat command and the showSchema command will also work\\nURL=\"${URL_PREFIX}/${TAXI_TYPE}/${TAXI_TYPE}_tripdata_${YEAR}-${FMONTH}.csv.gz\"\\nLOCAL_PREFIX=\"data/raw/${TAXI_TYPE}/${YEAR}/${FMONTH}\"\\nLOCAL_FILE=\"${TAXI_TYPE}_tripdata_${YEAR}_${FMONTH}.csv.gz\"\\nLOCAL_PATH=\"${LOCAL_PREFIX}/${LOCAL_FILE}\"\\necho \"downloading ${URL} to ${LOCAL_PATH}\"\\nmkdir -p ${LOCAL_PREFIX}\\nwget ${URL} -O ${LOCAL_PATH}\\necho \"compressing ${LOCAL_PATH}\"\\n# gzip ${LOCAL_PATH} <- uncomment this line',\n",
       "    'section': 'Module 5: pyspark',\n",
       "    'question': 'Compression Error: zcat output is gibberish, seems like still compressed'},\n",
       "   {'text': 'Occurred while running : spark.createDataFrame(df_pandas).show()\\nThis error is usually due to the python version, since spark till date of 2 march 2023 doesn’t support python 3.11, try creating a new env with python version 3.8 and then run this command.\\nOn the virtual machine, you can create a conda environment (here called myenv) with python 3.10 installed:\\nconda create -n myenv python=3.10 anaconda\\nThen you must run conda activate myenv to run python 3.10. Otherwise you’ll still be running version 3.11. You can deactivate by typing conda deactivate.',\n",
       "    'section': 'Module 5: pyspark',\n",
       "    'question': 'PicklingError: Could not serialise object: IndexError: tuple index out of range.'},\n",
       "   {'text': 'Make sure you have your credentials of your GCP in your VM under the location defined in the script.',\n",
       "    'section': 'Module 5: pyspark',\n",
       "    'question': 'Connecting from local Spark to GCS - Spark does not find my google credentials as shown in the video?'},\n",
       "   {'text': 'To run spark in docker setup\\n1. Build bitnami spark docker\\na. clone bitnami repo using command\\ngit clone https://github.com/bitnami/containers.git\\n(tested on commit 9cef8b892d29c04f8a271a644341c8222790c992)\\nb. edit file `bitnami/spark/3.3/debian-11/Dockerfile` and update java and spark version as following\\n\"python-3.10.10-2-linux-${OS_ARCH}-debian-11\" \\\\\\n\"java-17.0.5-8-3-linux-${OS_ARCH}-debian-11\" \\\\\\nreference: https://github.com/bitnami/containers/issues/13409\\nc. build docker image by navigating to above directory and running docker build command\\nnavigate cd bitnami/spark/3.3/debian-11/\\nbuild command docker build -t spark:3.3-java-17 .\\n2. run docker compose\\nusing following file\\n```yaml docker-compose.yml\\nversion: \\'2\\'\\nservices:\\nspark:\\nimage: spark:3.3-java-17\\nenvironment:\\n- SPARK_MODE=master\\n- SPARK_RPC_AUTHENTICATION_ENABLED=no\\n- SPARK_RPC_ENCRYPTION_ENABLED=no\\n- SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no\\n- SPARK_SSL_ENABLED=no\\nvolumes:\\n- \"./:/home/jovyan/work:rw\"\\nports:\\n- \\'8080:8080\\'\\n- \\'7077:7077\\'\\nspark-worker:\\nimage: spark:3.3-java-17\\nenvironment:\\n- SPARK_MODE=worker\\n- SPARK_MASTER_URL=spark://spark:7077\\n- SPARK_WORKER_MEMORY=1G\\n- SPARK_WORKER_CORES=1\\n- SPARK_RPC_AUTHENTICATION_ENABLED=no\\n- SPARK_RPC_ENCRYPTION_ENABLED=no\\n- SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no\\n- SPARK_SSL_ENABLED=no\\nvolumes:\\n- \"./:/home/jovyan/work:rw\"\\nports:\\n- \\'8081:8081\\'\\nspark-nb:\\nimage: jupyter/pyspark-notebook:java-17.0.5\\nenvironment:\\n- SPARK_MASTER_URL=spark://spark:7077\\nvolumes:\\n- \"./:/home/jovyan/work:rw\"\\nports:\\n- \\'8888:8888\\'\\n- \\'4040:4040\\'\\n```\\nrun command to deploy docker compose\\ndocker-compose up\\nAccess jupyter notebook using link logged in docker compose logs\\nSpark master url is spark://spark:7077',\n",
       "    'section': 'Module 5: pyspark',\n",
       "    'question': 'Spark docker-compose setup'},\n",
       "   {'text': 'To do this\\npip install gcsfs,\\nThereafter copy the uri path to the file and use \\ndf = pandas.read_csc(gs://path)',\n",
       "    'section': 'Module 5: pyspark',\n",
       "    'question': 'How do you read data stored in gcs on pandas with your local computer?'},\n",
       "   {'text': 'Error:\\nspark.createDataFrame(df_pandas).schema\\nTypeError: field Affiliated_base_number: Can not merge type <class \\'pyspark.sql.types.StringType\\'> and <class \\'pyspark.sql.types.DoubleType\\'>\\nSolution:\\nAffiliated_base_number is a mix of letters and numbers (you can check this with a preview of the table), so it cannot be set to DoubleType (only for double-precision numbers). The suitable type would be StringType. Spark  inferSchema is more accurate than Pandas infer type method in this case. You can set it to  true  while reading the csv, so you don’t have to take out any data from your dataset. Something like this can help:\\ndf = spark.read \\\\\\n.options(\\nheader = \"true\", \\\\\\ninferSchema = \"true\", \\\\\\n) \\\\\\n.csv(\\'path/to/your/csv/file/\\')\\nSolution B:\\nIt\\'s because some rows in the affiliated_base_number are null and therefore it is assigned the datatype String and this cannot be converted to type Double. So if you really want to convert this pandas df to a pyspark df only take the  rows from the pandas df that are not null in the \\'Affiliated_base_number\\' column. Then you will be able to apply the pyspark function createDataFrame.\\n# Only take rows that have no null values\\npandas_df= pandas_df[pandas_df.notnull().all(1)]',\n",
       "    'section': 'Module 5: pyspark',\n",
       "    'question': 'TypeError when using spark.createDataFrame function on a pandas df'},\n",
       "   {'text': 'Default executor memory is 1gb. This error appeared when working with the homework dataset.\\nError: MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\\nScaling row group sizes to 95.00% for 8 writers\\nSolution:\\nIncrease the memory of the executor when creating the Spark session like this:\\nRemember to restart the Jupyter session (ie. close the Spark session) or the config won’t take effect.',\n",
       "    'section': 'Module 5: pyspark',\n",
       "    'question': 'MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory'},\n",
       "   {'text': 'Change the working directory to the spark directory:\\nif you have setup up your SPARK_HOME variable, use the following;\\ncd %SPARK_HOME%\\nif not, use the following;\\ncd <path to spark installation>\\nCreating a Local Spark Cluster\\nTo start Spark Master:\\nbin\\\\spark-class org.apache.spark.deploy.master.Master --host localhost\\nStarting up a cluster:\\nbin\\\\spark-class org.apache.spark.deploy.worker.Worker spark://localhost:7077 --host localhost',\n",
       "    'section': 'Module 5: pyspark',\n",
       "    'question': 'How to spark standalone cluster is run on windows OS'},\n",
       "   {'text': 'I added PYTHONPATH, JAVA_HOME and SPARK_HOME to ~/.bashrc, import pyspark worked ok in iPython in terminal, but couldn’t be found in .ipynb opened in VS Code\\nAfter adding new lines to ~/.bashrc, need to restart the shell to activate the new lines, do either\\nsource ~/.bashrc\\nexec bash\\nInstead of configuring paths in ~/.bashrc, I created .env file in the root of my workspace:',\n",
       "    'section': 'Module 5: pyspark',\n",
       "    'question': 'Env variables set in ~/.bashrc are not loaded to Jupyter in VS Code'},\n",
       "   {'text': 'I don’t use visual studio, so I did it the old fashioned way: ssh -L 8888:localhost:8888 <my user>@<VM IP> (replace user and IP with the ones used by the GCP VM, e.g. : ssh -L 8888:localhost:8888 myuser@34.140.188.1',\n",
       "    'section': 'Module 5: pyspark',\n",
       "    'question': 'How to port forward outside VS Code'},\n",
       "   {'text': 'If you are doing wc -l fhvhv_tripdata_2021-01.csv.gz  with the gzip file as the file argument, you will get a different result, obviously! Since the file is compressed.\\nUnzip the file and then do wc -l fhvhv_tripdata_2021-01.csv to get the right results.',\n",
       "    'section': 'Module 5: pyspark',\n",
       "    'question': '“wc -l” is giving a different result then shown in the video'},\n",
       "   {'text': 'when trying to:\\nURL=\"spark://$HOSTNAME:7077\"\\nspark-submit \\\\\\n--master=\"{$URL}\" \\\\\\n06_spark_sql.py \\\\\\n--input_green=data/pq/green/2021/*/ \\\\\\n--input_yellow=data/pq/yellow/2021/*/ \\\\\\n--output=data/report-2021\\nand you get errors like the following (SUMMARIZED):\\nWARN Utils: Your hostname, <HOSTNAME> resolves to a loopback address..\\nWARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address Setting default log level to \"WARN\".\\nException in thread \"main\" org.apache.spark.SparkException: Master must either be yarn or start with spark, mesos, k8s, or local at …\\nTry replacing --master=\"{$URL}\"\\nwith --master=$URL (edited)\\nExtra edit for spark version 3.4.2 - if encountering:\\n`Error: Unrecognized option: --master=`\\n→ Replace `--master=\"{$URL}\"` with  `--master \"${URL}\"`',\n",
       "    'section': 'Module 5: pyspark',\n",
       "    'question': '`spark-submit` errors'},\n",
       "   {'text': 'If you are seeing this (or similar) error when attempting to write to parquet, it is likely an issue with your path variables.\\nFor Windows, create a new User Variable “HADOOP_HOME” that points to your Hadoop directory. Then add “%HADOOP_HOME%\\\\bin” to the PATH variable.\\nAdditional tips can be found here: https://stackoverflow.com/questions/41851066/exception-in-thread-main-java-lang-unsatisfiedlinkerror-org-apache-hadoop-io',\n",
       "    'section': 'Module 5: pyspark',\n",
       "    'question': 'Hadoop - Exception in thread \"main\" java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z'},\n",
       "   {'text': \"Change the hadoop version to 3.0.1.Replace all the files in the local hadoop bin folder with the files in this repo:  winutils/hadoop-3.0.1/bin at master · cdarlint/winutils (github.com)\\nIf this does not work try to change other versions found in this repository.\\nFor more information please see this link: This version of %1 is not compatible with the version of Windows you're running · Issue #20 · cdarlint/winutils (github.com)\",\n",
       "    'section': 'Module 5: pyspark',\n",
       "    'question': 'Java.io.IOException. Cannot run program “C:\\\\hadoop\\\\bin\\\\winutils.exe”. CreateProcess error=216, This version of 1% is not compatible with the version of Windows you are using.'},\n",
       "   {'text': 'Fix is to set the flag like the error states. Get your project ID from your dashboard and set it like so:\\ngcloud dataproc jobs submit pyspark \\\\\\n--cluster=my_cluster \\\\\\n--region=us-central1 \\\\\\n--project=my-dtc-project-1010101 \\\\\\ngs://my-dtc-bucket-id/code/06_spark_sql.py\\n-- \\\\\\n…',\n",
       "    'section': 'Module 5: pyspark',\n",
       "    'question': 'Dataproc - ERROR: (gcloud.dataproc.jobs.submit.pyspark) The required property [project] is not currently set. It can be set on a per-command basis by re-running your command with the [--project] flag.'},\n",
       "   {'text': 'Go to %SPARK_HOME%\\\\bin\\nRun spark-class org.apache.spark.deploy.master.Master to run the master. This will give you a URL of the form spark://ip:port\\nRun spark-class org.apache.spark.deploy.worker.Worker spark://ip:port to run the worker. Make sure you use the URL you obtained in step 2.\\nCreate a new Jupyter notebook:\\nspark = SparkSession.builder \\\\\\n.master(\"spark://{ip}:7077\") \\\\\\n.appName(\\'test\\') \\\\\\n.getOrCreate()\\nCheck on Spark UI the master, worker and app.',\n",
       "    'section': 'Module 5: pyspark',\n",
       "    'question': 'Run Local Cluster Spark in Windows 10 with CMD'},\n",
       "   {'text': 'This occurs because you are not logged in “gcloud auth login” and maybe the project id is not settled. Then type in a terminal:\\ngcloud auth login\\nThis will open a tab in the browser, accept the terms, after that close the tab if you want. Then set the project is like:\\ngcloud config set project <YOUR PROJECT_ID>\\nThen you can run the command to upload the pq dir to a GCS Bucket:\\ngsutil -m cp -r pq/ <YOUR URI from gsutil>/pq',\n",
       "    'section': 'Module 5: pyspark',\n",
       "    'question': \"lServiceException: 401 Anonymous caller does not have storage.objects.list access to the Google Cloud Storage bucket. Permission 'storage.objects.list' denied on resource (or it may not exist).\"},\n",
       "   {'text': \"When submit a job, it might throw an error about Java in log panel within Dataproc. I changed the Versioning Control when I created a cluster, so it means that I delete the cluster and created a new one, and instead of choosing Debian-Hadoop-Spark, I switch to Ubuntu 20.02-Hadoop3.3-Spark3.3 for Versioning Control feature, the main reason to choose this is because I have the same Ubuntu version in mi laptop, I tried to find documentation to sustent this but unfortunately I couldn't nevertheless it works for me.\",\n",
       "    'section': 'Module 5: pyspark',\n",
       "    'question': 'py4j.protocol.Py4JJavaError  GCP'},\n",
       "   {'text': \"Use both repartition and coalesce, like so:\\ndf = df.repartition(6)\\ndf = df.coalesce(6)\\ndf.write.parquet('fhv/2019/10', mode='overwrite')\",\n",
       "    'section': 'Module 5: pyspark',\n",
       "    'question': 'Repartition the Dataframe to 6 partitions using df.repartition(6) - got 8 partitions instead'},\n",
       "   {'text': \"Possible solution - Try to forward the port using ssh cli instead of vs code.\\nRun > “ssh -L <local port>:<VM host/ip>:<VM port> <ssh hostname>”\\nssh hostname is the name you specified in the ~/.ssh/config file\\nIn case of Jupyter Notebook run\\n“ssh -L 8888:localhost:8888 gcp-vm”\\nfrom your local machine’s cli.\\nNOTE: If you logout from the session, the connection would break. Also while creating the spark session notice the block's log because sometimes it fails to run at 4040 and then switches to 4041.\\n~Abhijit Chakrborty: If you are having trouble accessing localhost ports from GCP VM consider adding the forwarding instructions to .ssh/config file as following:\\n```\\nHost <hostname>\\nHostname <external-gcp-ip>\\nUser xxxx\\nIdentityFile yyyy\\nLocalForward 8888 localhost:8888\\nLocalForward 8080 localhost:8080\\nLocalForward 5432 localhost:5432\\nLocalForward 4040 localhost:4040\\n```\\nThis should automatically forward all ports and will enable accessing localhost ports.\",\n",
       "    'section': 'Module 5: pyspark',\n",
       "    'question': 'Jupyter Notebook or SparkUI not loading properly at localhost after port forwarding from VS code?'},\n",
       "   {'text': '~ Abhijit Chakraborty\\n`sdk list java`  to check for available java sdk versions.\\n`sdk install java 11.0.22-amzn`  as  java-11.0.22-amzn was available for my codespace.\\nclick on Y if prompted to change the default java version.\\nCheck for java version using `java -version `.\\nIf working fine great, else `sdk default java 11.0.22-amzn` or whatever version you have installed.',\n",
       "    'section': 'Module 5: pyspark',\n",
       "    'question': 'Installing Java 11 on codespaces'},\n",
       "   {'text': 'Sometimes while creating a dataproc cluster on GCP, the following error is encountered.\\nSolution: As mentioned here, sometimes there might not be enough resources in the given region to allocate the request. Usually, gets freed up in a bit and one can create a cluster. – abhirup ghosh\\nSolution 2:  Changing the type of boot-disk from PD-Balanced to PD-Standard, in terraform, helped solve the problem.- Sundara Kumar Padmanabhan',\n",
       "    'section': 'Module 5: pyspark',\n",
       "    'question': \"Error: Insufficient 'SSD_TOTAL_GB' quota. Requested 500.0, available 470.0.\"},\n",
       "   {'text': \"Pyspark converts the difference of two TimestampType values to Python's native datetime.timedelta object. The timedelta object only stores the duration in terms of days, seconds, and microseconds. Each of the three units of time must be manually converted into hours in order to express the total duration between the two timestamps using only hours.\\nAnother way for achieving this is using the datediff (sql function). It receives this parameters\\nUpper Date: the closest date you have. For example dropoff_datetime\\nLower Date: the farthest date you have.  For example pickup_datetime\\nAnd the result is returned in terms of days, so you could multiply the result for 24 in order to get the hours.\",\n",
       "    'section': 'Module 5: pyspark',\n",
       "    'question': 'Homework - how to convert the time difference of two timestamps to hours'},\n",
       "   {'text': 'This version combination worked for me:\\nPySpark = 3.3.2\\nPandas = 1.5.3\\n\\nIf it still has an error,',\n",
       "    'section': 'Module 5: pyspark',\n",
       "    'question': 'PicklingError: Could not serialize object: IndexError: tuple index out of range'},\n",
       "   {'text': \"Run this before SparkSession\\nimport os\\nimport sys\\nos.environ['PYSPARK_PYTHON'] = sys.executable\\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\",\n",
       "    'section': 'Module 5: pyspark',\n",
       "    'question': 'Py4JJavaError: An error occurred while calling o180.showString. : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6.0 failed 1 times, most recent failure: Lost task 0.0 in stage 6.0 (TID 6) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.'},\n",
       "   {'text': \"import os\\nimport sys\\nos.environ['PYSPARK_PYTHON'] = sys.executable\\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\\nDataproc Pricing: https://cloud.google.com/dataproc/pricing#on_gke_pricing\",\n",
       "    'section': 'Module 5: pyspark',\n",
       "    'question': 'RuntimeError: Python in worker has different version 3.11 than that in driver 3.10, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.'},\n",
       "   {'text': 'Ans: No, you can submit a job to DataProc from your local computer by installing gsutil (https://cloud.google.com/storage/docs/gsutil_install) and configuring it. Then, you can execute the following command from your local computer.\\ngcloud dataproc jobs submit pyspark \\\\\\n--cluster=de-zoomcamp-cluster \\\\\\n--region=europe-west6 \\\\\\ngs://dtc_data_lake_de-zoomcamp-nytaxi/code/06_spark_sql.py \\\\\\n-- \\\\\\n--input_green=gs://dtc_data_lake_de-zoomcamp-nytaxi/pq/green/2020/*/ \\\\\\n--input_yellow=gs://dtc_data_lake_de-zoomcamp-nytaxi/pq/yellow/2020/*/ \\\\\\n--output=gs://dtc_data_lake_de-zoomcamp-nytaxi/report-2020 (edited)',\n",
       "    'section': 'Module 5: pyspark',\n",
       "    'question': 'Dataproc Qn: Is it essential to have a VM on GCP for running Dataproc and submitting jobs ?'},\n",
       "   {'text': \"AttributeError: 'DataFrame' object has no attribute 'iteritems'\\nthis is because the method inside the pyspark refers to a package that has been already deprecated\\n(https://stackoverflow.com/questions/76404811/attributeerror-dataframe-object-has-no-attribute-iteritems)\\nYou can do this code below, which is mentioned in the stackoverflow link above:\\nQ: DE Zoomcamp 5.6.3 - Setting up a Dataproc Cluster I cannot create a cluster and get this message. I tried many times as the FAQ said, but it didn't work. What can I do?\\nError\\nInsufficient 'SSD_TOTAL_GB' quota. Requested 500.0, available 250.0.\\nRequest ID: 17942272465025572271\\nA: The master and worker nodes are allocated a maximum of 250 GB of memory combined. In the configuration section, adhere to the following specifications:\\nMaster Node:\\nMachine type: n2-standard-2\\nPrimary disk size: 85 GB\\nWorker Node:\\nNumber of worker nodes: 2\\nMachine type: n2-standard-2\\nPrimary disk size: 80 GB\\nYou can allocate up to 82.5 GB memory for worker nodes, keeping in mind that the total memory allocated across all nodes cannot exceed 250 GB.\",\n",
       "    'section': 'Module 5: pyspark',\n",
       "    'question': 'In module 5.3.1, trying to run spark.createDataFrame(df_pandas).show() returns error'},\n",
       "   {'text': 'The MacOS setup instruction (https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/05-batch/setup/macos.md#installing-java) for setting the JAVA_HOME environment variable is for Intel-based Macs which have a default install location at /usr/local/. If you have an Apple Silicon mac, you will have to set JAVA_HOME to /opt/homebrew/, specifically in your .bashrc or .zshrc:\\nexport JAVA_HOME=\"/opt/homebrew/opt/openjdk/bin\"\\nexport PATH=\"$JAVA_HOME:$PATH\"\\nConfirm that your path was correctly set by running the command: which java\\nYou should expect to see the output:\\n/opt/homebrew/opt/openjdk/bin/java\\nReference: https://docs.brew.sh/Installation',\n",
       "    'section': 'Module 6: streaming with kafka',\n",
       "    'question': 'Setting JAVA_HOME with Homebrew on Apple Silicon'},\n",
       "   {'text': 'Check Docker Compose File:\\nEnsure that your docker-compose.yaml file is correctly configured with the necessary details for the \"control-center\" service. Check the service name, image name, ports, volumes, environment variables, and any other configurations required for the container to start.\\nOn Mac OSX 12.2.1 (Monterey) I could not start the kafka control center. I opened Docker Desktop and saw docker images still running from week 4, which I did not see when I typed “docker ps.” I deleted them in docker desktop and then had no problem starting up the kafka environment.',\n",
       "    'section': 'Module 6: streaming with kafka',\n",
       "    'question': 'Could not start docker image “control-center” from the docker-compose.yaml file.'},\n",
       "   {'text': \"Solution from Alexey: create a virtual environment and run requirements.txt and the python files in that environment.\\nTo create a virtual env and install packages (run only once)\\npython -m venv env\\nsource env/bin/activate\\npip install -r ../requirements.txt\\nTo activate it (you'll need to run it every time you need the virtual env):\\nsource env/bin/activate\\nTo deactivate it:\\ndeactivate\\nThis works on MacOS, Linux and Windows - but for Windows the path is slightly different (it's env/Scripts/activate)\\nAlso the virtual environment should be created only to run the python file. Docker images should first all be up and running.\",\n",
       "    'section': 'Module 6: streaming with kafka',\n",
       "    'question': 'Module “kafka” not found when trying to run producer.py'},\n",
       "   {'text': 'ImportError: DLL load failed while importing cimpl: The specified module could not be found\\nVerify Python Version:\\nMake sure you are using a compatible version of Python with the Avro library. Check the Python version and compatibility requirements specified by the Avro library documentation.\\n... you may have to load librdkafka-5d2e2910.dll in the code. Add this before importing avro:\\nfrom ctypes import CDLL\\nCDLL(\"C:\\\\\\\\Users\\\\\\\\YOUR_USER_NAME\\\\\\\\anaconda3\\\\\\\\envs\\\\\\\\dtcde\\\\\\\\Lib\\\\\\\\site-packages\\\\\\\\confluent_kafka.libs\\\\librdkafka-5d2e2910.dll\")\\nIt seems that the error may occur depending on the OS and python version installed.\\nALTERNATIVE:\\nImportError: DLL load failed while importing cimpl\\n✅SOLUTION: $env:CONDA_DLL_SEARCH_MODIFICATION_ENABLE=1 in Powershell.\\nYou need to set this DLL manually in Conda Env.\\nSource: https://githubhot.com/repo/confluentinc/confluent-kafka-python/issues/1186?page=2',\n",
       "    'section': 'Module 6: streaming with kafka',\n",
       "    'question': 'Error importing cimpl dll when running avro examples'},\n",
       "   {'text': \"✅SOLUTION: pip install confluent-kafka[avro].\\nFor some reason, Conda also doesn't include this when installing confluent-kafka via pip.\\nMore sources on Anaconda and confluent-kafka issues:\\nhttps://github.com/confluentinc/confluent-kafka-python/issues/590\\nhttps://github.com/confluentinc/confluent-kafka-python/issues/1221\\nhttps://stackoverflow.com/questions/69085157/cannot-import-producer-from-confluent-kafka\",\n",
       "    'section': 'Module 6: streaming with kafka',\n",
       "    'question': \"ModuleNotFoundError: No module named 'avro'\"},\n",
       "   {'text': 'If you get an error while running the command python3 stream.py worker\\nRun pip uninstall kafka-python\\nThen run pip install kafka-python==1.4.6\\nWhat is the use of  Redpanda ?\\nRedpanda: Redpanda is built on top of the Raft consensus algorithm and is designed as a high-performance, low-latency alternative to Kafka. It uses a log-centric architecture similar to Kafka but with different underlying principles.\\nRedpanda is a powerful, yet simple, and cost-efficient streaming data platform that is compatible with Kafka® APIs while eliminating Kafka complexity.',\n",
       "    'section': 'Module 6: streaming with kafka',\n",
       "    'question': 'Error while running python3 stream.py worker'},\n",
       "   {'text': 'Got this error because the docker container memory was exhausted. The dta file was upto 800MB but my docker container does not have enough memory to handle that.\\nSolution was to load the file in chunks with Pandas, then create multiple parquet files for each dat file I was processing. This worked smoothly and the issue was resolved.',\n",
       "    'section': 'Module 6: streaming with kafka',\n",
       "    'question': 'Negsignal:SIGKILL while converting dta files to parquet format'},\n",
       "   {'text': 'Copy the file found in the Java example: data-engineering-zoomcamp/week_6_stream_processing/java/kafka_examples/src/main/resources/rides.csv',\n",
       "    'section': 'Module 6: streaming with kafka',\n",
       "    'question': 'data-engineering-zoomcamp/week_6_stream_processing/python/resources/rides.csv is missing'},\n",
       "   {'text': 'tip:As the videos have low audio so I downloaded them and used VLC media player with putting the audio to the max 200% of original audio and the audio became quite good or try to use auto caption generated on Youtube directly.\\nKafka Python Videos - Rides.csv\\nThere is no clear explanation of the rides.csv data that the producer.py python programs use. You can find that here https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/2bd33e89906181e424f7b12a299b70b19b7cfcd5/week_6_stream_processing/python/resources/rides.csv.',\n",
       "    'section': 'Module 6: streaming with kafka',\n",
       "    'question': 'Kafka- python videos have low audio and hard to follow up'},\n",
       "   {'text': 'If you have this error, it most likely that your kafka broker docker container is not working.\\nUse docker ps to confirm\\nThen in the docker compose yaml file folder, run docker compose up -d to start all the instances.',\n",
       "    'section': 'Module 6: streaming with kafka',\n",
       "    'question': 'kafka.errors.NoBrokersAvailable: NoBrokersAvailable'},\n",
       "   {'text': 'Ankush said we can focus on horizontal scaling option.\\n“think of scaling in terms of scaling from consumer end. Or consuming message via horizontal scaling”',\n",
       "    'section': 'Module 6: streaming with kafka',\n",
       "    'question': 'Kafka homwork Q3, there are options that support scaling concept more than the others:'},\n",
       "   {'text': 'If you get this error, know that you have not built your sparks and juypter images. This images aren’t readily available on dockerHub.\\nIn the spark folder, run ./build.sh from a bash cli to to build all images before running docker compose',\n",
       "    'section': 'Module 6: streaming with kafka',\n",
       "    'question': \"How to fix docker compose error: Error response from daemon: pull access denied for spark-3.3.1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"},\n",
       "   {'text': 'Run this command in terminal in the same directory (/docker/spark):\\nchmod +x build.sh',\n",
       "    'section': 'Module 6: streaming with kafka',\n",
       "    'question': 'Python Kafka: ./build.sh: Permission denied Error'},\n",
       "   {'text': 'Restarting all services worked for me:\\ndocker-compose down\\ndocker-compose up',\n",
       "    'section': 'Module 6: streaming with kafka',\n",
       "    'question': 'Python Kafka: ‘KafkaTimeoutError: Failed to update metadata after 60.0 secs.’ when running stream-example/producer.py'},\n",
       "   {'text': 'While following tutorial 13.2 , when running ./spark-submit.sh streaming.py, encountered the following error:\\n…\\n24/03/11 09:48:36 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077...\\n24/03/11 09:48:36 INFO TransportClientFactory: Successfully created connection to localhost/127.0.0.1:7077 after 10 ms (0 ms spent in bootstraps)\\n24/03/11 09:48:54 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\\n24/03/11 09:48:56 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077…\\n24/03/11 09:49:16 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077...\\n24/03/11 09:49:36 WARN StandaloneSchedulerBackend: Application ID is not initialized yet.\\n24/03/11 09:49:36 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up.\\n…\\npy4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.sql.SparkSession.\\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\\n…\\nSolution:\\nDowngrade your local PySpark to 3.3.1 (same as Dockerfile)\\nThe reason for the failed connection in my case was the mismatch of PySpark versions. You can see that from the logs of spark-master in the docker container.\\nSolution 2:\\nCheck what Spark version your local machine has\\npyspark –version\\nspark-submit –version\\nAdd your version to SPARK_VERSION in build.sh',\n",
       "    'section': 'Module 6: streaming with kafka',\n",
       "    'question': 'Python Kafka: ./spark-submit.sh streaming.py - ERROR StandaloneSchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up.'},\n",
       "   {'text': 'Start a new terminal\\nRun: docker ps\\nCopy the CONTAINER ID of the spark-master container\\nRun: docker exec -it <spark_master_container_id> bash\\nRun: cat logs/spark-master.out\\nCheck for the log when the error happened\\nGoogle the error message from there',\n",
       "    'section': 'Module 6: streaming with kafka',\n",
       "    'question': 'Python Kafka: ./spark-submit.sh streaming.py - How to check why Spark master connection fails'},\n",
       "   {'text': 'Make sure your java version is 11 or 8.\\nCheck your version by:\\njava --version\\nCheck all your versions by:\\n/usr/libexec/java_home -V\\nIf you already have got java 11 but just not selected as default, select the specific version by:\\nexport JAVA_HOME=$(/usr/libexec/java_home -v 11.0.22)\\n(or other version of 11)',\n",
       "    'section': 'Module 6: streaming with kafka',\n",
       "    'question': 'Python Kafka: ./spark-submit.sh streaming.py Error: py4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.'},\n",
       "   {'text': 'In my set up, all of the dependencies listed in gradle.build were not installed in <project_name>-1.0-SNAPSHOT.jar.\\nSolution:\\nIn build.gradle file, I added the following at the end:\\nshadowJar {\\narchiveBaseName = \"java-kafka-rides\"\\narchiveClassifier = \\'\\'\\n}\\nAnd then in the command line ran ‘gradle shadowjar’, and run the script from java-kafka-rides-1.0-SNAPSHOT.jar created by the shadowjar',\n",
       "    'section': 'Module 6: streaming with kafka',\n",
       "    'question': 'Java Kafka: <project_name>-1.0-SNAPSHOT.jar errors: package xxx does not exist even after gradle build'},\n",
       "   {'text': 'confluent-kafka: `pip install confluent-kafka` or `conda install conda-forge::python-confluent-kafka`\\nfastavro: pip install fastavro\\nAbhirup Ghosh\\nCan install Faust Library for Module 6 Python Version due to dependency conflicts?\\nThe Faust repository and library is no longer maintained - https://github.com/robinhood/faust\\nIf you do not know Java, you now have the option to follow the Python Videos 6.13 & 6.14 here https://www.youtube.com/watch?v=BgAlVknDFlQ&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=80  and follow the RedPanda Python version here https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/06-streaming/python/redpanda_example - NOTE: I highly recommend watching the Java videos to understand the concept of streaming but you can skip the coding parts - all will become clear when you get to the Python videos and RedPanda files.',\n",
       "    'section': 'Module 6: streaming with kafka',\n",
       "    'question': 'Python Kafka: Installing dependencies for python3 06-streaming/python/avro_example/producer.py'},\n",
       "   {'text': 'In the project directory, run:\\njava -cp build/libs/<jar_name>-1.0-SNAPSHOT.jar:out src/main/java/org/example/JsonProducer.java',\n",
       "    'section': 'Module 6: streaming with kafka',\n",
       "    'question': 'Java Kafka: How to run producer/consumer/kstreams/etc in terminal'},\n",
       "   {'text': 'For example, when running JsonConsumer.java, got:\\nConsuming form kafka started\\nRESULTS:::0\\nRESULTS:::0\\nRESULTS:::0\\nOr when running JsonProducer.java, got:\\nException in thread \"main\" java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.SaslAuthenticationException: Authentication failed\\nSolution:\\nMake sure in the scripts in src/main/java/org/example/ that you are running (e.g. JsonConsumer.java, JsonProducer.java), the StreamsConfig.BOOTSTRAP_SERVERS_CONFIG is the correct server url (e.g. europe-west3 from example vs europe-west2)\\nMake sure cluster key and secrets are updated in src/main/java/org/example/Secrets.java (KAFKA_CLUSTER_KEY and KAFKA_CLUSTER_SECRET)',\n",
       "    'section': 'Module 6: streaming with kafka',\n",
       "    'question': 'Java Kafka: When running the producer/consumer/etc java scripts, no results retrieved or no message sent'},\n",
       "   {'text': 'Situation: in VS Code, usually there will be a triangle icon next to each test. I couldn’t see it at first and had to do some fixes.\\nSolution:\\n(Source)\\nVS Code\\n→ Explorer (first icon on the left navigation bar)\\n→ JAVA PROJECTS (bottom collapsable)\\n→  icon next in the rightmost position to JAVA PROJECTS\\n→  clean Workspace\\n→ Confirm by clicking Reload and Delete\\nNow you will be able to see the triangle icon next to each test like what you normally see in python tests.\\nE.g.:\\nYou can also add classes and packages in this window instead of creating files in the project directory',\n",
       "    'section': 'Module 6: streaming with kafka',\n",
       "    'question': 'Java Kafka: Tests are not picked up in VSCode'},\n",
       "   {'text': 'In Confluent Cloud:\\nEnvironment → default (or whatever you named your environment as) → The right navigation bar →  “Stream Governance API” →  The URL under “Endpoint”\\nAnd create credentials from Credentials section below it',\n",
       "    'section': 'Module 6: streaming with kafka',\n",
       "    'question': 'Confluent Kafka: Where can I find schema registry URL?'},\n",
       "   {'text': 'You can check the version of your local spark using spark-submit --version. In the build.sh file of the Python folder, make sure that SPARK_VERSION matches your local version. Similarly, make sure the pyspark you pip installed also matches this version.',\n",
       "    'section': 'Module 6: streaming with kafka',\n",
       "    'question': 'How do I check compatibility of local and container Spark versions?'},\n",
       "   {'text': 'According to https://github.com/dpkp/kafka-python/\\n“DUE TO ISSUES WITH RELEASES, IT IS SUGGESTED TO USE https://github.com/wbarnha/kafka-python-ng FOR THE TIME BEING”\\nUse pip install kafka-python-ng instead',\n",
       "    'section': 'Project',\n",
       "    'question': 'How to fix the error \"ModuleNotFoundError: No module named \\'kafka.vendor.six.moves\\'\"?'},\n",
       "   {'text': 'Each submitted project will be evaluated by 3 (three) randomly assigned students that have also submitted the project.\\nYou will also be responsible for grading the projects from 3 fellow students yourself. Please be aware that: not complying to this rule also implies you failing to achieve the Certificate at the end of the course.\\nThe final grade you get will be the median score of the grades you get from the peer reviewers.\\nAnd of course, the peer review criteria for evaluating or being evaluated must follow the guidelines defined here.',\n",
       "    'section': 'Project',\n",
       "    'question': 'How is my capstone project going to be evaluated?'},\n",
       "   {'text': 'There is only ONE project for this Zoomcamp. You do not need to submit or create two projects. There are simply TWO chances to pass the course. You can use the Second Attempt if you a) fail the first attempt b) do not have the time due to other engagements such as holiday or sickness etc. to enter your project into the first attempt.',\n",
       "    'section': 'Project',\n",
       "    'question': 'Project 1 & Project 2'},\n",
       "   {'text': 'See a list of datasets here: https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_7_project/datasets.md',\n",
       "    'section': 'Project',\n",
       "    'question': 'Does anyone know nice and relatively large datasets?'},\n",
       "   {'text': 'You need to redefine the python environment variable to that of your user account',\n",
       "    'section': 'Project',\n",
       "    'question': 'How to run python as start up script?'},\n",
       "   {'text': 'Initiate a Spark Session\\nspark = (SparkSession\\n.builder\\n.appName(app_name)\\n.master(master=master)\\n.getOrCreate())\\nspark.streams.resetTerminated()\\nquery1 = spark\\n.readStream\\n…\\n…\\n.load()\\nquery2 = spark\\n.readStream\\n…\\n…\\n.load()\\nquery3 = spark\\n.readStream\\n…\\n…\\n.load()\\nquery1.start()\\nquery2.start()\\nquery3.start()\\nspark.streams.awaitAnyTermination() #waits for any one of the query to receive kill signal or error failure. This is asynchronous\\n# On the contrary query3.start().awaitTermination() is a blocking ex call. Works well when we are reading only from one topic.',\n",
       "    'section': 'Project',\n",
       "    'question': 'Spark Streaming - How do I read from multiple topics in the same Spark Session'},\n",
       "   {'text': 'Transformed data can be moved in to azure blob storage and then it can be moved in to azure SQL DB, instead of moving directly from databricks to Azure SQL DB.',\n",
       "    'section': 'Project',\n",
       "    'question': 'Data Transformation from Databricks to Azure SQL DB'},\n",
       "   {'text': 'The trial dbt account provides access to dbt API. Job will still be needed to be added manually. Airflow will run the job using a python operator calling the API. You will need to provide api key, job id, etc. (be careful not committing it to Github).\\nDetailed explanation here: https://docs.getdbt.com/blog/dbt-airflow-spiritual-alignment\\nSource code example here: https://github.com/sungchun12/airflow-toolkit/blob/95d40ac76122de337e1b1cdc8eed35ba1c3051ed/dags/examples/dbt_cloud_example.py',\n",
       "    'section': 'Project',\n",
       "    'question': 'Orchestrating dbt with Airflow'},\n",
       "   {'text': 'https://airflow.apache.org/docs/apache-airflow-providers-google/stable/_api/airflow/providers/google/cloud/operators/dataproc/index.html\\nhttps://airflow.apache.org/docs/apache-airflow-providers-google/stable/_modules/airflow/providers/google/cloud/operators/dataproc.html\\nGive the following roles to you service account:\\nDataProc Administrator\\nService Account User (explanation here)\\nUse DataprocSubmitPySparkJobOperator, DataprocDeleteClusterOperator and  DataprocCreateClusterOperator.\\nWhen using  DataprocSubmitPySparkJobOperator, do not forget to add:\\ndataproc_jars = [\"gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.24.0.jar\"]\\nBecause DataProc does not already have the BigQuery Connector.',\n",
       "    'section': 'Project',\n",
       "    'question': 'Orchestrating DataProc with Airflow'},\n",
       "   {'text': 'You can trigger your dbt job in Mage pipeline. For this get your dbt cloud api key under settings/Api tokens/personal tokens. Add it safely to  your .env\\nFor example\\ndbt_api_trigger=dbt_**\\nNavigate to job page and find api trigger  link\\nThen create a custom mage Python block with a simple http request like here\\nfrom dotenv import load_dotenv\\nfrom pathlib import Path\\ndotenv_path = Path(\\'/home/src/.env\\')\\nload_dotenv(dotenv_path=dotenv_path)\\ndbt_api_trigger= os.getenv(dbt_api_trigger)\\nurl = f\"https://cloud.getdbt.com/api/v2/accounts/{dbt_account_id}/jobs/<job_id>/run/\"\\nheaders = {\\n        \"Authorization\": f\"Token {dbt_api_trigger}\",\\n        \"Content-Type\": \"application/json\" }\\nbody = {\\n        \"cause\": \"Triggered via API\"\\n    }\\n    response = requests.post(url, headers=headers, json=body)\\nvoila! You triggered dbt job form your mage pipeline.',\n",
       "    'section': 'Project',\n",
       "    'question': 'Orchestrating dbt cloud with Mage'},\n",
       "   {'text': \"The slack thread : thttps://datatalks-club.slack.com/archives/C01FABYF2RG/p1677678161866999\\nThe question is that sometimes even if you take plenty of effort to document every single step, and we can't even sure if the person doing the peer review will be able to follow-up, so how this criteria will be evaluated?\\nAlex clarifies: “Ideally yes, you should try to re-run everything. But I understand that not everyone has time to do it, so if you check the code by looking at it and try to spot errors, places with missing instructions and so on - then it's already great”\",\n",
       "    'section': 'Project',\n",
       "    'question': 'Project evaluation - Reproducibility'},\n",
       "   {'text': 'The key valut in Azure cloud is used to store credentials or passwords or secrets of different tech stack used in Azure. For example if u do not want to expose the password in SQL database, then we can save the password under a given name and use them in other Azure stack.',\n",
       "    'section': 'Project',\n",
       "    'question': 'Key Vault in Azure cloud stack'},\n",
       "   {'text': 'You can get the version of py4j from inside docker using this command\\ndocker exec -it --user airflow airflow-airflow-scheduler-1 bash -c \"ls /opt/spark/python/lib\"',\n",
       "    'section': 'Project',\n",
       "    'question': \"Spark docker - `ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`\"},\n",
       "   {'text': 'Either use conda or pip for managing venv, using both of them together will cause incompatibility.\\nIf you’re using conda, install psycopg2 using the conda-forge channel, which may handle the architecture compatibility automatically\\nconda install -c conda-forge psycopg2\\nIf pip, do the normal install\\npip install psycopg2',\n",
       "    'section': 'Project',\n",
       "    'question': 'psycopg2 complains of incompatible environment e.g x86 instead of amd'},\n",
       "   {'text': 'This is not a FAQ but more of an advice if you want to set up dbt locally, I did it in the following way:\\nI had the postgres instance from week 2 (year 2024) up (the docker-compose)\\nmkdir dbt\\nvi dbt/profiles.yml\\nAnd here I attached this content (only the required fields) and replaced them with the proper values (for instance mine where in the .env file of the folder of week 2 docker stuff)\\ncd dbt && git clone https://github.com/dbt-labs/dbt-starter-project\\nmkdir project && cd project && mv dbt-starter-project/* .\\nMake sure that you align the profile name in profiles.yml with the dbt_project.yml file\\nAdd this line anywhere on the dbt_project.yml file:\\nconfig-version: 2\\ndocker run --network=mage-zoomcamp_default --mount type=bind,source=/<your-path>/dbt/project,target=/usr/app --mount type=bind,source=/<your-path>/profiles.yml,target=/root/.dbt/profiles.yml ghcr.io/dbt-labs/dbt-postgres ls\\nIf you have trouble run\\ndocker run --network=mage-zoomcamp_default --mount type=bind,source=/<your-path>/dbt/project,target=/usr/app --mount type=bind,source=/<your-path>/profiles.yml,target=/root/.dbt/profiles.yml ghcr.io/dbt-labs/dbt-postgres debug',\n",
       "    'section': 'Project',\n",
       "    'question': 'Setting up dbt locally with Docker and Postgres'},\n",
       "   {'text': 'The following line should be included in pyspark configuration\\n# Example initialization of SparkSession variable\\nspark = (SparkSession.builder\\n.master(...)\\n.appName(...)\\n# Add the following configuration\\n.config(\"spark.jars.packages\", \"com.google.cloud.spark:spark-3.5-bigquery:0.37.0\")\\n)',\n",
       "    'section': 'Project',\n",
       "    'question': 'How to connect Pyspark with BigQuery?'},\n",
       "   {'text': 'Install the astronomer-cosmos package as a dependency. (see Terraform example).\\nMake a new folder, dbt/, inside the dags/ folder of your Composer GCP bucket and copy paste your dbt-core project there. (see example)\\nEnsure your profiles.yml is configured to authenticate with a service account key. (see BigQuery example)\\nCreate a new DAG using the DbtTaskGroup class and a ProfileConfig specifying a profiles_yml_filepath that points to the location of your JSON key file. (see example)\\nYour dbt lineage graph should now appear as tasks inside a task group like this:',\n",
       "    'section': 'Course Management Form for Homeworks',\n",
       "    'question': 'How to run a dbt-core project as an Airflow Task Group on Google Cloud Composer using a service account JSON key'},\n",
       "   {'text': 'The display name listed on the leaderboard is an auto-generated randomized name. You can edit it to be a nickname, or your real name, if you prefer. Your entry on the Leaderboard is the one highlighted in teal(?) / light green (?).\\nThe Certificate name should be your actual name that you want to appear on your certificate after completing the course.\\nThe \"Display on Leaderboard\" option indicates whether you want your name to be listed on the course leaderboard.\\nQuestion: Is it possible to create external tables in BigQuery using URLs, such as those from the NY Taxi data website?\\nAnswer: Not really, only Bigtable, Cloud Storage, and Google Drive are supported data stores.',\n",
       "    'section': 'Workshop 1 - dlthub',\n",
       "    'question': 'Edit Course Profile.'},\n",
       "   {'text': \"Answer: To run the provided code, ensure that the 'dlt[duckdb]' package is installed. You can do this by executing the provided installation command: !pip install dlt[duckdb]. If you’re doing it locally, be sure to also have duckdb pip installed (even before the duckdb package is loaded).\",\n",
       "    'section': 'Workshop 1 - dlthub',\n",
       "    'question': 'How do I install the necessary dependencies to run the code?'},\n",
       "   {'text': 'If you are running Jupyter Notebook on a fresh new Codespace or in local machine with a new Virtual Environment, you will need this package to run the starter Jupyter Notebook offered by the teacher. Execute this:\\npip install jupyter',\n",
       "    'section': 'Workshop 1 - dlthub',\n",
       "    'question': 'Other packages needed but not listed'},\n",
       "   {'text': 'Alternatively, you can switch to in-file storage with:',\n",
       "    'section': 'Workshop 1 - dlthub',\n",
       "    'question': 'How can I use DuckDB In-Memory database with dlt ?'},\n",
       "   {'text': 'After loading, you should have a total of 8 records, and ID 3 should have age 33\\nQuestion: Calculate the sum of ages of all the people loaded as described above\\nThe sum of all eight records\\' respective ages is too big to be in the choices. You need to first filter out the people whose occupation is equal to None in order to get an answer that is close to or present in the given choices. 😃\\n----------------------------------------------------------------------------------------\\nFIXED = use a raw string and keep the file:/// at the start of your file path\\nI\\'m having an issue with the dlt workshop notebook. The \\'Load to Parquet file\\' section specifically. No matter what I change the file path to, it\\'s still saving the dlt files directly to my C drive.\\n# Set the bucket_url. We can also use a local folder\\nos.environ[\\'DESTINATION__FILESYSTEM__BUCKET_URL\\'] = r\\'file:///content/.dlt/my_folder\\'\\nurl = \"https://storage.googleapis.com/dtc_zoomcamp_api/yellow_tripdata_2009-06.jsonl\"\\n# Define your pipeline\\npipeline = dlt.pipeline(\\npipeline_name=\\'my_pipeline\\',\\ndestination=\\'filesystem\\',\\ndataset_name=\\'mydata\\'\\n)\\n# Run the pipeline with the generator we created earlier.\\nload_info = pipeline.run(stream_download_jsonl(url), table_name=\"users\", loader_file_format=\"parquet\")\\nprint(load_info)\\n# Get a list of all Parquet files in the specified folder\\nparquet_files = glob.glob(\\'/content/.dlt/my_folder/mydata/users/*.parquet\\')\\n# show parquet files\\nfor file in parquet_files:\\nprint(file)',\n",
       "    'section': 'Workshop 2 - RisingWave',\n",
       "    'question': 'Homework - dlt Exercise 3 - Merge a generator concerns'},\n",
       "   {'text': 'Check the contents of the repository with ls - the command.sh file should be in the root folder\\nIf it is not, verify that you had cloned the correct repository - https://github.com/risingwavelabs/risingwave-data-talks-workshop-2024-03-04',\n",
       "    'section': 'Workshop 2 - RisingWave',\n",
       "    'question': 'command.sh Error - source: no such file or directory: command.sh'},\n",
       "   {'text': \"psql is a command line tool that is installed alongside PostgreSQL DB, but since we've always been running PostgreSQL in a container, you've only got `pgcli`, which lacks the feature to run a sql script into the DB. Besides, having a command line for each database flavor you'll have to deal with as a Data Professional is far from ideal.\\nSo, instead, you can use usql. Check the docs for details on how to install for your OS. On macOS, it supports `homebrew`, and on Windows, it supports scoop.\\nSo, to run the taxi_trips.sql script with usql:\",\n",
       "    'section': 'Workshop 2 - RisingWave',\n",
       "    'question': 'psql - command not found: psql (alternative install)'},\n",
       "   {'text': 'If you encounter this error and are certain that you have docker compose installed, but typically run it as docker compose without the hyphen, then consider editing command.sh file by removing the hyphen from ‘docker-compose’. Example:\\nstart-cluster() {\\ndocker compose -f docker/docker-compose.yml up -d\\n}',\n",
       "    'section': 'Workshop 2 - RisingWave',\n",
       "    'question': 'Setup - source command.sh - error: “docker-compose” not found'},\n",
       "   {'text': 'ERROR: The Compose file \\'./docker/docker-compose.yml\\' is invalid because:\\nInvalid top-level property \"x-image\". Valid top-level sections for this Compose file are: version, services, networks, volumes, secrets, configs, and extensions starting with \"x-\".\\nYou might be seeing this error because you\\'re using the wrong Compose file version. Either specify a supported version (e.g \"2.2\" or \"3.3\") and place your service definitions under the `services` key, or omit the `version` key and place your service definitions at the root of the file to use version 1.\\nFor more on the Compose file format versions, see https://docs.docker.com/compose/compose-file/\\nIf you encounter the above error and have docker-compose installed, try updating your version of docker-compose. At the time of reporting this issue (March 17 2024), Ubuntu does not seem to support a docker-compose version high enough to run the required docker images. If you have this error and are on a Ubuntu machine, consider starting a VM with a Debian machine or look for an alternative way to download docker-compose at the latest version on your machine.',\n",
       "    'section': 'Workshop 2 - RisingWave',\n",
       "    'question': 'Setup - start-cluster error: Invalid top-level property x-image'},\n",
       "   {'text': 'Ans: [source] Yes, it is so that we can observe the changes as we’re working on the queries in real-time. The script is changing the date timestamp to the current time, so our queries with the now()filter would work. Open another terminal tab to copy+paste the queries while the stream-kafka script is running in the background.\\nNoel: I have recently increased this up to 100 at a time, you may pull the latest changes from the repository.',\n",
       "    'section': 'Workshop 2 - RisingWave',\n",
       "    'question': 'stream-kafka Qn: Is it expected that the records are being ingested 10 at a time?'},\n",
       "   {'text': 'Ans: No, it is not.',\n",
       "    'section': 'Workshop 2 - RisingWave',\n",
       "    'question': 'Setup - Qn: Is kafka install required for the RisingWave workshop? [source]'},\n",
       "   {'text': 'Ans: about 7GB free for all the containers to be provisioned and then the psql still needs to run and ingest the taxi data, so maybe 10gb in total?',\n",
       "    'section': 'Workshop 2 - RisingWave',\n",
       "    'question': 'Setup - Qn: How much free disk space should we have? [source]'},\n",
       "   {'text': 'Replace psycopg2==2.9.9 with psycopg2-binary in the requirements.txt file [source] [another]\\nWhen you open another terminal to run the psql, remember to do the source command.sh step for each terminal session\\n---------------------------------------------------------------------------------------------',\n",
       "    'section': 'Workshop 2 - RisingWave',\n",
       "    'question': 'Psycopg2 - issues when running stream-kafka script'},\n",
       "   {'text': \"If you’re using an Anaconda installation:\\nCd home/\\nConda install gcc\\nSource back to your RisingWave Venv - source .venv/bin/activate\\nPip install psycopg2-binary\\nPip install -r requirements.txt\\nFor some reason this worked - the Conda base doesn’t have the GCC installed - (GNU Compiler Collection) a compiler system that supports various programming languages. Without this the it fails to install pyproject.toml-based projects\\n“It's possible that in your specific environment, the gcc installation was required at the system level rather than within the virtual environment. This can happen if the build process for psycopg2 tries to access system-level dependencies during installation.\\nInstalling gcc in your main Python installation (Conda) would make it available system-wide, allowing any Python environment to access it when necessary for building packages.”\\ngcc stands for GNU Compiler Collection. It is a compiler system developed by the GNU Project that supports various programming languages, including C, C++, Objective-C, and Fortran.\\nGCC is widely used for compiling source code written in these languages into executable programs or libraries. It's a key tool in the software development process, particularly in the compilation stage where source code is translated into machine code that can be executed by a computer's processor.\\nIn addition to compiling source code, GCC also provides various optimization options, debugging support, and extensive documentation, making it a powerful and versatile tool for developers across different platforms and architectures.\\n—-----------------------------------------------------------------------------------\",\n",
       "    'section': 'Workshop 2 - RisingWave',\n",
       "    'question': 'Psycopg2 - `Could not build wheels for psycopg2, which is required to install pyproject.toml-based projects`'},\n",
       "   {'text': \"Below I have listed some steps I took to rectify this and potentially other minor errors, in Windows:\\nUse the git bash terminal in windows.\\nActivate python venv from git bash: source .venv/Scripts/activate\\nModify the seed_kafka.py file: in the first line, replace python3 with python.\\nNow from git bash, run the seed-kafka cmd. It should work now.\\nAdditional Notes:\\nYou can connect to the RisingWave cluster from Powershell with the command psql -h localhost -p 4566 -d dev -U root , otherwise it asks for a password.\\nThe equivalent of source commands.sh  in Powershell is . .\\\\commands.sh from the workshop directory.\\nHope this can save you from some trouble in case you're doing this workshop on Windows like I am.\\n—--------------------------------------------------------------------------------------\",\n",
       "    'section': 'Workshop 2 - RisingWave',\n",
       "    'question': 'Psycopg2 InternalError: Failed to run the query - when running the seed-kafka command after initial setup.'},\n",
       "   {'text': 'In case the script gets stuck on\\n%3|1709652240.100|FAIL|rdkafka#producer-2| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Connect to ipv4#127.0.0.1:9092 failed: Connection refused (after 0ms in state CONNECT)gre\\nafter trying to load the trip data, check the logs of the message_queue container in docker. If it keeps restarting with Could not initialize seastar: std::runtime_error (insufficient physical memory: needed 4294967296 available 4067422208)  as the last message, then go to the docker-compose file in the docker folder of the project and change the ‘memory’ command for the message_queue service for some lower value.\\nSolution: lower the memory allocation of the service “message_queue” in your docker-compose file from 4GB. If you have the “insufficient physical memory” error message (try 3GB)\\nIssue: Running psql -f risingwave-sql/table/trip_data.sql after starting services with ‘default’ values using docker-compose up gives the error  “psql:risingwave-sql/table/trip_data.sql:61: ERROR:  syntax error at or near \".\" LINE 60:       properties.bootstrap.server=\\'message_queue:29092\\'”\\nSolution: Make sure you have run source commands.sh in each terminal window',\n",
       "    'section': 'Workshop 2 - RisingWave',\n",
       "    'question': 'Running stream-kafka script gets stuck on a loop with Connection Refused'},\n",
       "   {'text': 'Use seed-kafka instead of stream-kafka to get a static set of results.',\n",
       "    'section': 'Workshop 2 - RisingWave',\n",
       "    'question': 'For the homework questions is there a specific number of records that have to be processed to obtain the final answer?'},\n",
       "   {'text': 'It is best to use the order by and limit clause on the query to the materialized view instead of the materialized view creation in order to guarantee consistent results\\nHomework - The answers in the homework do not match the provided options: You must follow the following steps: 1. clean-cluster 2. docker volume prune and use seed-kafka instead of stream-kafka. Ensure that the number of records is 100K.',\n",
       "    'section': 'Workshop 2 - RisingWave',\n",
       "    'question': 'Homework - Materialized view does not guarantee order by warning'},\n",
       "   {'text': 'For this workshop, and if you are following the view from Noel (2024) this requires you to install postgres to use it on your terminal. Found this steps (commands) to get it done [source]:\\nwget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -\\nsudo sh -c \\'echo \"deb http://apt.postgresql.org/pub/repos/apt/ $(lsb_release -cs)-pgdg main\" >> /etc/apt/sources.list.d/pgdg.list\\'\\nsudo apt update\\napt install postgresql postgresql-contrib\\n(comment): now let’s check the service for postgresql\\nservice postgresql status\\n(comment) If down: use the next command\\nservice postgresql start\\n(comment) And your are done',\n",
       "    'section': 'Workshop 2 - RisingWave',\n",
       "    'question': 'How to install postgress on Linux like OS'},\n",
       "   {'text': 'Refer to the solution given in the first solution here:\\nhttps://stackoverflow.com/questions/24683221/xdg-open-no-method-available-even-after-installing-xdg-utils\\nInstead of w3m use any other browser of your choice.\\nIt is just trying to open the index.html file. Which you can do from your File Explorer/Finder. If you’re on wsl try using explorer.exe index.html',\n",
       "    'section': 'Workshop 2 - RisingWave',\n",
       "    'question': 'Unable to Open Dashboard as xdg-open doesn’t open any browser'},\n",
       "   {'text': 'Example Error:\\nWhen attempting to execute a Python script named seed-kafka.py or server.py with the following shebang line specifying Python 3 as the interpreter:\\nUsers may encounter the following error in a Unix-like environment:\\nThis error indicates that there is a problem with the Python interpreter path specified in the shebang line. The presence of the \\\\r character suggests that the script was edited or created in a Windows environment, causing the interpreter path to be incorrect when executed in Unix-like environments.\\n2 Solutions:\\nEither one or the other\\nUpdate Shebang Line:\\nVerify Python Interpreter Path: Use the which python3 command to determine the path to the Python 3 interpreter available in the current environment.\\nUpdate Shebang Line: Open the script file in a text editor. Modify the shebang line to point to the correct Python interpreter path found in the previous step. Ensure that the shebang line is consistent with the Python interpreter path in the execution environment.\\nExample Shebang Line:\\nReplace /usr/bin/env python3 with the correct Python interpreter path found using which python3.\\nConvert Line Endings:\\nUse the dos2unix command-line tool to convert the line endings of the script from Windows-style to Unix-style.\\nThis removes the extraneous carriage return characters (\\\\r), resolving issues related to unexpected tokens and ensuring compatibility with Unix-like environments.\\nExample Command:',\n",
       "    'section': 'Workshop 2 - RisingWave',\n",
       "    'question': 'Resolving Python Interpreter Path Inconsistencies in Unix-like Environments'},\n",
       "   {'text': 'Ans : Windowing in streaming SQL involves defining a time-based or row-based boundary for data processing. It allows you to analyze and aggregate data over specific time intervals or based on the number of events received, providing a way to manage and organize streaming data for analysis.',\n",
       "    'section': 'Workshop 2 - RisingWave',\n",
       "    'question': 'How does windowing work in Sql?'},\n",
       "   {'text': 'Python 3.12.1, is not compatible with kafka-python-2.0.2. Therefore, instead of running \"pip install kafka-python\", you can resolve the issue by using \"pip install git+https://github.com/dpkp/kafka-python.git\". If you have already installed kafka-python, you need to run \"pip uninstall kafka-python\" before executing \"pip install git+https://github.com/dpkp/kafka-python.git\" to resolve the compatibility issue.\\nQ:In the Mage pipeline, individual blocks run successfully. However, when executing the pipeline as a whole, some blocks fail.\\nA: I have the following key-value pair in io_config.yaml file configured but still Mage blocks failed to generate OAuth and authenticate with GCP: GOOGLE_SERVICE_ACC_KEY_FILEPATH: \"{{ env_var(\\'GCP_CREDENTIALS\\') }}\". The GCP_CREDENTIALS variable holds the full path to the service account key\\'s JSON file. Adding the following line within the failed code block resolved the issue: os.environ[\\'GOOGLE_APPLICATION_CREDENTIALS\\'] = os.environ.get(\\'GCP_CREDENTIALS\\').\\nThis occurs because the path to profiles.yml is not correctly specified. You can rectify this by:\\n“export DBT_PROFILES_DBT=path/to/profiles.yml”\\nEg., /home/src/magic-zoomcamp/dbt/project_name/\\nDo the similar for DBT_PROJECT_DIR if getting similar issue with dbt_project.yml.\\nOnce DIRs are set,:\\n“dbt debug –config-dir”\\nThis would update your paths. To maintain same path across sessions, use the path variables in your .env file.\\nTo add triggers in mage pipelines via CLI, you can create a trigger of type API, and copy the API links.\\nEg. link: http://localhost:6789/api/pipeline_schedules/10/pipeline_runs/f3a1a4228fc64cfd85295b668c93f3b2\\nThen create a trigger.py as such:\\nimport os\\nimport requests\\nclass MageTrigger:\\nOPTIONS = {\\n\"<pipeline_name>\": {\\n\"trigger_id\": 10,\\n\"key\": \"f3a1a4228fc64cfd85295b668c93f3b2\"\\n}\\n}\\n@staticmethod\\ndef trigger_pipeline(pipeline_name, variables=None):\\ntrigger_id = MageTrigger.OPTIONS[pipeline_name][\"trigger_id\"]\\nkey = MageTrigger.OPTIONS[pipeline_name][\"key\"]\\nendpoint = f\"http://localhost:6789/api/pipeline_schedules/{trigger_id}/pipeline_runs/{key}\"\\nheaders = {\\'Content-Type\\': \\'application/json\\'}\\npayload = {}\\nif variables is not None:\\npayload[\\'pipeline_run\\'] = {\\'variables\\': variables}\\nresponse = requests.post(endpoint, headers=headers, json=payload)\\nreturn response\\nMageTrigger.trigger_pipeline(\"<pipeline_name>\")\\nFinally, after the mage server is up an running, simply this command:\\npython trigger.py from mage directory in terminal.\\nCan I do data partitioning & clustering run by dbt pipeline, or I would need to do this manually in BigQuery afterwards?\\nYou can use this configuration in your DBT model:\\n{\\n\"field\": \"<field name>\",\\n\"data_type\": \"<timestamp | date | datetime | int64>\",\\n\"granularity\": \"<hour | day | month | year>\"\\n# Only required if data_type is \"int64\"\\n\"range\": {\\n\"start\": <int>,\\n\"end\": <int>,\\n\"interval\": <int>\\n}\\n}\\nand for clustering\\n{{\\nconfig(\\nmaterialized = \"table\",\\ncluster_by = \"order_id\",\\n)\\n}}\\nmore details in: https://docs.getdbt.com/reference/resource-configs/bigquery-configs',\n",
       "    'section': 'Triggers in Mage via CLI',\n",
       "    'question': 'Encountering the error \"ModuleNotFoundError: No module named \\'kafka.vendor.six.moves\\'\" when running \"from kafka import KafkaProducer\" in Jupyter Notebook for Module 6 Homework?'},\n",
       "   {'text': 'Docker Commands\\n# Create a Docker Image from a base image\\nDocker run -it ubuntu bash\\n#List docker images\\nDocker images list\\n#List  Running containers\\nDocker ps -a\\n#List with full container ids\\nDocker ps -a --no-trunc\\n#Add onto existing image to create new image\\nDocker commit -a <User_Name> -m \"Message\" container_id New_Image_Name\\n# Create a Docker Image with an entrypoint from a base image\\nDocker run -it --entry_point=bash python:3.11\\n#Attach to a stopped container\\nDocker start -ai <Container_Name>\\n#Attach to a running container\\ndocker exec -it <Container_ID> bash\\n#copying from host to container\\nDocker cp <SRC_PATH/file> <containerid>:<dest_path>\\n#copying from container to host\\nDocker cp <containerid>:<Srct_path> <Dest Path on host/file>\\n#Create an image from a docker file\\nDocker build -t <Image_Name> <Location of Dockerfile>\\n#DockerFile Options and best practices\\nhttps://devopscube.com/build-docker-image/\\n#Docker delete all images forcefully\\ndocker rmi -f $(docker images -aq)\\n#Docker delete all containers forcefully\\ndocker rm -f $(docker ps -qa)\\n#docker compose creation\\nhttps://www.composerize.com/\\nGCP Commands\\n1.     Create SSH Keys\\n2.     Added to the Settings of Compute Engine VM Instance\\n3.     SSH-ed into the VM Instance with a config similar to following\\nHost my-website.com\\nHostName my-website.com\\nUser my-user\\nIdentityFile ~/.ssh/id_rsa\\n4.     Installed Anaconda by installing the sh file through bash <Anaconda.sh>\\n5.     Install Docker after\\na.     Sudo apt-get update\\nb.     Sudo apt-get docker\\n6.     To run Docker without SUDO permissions\\na.     https://github.com/sindresorhus/guides/blob/main/docker-without-sudo.md\\n7.     Google cloud remote copy\\na.     gcloud compute scp LOCAL_FILE_PATHVM_NAME:REMOTE_DIR\\nInstall GCP Cloud SDK on Docker Machine\\nhttps://stackoverflow.com/questions/23247943/trouble-installing-google-cloud-sdk-in-ubuntu\\nsudo apt-get install apt-transport-https ca-certificates gnupg && echo \"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main\"| sudo tee -a /etc/apt/sources.list.d/google-cloud-sdk.list&& curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key --keyring /usr/share/keyrings/cloud.google.gpg add - && sudo apt-get update && sudo apt-get install google-cloud-sdk && sudo apt-get install google-cloud-sdk-app-engine-java && sudo apt-get install google-cloud-sdk-app-engine-python && gcloud init\\nAnaconda Commands\\n#Activate environment\\nConda Activate <environment_name>\\n#DeActivate environment\\nConda DeActivate <environment_name>\\n#Start iterm without conda environment\\nconda config --set auto_activate_base false\\n# Using Conda forge as default (Community driven packaging recipes and solutions)\\nhttps://conda-forge.org/docs/user/introduction.html\\nconda --version\\nconda update conda\\nconda config --add channels conda-forge\\nconda config --set channel_priority strict\\n#Using Libmamba as Solver\\nconda install pgcli  --solver=libmamba\\nLinux/MAC Commands\\nStarting and Stopping Services on Linux\\n●  \\tsudo systemctl start postgresql\\n●  \\tsudo systemctl stop postgresql\\nStarting and Stopping Services on MAC\\n●      launchctl start postgresql\\n●      launchctl stop postgresql\\nIdentifying processes listening to a Port across MAC/Linux\\nsudo lsof -i -P -n | grep LISTEN\\n$ sudo netstat -tulpn | grep LISTEN\\n$ sudo ss -tulpn | grep LISTEN\\n$ sudo lsof -i:22 ## see a specific port such as 22 ##\\n$ sudo nmap -sTU -O IP-address-Here\\nInstalling a package on Debian\\nsudo apt install <packagename>\\nListing all package on Debian\\nDpkg -l | grep <packagename>\\nUnInstalling a package on Debian\\nSudo apt remove <packagename>\\nSudo apt autoclean  && sudo apt autoremove\\nList all Processes on Debian/Ubuntu\\nPs -aux\\napt-get update && apt-get install procps\\napt-get install iproute2 for ss -tulpn\\n#Postgres Install\\nsudo sh -c \\'echo \"deb https://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main\" > /etc/apt/sources.list.d/pgdg.list\\'\\nwget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -\\nsudo apt-get update\\nsudo apt-get -y install postgresql\\n#Changing Postgresql port to 5432\\n- sudo service postgresql stop - sed -e \\'s/^port.*/port = 5432/\\' /etc/postgresql/10/main/postgresql.conf > postgresql.conf\\n- sudo chown postgres postgresql.conf\\n- sudo mv postgresql.conf /etc/postgresql/10/main\\n- sudo systemctl restart postgresql',\n",
       "    'section': 'Triggers in Mage via CLI',\n",
       "    'question': 'Basic Commands'}]},\n",
       " {'course': 'machine-learning-zoomcamp',\n",
       "  'documents': [{'text': 'Machine Learning Zoomcamp FAQ\\nThe purpose of this document is to capture frequently asked technical questions.\\nWe did this for our data engineering course and it worked quite well. Check this document for inspiration on how to structure your questions and answers:\\nData Engineering Zoomcamp FAQ\\nIn the course GitHub repository there’s a link. Here it is: https://airtable.com/shryxwLd0COOEaqXo\\nwork',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'How do I sign up?'},\n",
       "   {'text': 'The course videos are pre-recorded, you can start watching the course right now.\\nWe will also occasionally have office hours - live sessions where we will answer your questions. The office hours sessions are recorded too.\\nYou can see the office hours as well as the pre-recorded course videos in the course playlist on YouTube.',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'Is it going to be live? When?'},\n",
       "   {'text': 'Everything is recorded, so you won’t miss anything. You will be able to ask your questions for office hours in advance and we will cover them during the live stream. Also, you can always ask questions in Slack.',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'What if I miss a session?'},\n",
       "   {'text': \"The bare minimum. The focus is more on practice, and we'll cover the theory only on the intuitive level.: https://mlbookcamp.com/article/python\\nFor example, we won't derive the gradient update rule for logistic regression (there are other great courses for that), but we'll cover how to use logistic regression and make sense of the results.\",\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'How much theory will you cover?'},\n",
       "   {'text': \"Yes! We'll cover some linear algebra in the course, but in general, there will be very few formulas, mostly code.\\nHere are some interesting videos covering linear algebra that you can already watch: ML Zoomcamp 1.8 - Linear Algebra Refresher from Alexey Grigorev or the excellent playlist from 3Blue1Brown Vectors | Chapter 1, Essence of linear algebra. Never hesitate to ask the community for help if you have any question.\\n(Mélanie Fouesnard)\",\n",
       "    'section': 'General course-related questions',\n",
       "    'question': \"I don't know math. Can I take the course?\"},\n",
       "   {'text': \"The process is automated now, so you should receive the email eventually. If you haven’t, check your promotions tab in Gmail as well as spam.\\nIf you unsubscribed from our newsletter, you won't get course related updates too.\\nBut don't worry, it’s not a problem. To make sure you don’t miss anything, join the #course-ml-zoomcamp channel in Slack and our telegram channel with announcements. This is enough to follow the course.\",\n",
       "    'section': 'General course-related questions',\n",
       "    'question': \"I filled the form, but haven't received a confirmation email. Is it normal?\"},\n",
       "   {'text': 'Approximately 4 months, but may take more if you want to do some extra activities (an extra project, an article, etc)',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'How long is the course?'},\n",
       "   {'text': 'Around ~10 hours per week. Timur Kamaliev did a detailed analysis of how much time students of the previous cohort needed to spend on different modules and projects. Full article',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'How much time do I need for this course?'},\n",
       "   {'text': 'Yes, if you finish at least 2 out of 3 projects and review 3 peers’ Projects by the deadline, you will get a certificate. This is what it looks like: link. There’s also a version without a robot: link.',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'Will I get a certificate?'},\n",
       "   {'text': \"Yes, it's possible. See the previous answer.\",\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'Will I get a certificate if I missed the midterm project?'},\n",
       "   {'text': 'Check this article. If you know everything in this article, you know enough. If you don’t, read the article and join the coursIntroduction to Pythone too :)\\nIntroduction to Python – Machine Learning Bookcamp\\nYou can follow this English course from the OpenClassrooms e-learning platform, which is free and covers the python basics for data analysis: Learn Python Basics for Data Analysis - OpenClassrooms . It is important to know some basics such as: how to run a Jupyter notebook, how to import libraries (and what libraries are), how to declare a variable (and what variables are) and some important operations regarding data analysis.\\n(Mélanie Fouesnard)',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'How much Python should I know?'},\n",
       "   {'text': 'For the Machine Learning part, all you need is a working laptop with an internet connection. The Deep Learning part is more resource intensive, but for that you can use a cloud (we use Saturn cloud but can be anything else).\\n(Rileen Sinha; based on response by Alexey on Slack)',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': \"Any particular hardware requirements for the course or everything is mostly cloud? TIA! Couldn't really find this in the FAQ.\"},\n",
       "   {'text': 'Here is an article that worked for me: https://knowmledge.com/2023/12/07/ml-zoomcamp-2023-project/',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'How to setup TensorFlow with GPU support on Ubuntu?'},\n",
       "   {'text': \"Here’s how you join a in Slack: https://slack.com/help/articles/205239967-Join-a-channel\\nClick “All channels” at the top of your left sidebar. If you don't see this option, click “More” to find it.\\nBrowse the list of public channels in your workspace, or use the search bar to search by channel name or description.\\nSelect a channel from the list to view it.\\nClick Join Channel.\\nDo we need to provide the GitHub link to only our code corresponding to the homework questions?\\nYes. You are required to provide the URL to your repo in order to receive a grade\",\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'I’m new to Slack and can’t find the course channel. Where is it?'},\n",
       "   {'text': 'Yes, you can. You won’t be able to submit some of the homeworks, but you can still take part in the course.\\nIn order to get a certificate, you need to submit 2 out of 3 course projects and review 3 peers’ Projects by the deadline. It means that if you join the course at the end of November and manage to work on two projects, you will still be eligible for a certificate.',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'The course has already started. Can I still join it?'},\n",
       "   {'text': 'The course is available in the self-paced mode too, so you can go through the materials at any time. But if you want to do it as a cohort with other students, the next iterations will happen in September 2023, September 2024 (and potentially other Septembers as well).',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'When does the next iteration start?'},\n",
       "   {'text': 'No, it’s not possible. The form is closed after the due date. But don’t worry, homework is not mandatory for finishing the course.',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'Can I submit the homework after the due date?'},\n",
       "   {'text': 'Welcome to the course! Go to the course page (http://mlzoomcamp.com/), scroll down and start going through the course materials. Then read everything in the cohort folder for your cohort’s year.\\nClick on the links and start watching the videos. Also watch office hours from previous cohorts. Go to DTC youtube channel and click on Playlists and search for {course yyyy}. ML Zoomcamp was first launched in 2021.\\nOr you can just use this link: http://mlzoomcamp.com/#syllabus',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'I just joined. What should I do next? How can I access course materials?'},\n",
       "   {'text': 'For the 2023 cohort, you can see the deadlines here (it’s taken from the 2023 cohort page)',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'What are the deadlines in this course?'},\n",
       "   {'text': 'There’s not much difference. There was one special module (BentoML) in the previous iteration of the course, but the rest of the modules are the same as in 2022. The homework this year is different.',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'What’s the difference between the previous iteration of the course (2022) and this one (2023)?'},\n",
       "   {'text': 'We won’t re-record the course videos. The focus of the course and the skills we want to teach remained the same, and the videos are still up-to-date.\\nIf you haven’t taken part in the previous iteration, you can start watching the videos. It’ll be useful for you and you will learn new things. However, we recommend using Python 3.10 now instead of Python 3.8.',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'The course videos are from the previous iteration. Will you release new ones or we’ll use the videos from 2021?'},\n",
       "   {'text': 'When you post about what you learned from the course on your social media pages, use the tag #mlzoomcamp. When you submit your homework, there’s a section in the form for putting the links there. Separate multiple links by any whitespace character (linebreak, space, tab, etc).\\nFor posting the learning in public links, you get extra scores. But the number of scores is limited to 7 points: if you put more than 7 links in your homework form, you’ll get only 7 points.\\nThe same content can be posted to 7 different social sites and still earn you 7 points if you add 7 URLs per week, see Alexey’s reply. (~ ellacharmed)\\nFor midterms/capstones, the awarded points are doubled as the duration is longer. So for projects the points are capped at 14 for 14 URLs.',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'Submitting learning in public links'},\n",
       "   {'text': \"You can create your own github repository for the course with your notes, homework, projects, etc.\\nThen fork the original course repo and add a link under the 'Community Notes' section to the notes that are in your own repo.\\nAfter that's done, create a pull request to sync your fork with the original course repo.\\n(By Wesley Barreto)\",\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'Adding community notes'},\n",
       "   {'text': \"Leaderboard Links:\\n2023 - https://docs.google.com/spreadsheets/d/e/2PACX-1vSNK_yGtELX1RJK1SSRl4xiUbD0XZMYS6uwHnybc7Mql-WMnMgO7hHSu59w-1cE7FeFZjkopbh684UE/pubhtml\\n2022 - https://docs.google.com/spreadsheets/d/e/2PACX-1vQzLGpva63gb2rIilFnpZMRSb-buyr5oGh8jmDtIb8DANo4n6hDalra_WRCl4EZwO1JvaC4UIS62n5h/pubhtml\\nPython Code:\\nfrom hashlib import sha1\\ndef compute_hash(email):\\nreturn sha1(email.lower().encode('utf-8')).hexdigest()\\nYou need to call the function as follows:\\nprint(compute_hash('YOUR_EMAIL_HERE'))\\nThe quotes are required to denote that your email is a string.\\n(By Wesley Barreto)\\nYou can also use this website directly by entering your email: http://www.sha1-online.com. Then, you just have to copy and paste your hashed email in the “research” bar of the leaderboard to get your scores.\\n(Mélanie Fouesnard)\",\n",
       "    'section': '1. Introduction to Machine Learning',\n",
       "    'question': 'Computing the hash for the leaderboard and project review'},\n",
       "   {'text': 'If you get “wget is not recognized as an internal or external command”, you need to install it.\\nOn Ubuntu, run\\nsudo apt-get install wget\\nOn Windows, the easiest way to install wget is to use Chocolatey:\\nchoco install wget\\nOr you can download a binary from here and put it to any location in your PATH (e.g. C:/tools/)\\nOn Mac, the easiest way to install wget is to use brew.\\nBrew install wget\\nAlternatively, you can use a Python wget library, but instead of simply using “wget” you’ll need eeeto use\\npython -m wget\\nYou need to install it with pip first:\\npip install wget\\nAnd then in your python code, for example in your jupyter notebook, use:\\nimport wget\\nwget.download(\"URL\")\\nThis should download whatever is at the URL in the same directory as your code.\\n(Memoona Tahira)\\nAlternatively, you can read a CSV file from a URL directly with pandas:\\nurl = \"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\"\\ndf = pd.read_csv(url)\\nValid URL schemes include http, ftp, s3, gs, and file.\\nIn some cases you might need to bypass https checks:\\nimport ssl\\nssl._create_default_https_context = ssl._create_unverified_context\\nOr you can use the built-in Python functionality for downloading the files:\\nimport urllib.request\\nurl = \"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\"\\nurllib.request.urlretrieve(url, \"housing.csv\")\\nUrllib.request.urlretrieve() is a standard Python library function available on all devices and platforms. URL requests and URL data retrieval are done with the urllib.request module.\\nThe urlretrieve() function allows you to download files from URLs and save them locally. Python programs use it to download files from the internet.\\nOn any Python-enabled device or platform, you can use the urllib.request.urlretrieve() function to download the file.\\n(Mohammad Emad Sharifi)',\n",
       "    'section': '1. Introduction to Machine Learning',\n",
       "    'question': 'wget is not recognized as an internal or external command'},\n",
       "   {'text': 'You can use\\n!wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\nTo download the data too. The exclamation mark !, lets you execute shell commands inside your notebooks. This works generally for shell commands such as ls, cp, mkdir, mv etc . . .\\nFor instance, if you then want to move your data into a data directory alongside your notebook-containing directory, you could execute the following:\\n!mkdir -p ../data/\\n!mv housing.csv ../data/',\n",
       "    'section': '1. Introduction to Machine Learning',\n",
       "    'question': 'Retrieving csv inside notebook'},\n",
       "   {'text': '(Tyler Simpson)',\n",
       "    'section': '1. Introduction to Machine Learning',\n",
       "    'question': 'Windows WSL and VS Code\\nIf you have a Windows 11 device and would like to use the built in WSL to access linux you can use the Microsoft Learn link Set up a WSL development environment | Microsoft Learn. To connect this to VS Code download the Microsoft verified VS Code extension ‘WSL’ this will allow you to remotely connect to your WSL Ubuntu instance as if it was a virtual machine.'},\n",
       "   {'text': 'This is my first time using Github to upload a code. I was getting the below error message when I type\\ngit push -u origin master:\\nerror: src refspec master does not match any\\nerror: failed to push some refs to \\'https://github.com/XXXXXX/1st-Homework.git\\'\\nSolution:\\nThe error message got fixed by running below commands:\\ngit commit -m \"initial commit\"\\ngit push origin main\\nIf this is your first time to use Github, you will find a great & straightforward tutorial in this link https://dennisivy.com/github-quickstart\\n(Asia Saeed)\\nYou can also use the “upload file” functionality from GitHub for that\\nIf you write your code on Google colab you can also directly share it on your Github.\\n(By Pranab Sarma)',\n",
       "    'section': '1. Introduction to Machine Learning',\n",
       "    'question': 'Uploading the homework to Github'},\n",
       "   {'text': \"I'm trying to invert the matrix but I got error that the matrix is singular matrix\\nThe singular matrix error is caused by the fact that not every matrix can be inverted. In particular, in the homework it happens because you have to pay close attention when dealing with multiplication (the method .dot) since multiplication is not commutative! X.dot(Y) is not necessarily equal to Y.dot(X), so respect the order otherwise you get the wrong matrix.\",\n",
       "    'section': '1. Introduction to Machine Learning',\n",
       "    'question': 'Singular Matrix Error'},\n",
       "   {'text': 'I have a problem with my terminal. Command\\nconda create -n ml-zoomcamp python=3.9\\ndoesn’t work. Any of 3.8/ 3.9 / 3.10 should be all fine\\nIf you’re on Windows and just installed Anaconda, you can use Anaconda’s own terminal called “Anaconda Prompt”.\\nIf you don’t have Anaconda or Miniconda, you should install it first\\n(Tatyana Mardvilko)',\n",
       "    'section': '1. Introduction to Machine Learning',\n",
       "    'question': 'Conda is not an internal command'},\n",
       "   {'text': 'How do I read the dataset with Pandas in Windows?\\nI used the code below but not working\\ndf = pd.read_csv(\\'C:\\\\Users\\\\username\\\\Downloads\\\\data.csv\\')\\nUnlike Linux/Mac OS, Windows uses the backslash (\\\\) to navigate the files that cause the conflict with Python. The problem with using the backslash is that in Python, the \\'\\\\\\' has a purpose known as an escape sequence. Escape sequences allow us to include special characters in strings, for example, \"\\\\n\" to add a new line or \"\\\\t\" to add spaces, etc. To avoid the issue we just need to add \"r\" before the file path and Python will treat it as a literal string (not an escape sequence).\\nHere’s how we should be loading the file instead:\\ndf = pd.read_csv(r\\'C:\\\\Users\\\\username\\\\Downloads\\\\data.csv\\')\\n(Muhammad Awon)',\n",
       "    'section': '1. Introduction to Machine Learning',\n",
       "    'question': 'Read-in the File in Windows OS'},\n",
       "   {'text': 'Type the following command:\\ngit config -l | grep url\\nThe output should look like this:\\nremote.origin.url=https://github.com/github-username/github-repository-name.git\\nChange this to the following format and make sure the change is reflected using command in step 1:\\ngit remote set-url origin \"https://github-username@github.com/github-username/github-repository-name.git\"\\n(Added by Dheeraj Karra)',\n",
       "    'section': '1. Introduction to Machine Learning',\n",
       "    'question': \"'403 Forbidden' error message when you try to push to a GitHub repository\"},\n",
       "   {'text': \"I had a problem when I tried to push my code from Git Bash:\\nremote: Support for password authentication was removed on August 13, 2021.\\nremote: Please see https://docs.github.com/en/get-started/getting-started-with-git/about-remote-repositories#cloning-with-https-urls for information on currently recommended modes of authentication.\\nfatal: Authentication failed for 'https://github.com/username\\nSolution:\\nCreate a personal access token from your github account and use it when you make a push of your last changes.\\nhttps://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent\\nBruno Bedón\",\n",
       "    'section': '1. Introduction to Machine Learning',\n",
       "    'question': \"Fatal: Authentication failed for 'https://github.com/username\"},\n",
       "   {'text': \"In Kaggle, when you are trying to !wget a dataset from github (or any other public repository/location), you get the following error:\\nGetting  this error while trying to import data- !wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\n--2022-09-17 16:55:24--  https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... failed: Temporary failure in name resolution.\\nwget: unable to resolve host address 'raw.githubusercontent.com'\\nSolution:\\nIn your Kaggle notebook settings, turn on the Internet for your session. It's on the settings panel, on the right hand side of the Kaggle screen. You'll be asked to verify your phone number so Kaggle knows you are not a bot.\",\n",
       "    'section': '1. Introduction to Machine Learning',\n",
       "    'question': \"wget: unable to resolve host address 'raw.githubusercontent.com'\"},\n",
       "   {'text': 'I found this video quite helpful: Creating Virtual Environment for Python from VS Code\\n[Native Jupiter Notebooks support in VS Code] In VS Code you can also have a native Jupiter Notebooks support, i.e. you do not need to open a web browser to code in a Notebook. If you have port forwarding enabled + run a ‘jupyter notebook ‘ command from a remote machine + have a remote connection configured in .ssh/config (as Alexey’s video suggests) - VS Code can execute remote Jupyter Notebooks files on a remote server from your local machine: https://code.visualstudio.com/docs/datascience/jupyter-notebooks.\\n[Git support from VS Code] You can work with Github from VSCode - staging and commits are easy from the VS Code’s UI:  https://code.visualstudio.com/docs/sourcecontrol/overview\\n(Added by Ivan Brigida)',\n",
       "    'section': '1. Introduction to Machine Learning',\n",
       "    'question': 'Setting up an environment using VS Code'},\n",
       "   {'text': 'With regards to creating an environment for the project, do we need to run the command \"conda create -n .......\" and \"conda activate ml-zoomcamp\" everytime we open vs code to work on the project?\\nAnswer:\\n\"conda create -n ....\" is just run the first time to create the environment. Once created, you just need to run \"conda activate ml-zoomcamp\" whenever you want to use it.\\n(Added by Wesley Barreto)\\nconda env export > environment.yml will also allow you to reproduce your existing environment in a YAML file.  You can then recreate it with conda env create -f environment.yml',\n",
       "    'section': '1. Introduction to Machine Learning',\n",
       "    'question': 'Conda Environment Setup'},\n",
       "   {'text': \"I was doing Question 7 from Week1 Homework and with step6: Invert XTX, I created the inverse. Now, an inverse when multiplied by the original matrix should return in an Identity matrix. But when I multiplied the inverse with the original matrix, it gave a matrix like this:\\nInverse * Original:\\n[[ 1.00000000e+00 -1.38777878e-16]\\n[ 3.16968674e-13  1.00000000e+00]]\\nSolution:\\nIt's because floating point math doesn't work well on computers as shown here: https://stackoverflow.com/questions/588004/is-floating-point-math-broken\\n(Added by Wesley Barreto)\",\n",
       "    'section': '1. Introduction to Machine Learning',\n",
       "    'question': 'Floating Point Precision'},\n",
       "   {'text': 'Answer:\\nIt prints the information about the dataset like:\\nIndex datatype\\nNo. of entries\\nColumn information with not-null count and datatype\\nMemory usage by dataset\\nWe use it as:\\ndf.info()\\n(Added by Aadarsha Shrestha & Emoghena Itakpe)',\n",
       "    'section': '1. Introduction to Machine Learning',\n",
       "    'question': 'What does pandas.DataFrame.info() do?'},\n",
       "   {'text': \"Pandas and numpy libraries are not being imported\\nNameError: name 'np' is not defined\\nNameError: name 'pd' is not defined\\nIf you're using numpy or pandas, make sure you use the first few lines before anything else.\\nimport pandas as pd\\nimport numpy as np\\nAdded by Manuel Alejandro Aponte\",\n",
       "    'section': '1. Introduction to Machine Learning',\n",
       "    'question': \"NameError: name 'np' is not defined\"},\n",
       "   {'text': \"What if there were hundreds of columns? How do you get the columns only with numeric or object data in a more concise way?\\ndf.select_dtypes(include=np.number).columns.tolist()\\ndf.select_dtypes(include='object').columns.tolist()\\nAdded by Gregory Morris\",\n",
       "    'section': '1. Introduction to Machine Learning',\n",
       "    'question': 'How to select column by dtype'},\n",
       "   {'text': 'There are many ways to identify the shape of dataset, one of them is using .shape attribute!\\ndf.shape\\ndf.shape[0] # for identify the number of rows\\ndf.shape[1] # for identify the number of columns\\nAdded by Radikal Lukafiardi',\n",
       "    'section': '1. Introduction to Machine Learning',\n",
       "    'question': 'How to identify the shape of dataset in Pandas'},\n",
       "   {'text': 'First of all use np.dot for matrix multiplication. When you compute matrix-matrix multiplication you should understand that order of multiplying is crucial and affects the result of the multiplication!\\nDimension Mismatch\\nTo perform matrix multiplication, the number of columns in the 1st matrix should match the number of rows in the 2nd matrix. You can rearrange the order to make sure that this satisfies the condition.\\nAdded by Leah Gotladera',\n",
       "    'section': '1. Introduction to Machine Learning',\n",
       "    'question': 'How to avoid Value errors with array shapes in homework?'},\n",
       "   {'text': 'You would first get the average of the column and save it to a variable, then replace the NaN values with the average variable.\\nThis method is called imputing - when you have NaN/ null values in a column, but you do not want to get rid of the row because it has valuable information contributing to other columns.\\nAdded by Anneysha Sarkar',\n",
       "    'section': '1. Introduction to Machine Learning',\n",
       "    'question': 'Question 5: How and why do we replace the NaN values with average of the column?'},\n",
       "   {'text': 'In Question 7 we are asked to calculate\\nThe initial problem  can be solved by this, where a Matrix X is multiplied by some unknown weights w resulting in the target y.\\nAdditional reading and videos:\\nOrdinary least squares\\nMultiple Linear Regression in Matrix Form\\nPseudoinverse Solution to OLS\\nAdded by Sylvia Schmitt\\nwith commends from Dmytro Durach',\n",
       "    'section': '1. Introduction to Machine Learning',\n",
       "    'question': 'Question 7: Mathematical formula for linear regression'},\n",
       "   {'text': 'This is most likely that you interchanged the first step of the multiplication\\nYou used  instead of\\nAdded by Emmanuel Ikpesu',\n",
       "    'section': '1. Introduction to Machine Learning',\n",
       "    'question': 'Question 7: FINAL MULTIPLICATION not having 5 column'},\n",
       "   {'text': 'Note, that matrix multiplication (matrix-matrix, matrix-vector multiplication) can be written as * operator in some sources, but performed as @ operator or np.matmul() via numpy. * operator performs element-wise multiplication (Hadamard product).\\nnumpy.dot() or ndarray.dot() can be used, but for matrix-matrix multiplication @ or np.matmul() is preferred (as per numpy doc).\\nIf multiplying by a scalar numpy.multiply() or * is preferred.\\nAdded by Andrii Larkin',\n",
       "    'section': '1. Introduction to Machine Learning',\n",
       "    'question': 'Question 7: Multiplication operators.'},\n",
       "   {'text': 'If you face an error kind of ImportError: cannot import name \\'contextfilter\\' from \\'jinja2\\' (anaconda\\\\lib\\\\site-packages\\\\jinja2\\\\__init__.py) when launching a new notebook for a brand new environment.\\nSwitch to the main environment and run \"pip install nbconvert --upgrade\".\\nAdded by George Chizhmak',\n",
       "    'section': '1. Introduction to Machine Learning',\n",
       "    'question': 'Error launching Jupyter notebook'},\n",
       "   {'text': 'If you face this situation and see IPv6 addresses in the terminal, go to your System Settings > Network > your network connection > Details > Configure IPv6 > set to Manually > OK. Then try again',\n",
       "    'section': '1. Introduction to Machine Learning',\n",
       "    'question': 'wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv hangs on MacOS Ventura M1'},\n",
       "   {'text': \"Wget doesn't ship with macOS, so there are other alternatives to use.\\nNo worries, we got curl:\\nexample:\\ncurl -o ./housing.csv https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\nExplanations:\\ncurl: a utility for retrieving information from the internet.\\n-o: Tell it to store the result as a file.\\nfilename: You choose the file's name.\\nLinks: Put the web address (URL) here, and cURL will extract data from it and save it under the name you provide.\\nMore about it at:\\nCurl Documentation\\nAdded by David Espejo\",\n",
       "    'section': '1. Introduction to Machine Learning',\n",
       "    'question': 'In case you are using mac os and having trouble with WGET'},\n",
       "   {'text': \"You can use round() function or f-strings\\nround(number, 4)  - this will round number up to 4 decimal places\\nprint(f'Average mark for the Homework is {avg:.3f}') - using F string\\nAlso there is pandas.Series. round idf you need to round values in the whole Series\\nPlease check the documentation\\nhttps://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.round.html#pandas.Series.round\\nAdded by Olga Rudakova\",\n",
       "    'section': '2. Machine Learning for Regression',\n",
       "    'question': 'How to output only a certain number of decimal places'},\n",
       "   {'text': 'Here are the crucial links for this Week 2 that starts September 18, 2023\\nAsk questions for Live Sessions: https://app.sli.do/event/vsUpjYsayZ8A875Hq8dpUa/live/questions\\nCalendar for weekly meetings: https://calendar.google.com/calendar/u/0/r?cid=cGtjZ2tkbGc1OG9yb2lxa2Vwc2g4YXMzMmNAZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ&pli=1\\nWeek 2 HW: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/02-regression/homework.md\\nSubmit HW Week 2: https://docs.google.com/forms/d/e/1FAIpQLSf8eMtnErPFqzzFsEdLap_GZ2sMih-H-Y7F_IuPGqt4fOmOJw/viewform (also available at the bottom of the above link)\\nAll HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\\nGitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\\nYoutube Link: 2.X --- https://www.youtube.com/watch?v=vM3SqPNlStE&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=12\\nFAQs: https://docs.google.com/document/d/1LpPanc33QJJ6BSsyxVg-pWNMplal84TdZtq10naIhD8/edit#heading=h.lpz96zg7l47j\\n~~Nukta Bhatia~~',\n",
       "    'section': '2. Machine Learning for Regression',\n",
       "    'question': 'How do I get started with Week 2?'},\n",
       "   {'text': 'We can use histogram:\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n# Load the data\\nurl = \\'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\'\\ndf = pd.read_csv(url)\\n# EDA\\nsns.histplot(df[\\'median_house_value\\'], kde=False)\\nplt.show()\\nOR ceck skewness and describe:\\nprint(df[\\'median_house_value\\'].describe())\\n# Calculate the skewness of the \\'median_house_value\\' variable\\nskewness = df[\\'median_house_value\\'].skew()\\n# Print the skewness value\\nprint(\"Skewness of \\'median_house_value\\':\", skewness)\\n(Mohammad Emad Sharifi)',\n",
       "    'section': '2. Machine Learning for Regression',\n",
       "    'question': 'Checking long tail of data'},\n",
       "   {'text': 'It’s possible that when you follow the videos, you’ll get a Singular Matrix error. We will explain why it happens in the Regularization video. Don’t worry, it’s normal that you have it.\\nYou can also have an error because you did the inverse of X once in your code and you’re doing it a second time.\\n(Added by Cécile Guillot)',\n",
       "    'section': '2. Machine Learning for Regression',\n",
       "    'question': 'LinAlgError: Singular matrix'},\n",
       "   {'text': 'You can find a detailed description of the dataset ere https://inria.github.io/scikit-learn-mooc/python_scripts/datasets_california_housing.html\\nKS',\n",
       "    'section': '2. Machine Learning for Regression',\n",
       "    'question': 'California housing dataset'},\n",
       "   {'text': 'I was using for loops to apply rmse to list of y_val and y_pred. But the resulting rmse is all nan.\\nI found out that the problem was when my data reached the mean step after squaring the error in the rmse function. Turned out there were nan in the array, then I traced the problem back to where I first started to split the data: I had only use fillna(0) on the train data, not on the validation and test data. So the problem was fixed after I applied fillna(0) to all the dataset (train, val, test). Voila, my for loops to get rmse from all the seed values work now.\\nAdded by Sasmito Yudha Husada',\n",
       "    'section': '2. Machine Learning for Regression',\n",
       "    'question': 'Getting NaNs after applying .mean()'},\n",
       "   {'text': 'Why should we transform the target variable to logarithm distribution? Do we do this for all machine learning projects?\\nOnly if you see that your target is highly skewed. The easiest way to evaluate this is by plotting the distribution of the target variable.\\nThis can help to understand skewness and how it can be applied to the distribution of your data set.\\nhttps://en.wikipedia.org/wiki/Skewness\\nPastor Soto',\n",
       "    'section': '2. Machine Learning for Regression',\n",
       "    'question': 'Target variable transformation'},\n",
       "   {'text': 'The dataset can be read directly to pandas dataframe from the github link using the technique shown below\\ndfh=pd.read_csv(\"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\")\\nKrishna Anand',\n",
       "    'section': '2. Machine Learning for Regression',\n",
       "    'question': 'Reading the dataset directly from github'},\n",
       "   {'text': \"For users of kaggle notebooks, the dataset can be loaded through widget using the below command. Please remember that ! before wget is essential\\n!wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\nOnce the dataset is loaded to the kaggle notebook server, it can be read through the below pandas command\\ndf = pd.read_csv('housing.csv')\\nHarish Balasundaram\",\n",
       "    'section': '2. Machine Learning for Regression',\n",
       "    'question': 'Loading the dataset directly through Kaggle Notebooks'},\n",
       "   {'text': 'We can filter a dataset by using its values as below.\\ndf = df[(df[\"ocean_proximity\"] == \"<1H OCEAN\") | (df[\"ocean_proximity\"] == \"INLAND\")]\\nYou can use | for ‘OR’, and & for ‘AND’\\nAlternative:\\ndf = df[df[\\'ocean_proximity\\'].isin([\\'<1H OCEAN\\', \\'INLAND\\'])]\\nRadikal Lukafiardi',\n",
       "    'section': '2. Machine Learning for Regression',\n",
       "    'question': 'Filter a dataset by using its values'},\n",
       "   {'text': 'Above users showed how to load the dataset directly from github. Here is another useful way of doing this using the `requests` library:\\n# Get data for homework\\nimport requests\\nurl = \\'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\'\\nresponse = requests.get(url)\\nif response.status_code == 200:\\nwith open(\\'housing.csv\\', \\'wb\\') as file:\\nfile.write(response.content)\\nelse:\\nprint(\"Download failed.\")\\nTyler Simpson',\n",
       "    'section': '2. Machine Learning for Regression',\n",
       "    'question': 'Alternative way to load the data using requests'},\n",
       "   {'text': 'When creating a duplicate of your dataframe by doing the following:\\nX_train = df_train\\nX_val = df_val\\nYou’re still referencing the original variable, this is called a shallow copy. You can make sure that no references are attaching both variables and still keep the copy of the data do the following to create a deep copy:\\nX_train = df_train.copy()\\nX_val = df_val.copy()\\nAdded by Ixchel García',\n",
       "    'section': '2. Machine Learning for Regression',\n",
       "    'question': 'Null column is appearing even if I applied .fillna()'},\n",
       "   {'text': 'Yes, you can. Here we implement it ourselves to better understand how it works, but later we will only rely on Scikit-Learn’s functions. If you want to start using it earlier — feel free to do it',\n",
       "    'section': '2. Machine Learning for Regression',\n",
       "    'question': 'Can I use Scikit-Learn’s train_test_split for this week?'},\n",
       "   {'text': 'Yes, you can. We will also do that next week, so don’t worry, you will learn how to do it.',\n",
       "    'section': '2. Machine Learning for Regression',\n",
       "    'question': 'Can I use LinearRegression from Scikit-Learn for this week?'},\n",
       "   {'text': 'What are equivalents in Scikit-Learn for the linear regression with and without regularization used in week 2.\\nCorresponding function for model without regularization:\\nsklearn.linear_model.LinearRegression\\nCorresponding function for model with regularization:\\nsklearn.linear_model.Ridge\\nThe linear model from Scikit-Learn are explained  here:\\nhttps://scikit-learn.org/stable/modules/linear_model.html\\nAdded by Sylvia Schmitt',\n",
       "    'section': '2. Machine Learning for Regression',\n",
       "    'question': 'Corresponding Scikit-Learn functions for Linear Regression (with and without Regularization)'},\n",
       "   {'text': '`r` is a regularization parameter.\\nIt’s similar to `alpha` in sklearn.Ridge(), as both control the \"strength\" of regularization (increasing both will lead to stronger regularization), but mathematically not quite, here\\'s how both are used:\\nsklearn.Ridge()\\n||y - Xw||^2_2 + alpha * ||w||^2_2\\nlesson’s notebook (`train_linear_regression_reg` function)\\nXTX = XTX + r * np.eye(XTX.shape[0])\\n`r` adds “noise” to the main diagonal to prevent multicollinearity, which “breaks” finding inverse matrix.',\n",
       "    'section': '2. Machine Learning for Regression',\n",
       "    'question': 'Question 4: what is `r`, is it the same as `alpha` in sklearn.Ridge()?'},\n",
       "   {'text': 'Q: “In lesson 2.8 why is y_pred different from y? After all, we trained X_train to get the weights that when multiplied by X_train should give exactly y, or?”\\nA: linear regression is a pretty simple model, it neither can nor should fit 100% (nor any other model, as this would be the sign of overfitting). This picture might illustrate some intuition behind this, imagine X is a single feature:\\nAs our model is linear, how would you draw a line to fit all the \"dots\"?\\nYou could \"fit\" all the \"dots\" on this pic using something like scipy.optimize.curve_fit (non-linear least squares) if you wanted to, but imagine how it would perform on previously unseen data.\\nAdded by Andrii Larkin',\n",
       "    'section': '2. Machine Learning for Regression',\n",
       "    'question': 'Why linear regression doesn’t provide a “perfect” fit?'},\n",
       "   {'text': 'One of the questions on the homework calls for using a random seed of 42. When using 42, all my missing values ended up in my training dataframe and not my validation or test dataframes, why is that?\\nThe purpose of the seed value is to randomly generate the proportion split. Using a seed of 42 ensures that all learners are on the same page by getting the same behavior (in this case, all missing values ending up in the training dataframe). If using a different seed value (e.g. 9), missing values will then appear in all other dataframes.',\n",
       "    'section': '2. Machine Learning for Regression',\n",
       "    'question': 'Random seed 42'},\n",
       "   {'text': 'It is possible to do the shuffling of the dataset with the pandas built-in function pandas.DataFrame.sample.The complete dataset can be shuffled including resetting the index with the following commands:\\nSetting frac=1 will result in returning a shuffled version of the complete Dataset.\\nSetting random_state=seed will result in the same randomization as used in the course resources.\\ndf_shuffled = df.sample(frac=1, random_state=seed)\\ndf_shuffled.reset_index(drop=True, inplace=True)\\nAdded by Sylvia Schmitt',\n",
       "    'section': '2. Machine Learning for Regression',\n",
       "    'question': 'Shuffling the initial dataset using pandas built-in function'},\n",
       "   {'text': 'That’s normal. We all have different environments: our computers have different versions of OS and different versions of libraries — even different versions of Python.\\nIf it’s the case, just select the option that’s closest to your answer',\n",
       "    'section': '2. Machine Learning for Regression',\n",
       "    'question': \"The answer I get for one of the homework questions doesn't match any of the options. What should I do?\"},\n",
       "   {'text': \"In question 3 of HW02 it is mentioned: ‘For computing the mean, use the training only’. What does that mean?\\nIt means that you should use only the training data set for computing the mean, not validation or  test data set. This is how you can calculate the mean\\ndf_train['column_name'].mean( )\\nAnother option:\\ndf_train[‘column_name’].describe()\\n(Bhaskar Sarma)\",\n",
       "    'section': '2. Machine Learning for Regression',\n",
       "    'question': 'Meaning of mean in homework 2, question 3'},\n",
       "   {'text': 'When the target variable has a long tail distribution, like in prices, with a wide range, you can transform the target variable with np.log1p() method, but be aware if your target variable has negative values, this method will not work',\n",
       "    'section': '2. Machine Learning for Regression',\n",
       "    'question': 'When should we transform the target variable to logarithm distribution?'},\n",
       "   {'text': 'If we try to perform an arithmetic operation between 2 arrays of different shapes or different dimensions, it throws an error like operands could not be broadcast together with shapes. There are some scenarios when broadcasting can occur and when it fails.\\nIf this happens sometimes we can use * operator instead of dot() method to solve the issue. So that the error is solved and also we get the dot product.\\n(Santhosh Kumar)',\n",
       "    'section': '2. Machine Learning for Regression',\n",
       "    'question': 'ValueError: shapes not aligned'},\n",
       "   {'text': 'Copy of a dataframe is made with X_copy = X.copy().\\nThis is called creating a deep copy.  Otherwise it will keep changing the original dataframe if used like this: X_copy = X.\\nAny changes to X_copy will reflect back to X. This is not a real copy, instead it is a “view”.\\n(Memoona Tahira)',\n",
       "    'section': '2. Machine Learning for Regression',\n",
       "    'question': 'How to copy a dataframe without changing the original dataframe?'},\n",
       "   {'text': 'One of the most important characteristics of the normal distribution is that mean=median=mode, this means that the most popular value, the mean of the distribution and 50% of the sample are under the same value, this is equivalent to say that the area under the curve (black) is the same on the left and on the right. The long tail (red curve) is the result of having a few observations with high values, now the behaviour of the distribution changes, first of all, the area is different on each side and now the mean, median and mode are different. As a consequence, the mean is no longer representative, the range is larger than before and the probability of being on the left or on the right is not the same.\\n(Tatiana Dávila)',\n",
       "    'section': '2. Machine Learning for Regression',\n",
       "    'question': 'What does ‘long tail’ mean?'},\n",
       "   {'text': 'In statistics, the standard deviation is a measure of the amount of variation or dispersion of a set of values. A low standard deviation indicates that the values tend to be close to the mean (also called the expected value) of the set, while a high standard deviation indicates that the values are spread out over a wider range. [Wikipedia] The formula to calculate standard deviation is:\\n(Aadarsha Shrestha)',\n",
       "    'section': '2. Machine Learning for Regression',\n",
       "    'question': 'What is standard deviation?'},\n",
       "   {'text': 'The application of regularization depends on the specific situation and problem. It is recommended to consider it when training machine learning models, especially with small datasets or complex models, to prevent overfitting. However, its necessity varies depending on the data quality and size. Evaluate each case individually to determine if it is needed.\\n(Daniel Muñoz Viveros)',\n",
       "    'section': '2. Machine Learning for Regression',\n",
       "    'question': 'Do we need to apply regularization techniques always? Or only in certain scenarios?'},\n",
       "   {'text': 'As it speeds up the development:\\nprepare_df(initial_df, seed, fill_na_type)  - that prepared all 3 dataframes and 3 y_vectors. Fillna() can be done before the initial_df is split.\\nOf course, you can reuse other functions: rmse() and train_linear_regression(X,y,r) from the class notebook\\n(Ivan Brigida)',\n",
       "    'section': '2. Machine Learning for Regression',\n",
       "    'question': 'Shortcut: define functions for faster execution'},\n",
       "   {'text': 'If we have a list or series of data for example x = [1,2,3,4,5]. We can use pandas to find the standard deviation. We can pass our list into panda series and call standard deviation directly on the series pandas.Series(x).std().\\n(Quinn Avila)',\n",
       "    'section': '2. Machine Learning for Regression',\n",
       "    'question': 'How to use pandas to find standard deviation'},\n",
       "   {'text': 'Numpy and Pandas packages use different equations to compute the standard deviation. Numpy uses  population standard deviation, whereas pandas uses sample standard deviation by default.\\nNumpy\\nPandas\\npandas default standard deviation is computed using one degree of freedom. You can change degree in of freedom in NumPy to change this to unbiased estimator by using ddof parameter:\\nimport numpy as np\\nnp.std(df.weight, ddof=1)\\nThe result will be similar if we change the dof = 1 in numpy\\n(Harish Balasundaram)',\n",
       "    'section': '2. Machine Learning for Regression',\n",
       "    'question': 'Standard Deviation Differences in Numpy and Pandas'},\n",
       "   {'text': \"In pandas you can use built in Pandas function names std() to get standard deviation. For example\\ndf['column_name'].std() to get standard deviation of that column.\\ndf[['column_1', 'column_2']].std() to get standard deviation of multiple columns.\\n(Khurram Majeed)\",\n",
       "    'section': '2. Machine Learning for Regression',\n",
       "    'question': 'Standard deviation using Pandas built in Function'},\n",
       "   {'text': 'Use ‘pandas.concat’ function (https://pandas.pydata.org/docs/reference/api/pandas.concat.html) to combine two dataframes. To combine two numpy arrays use numpy.concatenate (https://numpy.org/doc/stable/reference/generated/numpy.concatenate.html) function. So the code would be as follows:\\ndf_train_combined = pd.concat([df_train, df_val])\\ny_train = np.concatenate((y_train, y_val), axis=0)\\n(George Chizhmak)',\n",
       "    'section': '2. Machine Learning for Regression',\n",
       "    'question': 'How to combine train and validation datasets'},\n",
       "   {'text': 'The Root Mean Squared Error (RMSE) is one of the primary metrics to evaluate the performance of a regression model. It calculates the average deviation between the model\\'s predicted values and the actual observed values, offering insight into the model\\'s ability to accurately forecast the target variable. To calculate RMSE score:\\nLibraries needed\\nimport numpy as np\\nfrom sklearn.metrics import mean_squared_error\\nmse = mean_squared_error(actual_values, predicted_values)\\nrmse = np.sqrt(mse)\\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\\n(Aminat Abolade)',\n",
       "    'section': '2. Machine Learning for Regression',\n",
       "    'question': 'Understanding RMSE and how to calculate RMSE score'},\n",
       "   {'text': 'If you would like to use multiple conditions as an example below you will get the error. The correct syntax for OR is |, and for AND is &\\n(Olga Rudakova)\\n–',\n",
       "    'section': '2. Machine Learning for Regression',\n",
       "    'question': 'What syntax use in Pandas for multiple conditions using logical AND and OR'},\n",
       "   {'text': 'I found this video pretty usual for understanding how we got the normal form with linear regression Normal Equation Derivation for Regression',\n",
       "    'section': '2. Machine Learning for Regression',\n",
       "    'question': 'Deep dive into normal equation for regression'},\n",
       "   {'text': '(Hrithik Kumar Advani)',\n",
       "    'section': '2. Machine Learning for Regression',\n",
       "    'question': 'Useful Resource for Missing Data Treatment\\nhttps://www.kaggle.com/code/parulpandey/a-guide-to-handling-missing-values-in-python/notebook'},\n",
       "   {'text': 'The instruction for applying log transformation to the ‘median_house_value’ variable is provided before Q3 in the homework for Week-2 under the ‘Prepare and split the dataset’ heading.\\nHowever, this instruction is absent in the subsequent questions of the homework, and I got stuck with Q5 for a long time, trying to figure out why my RMSE was so huge, when it clicked to me that I forgot to apply log transformation to the target variable. Please remember to apply log transformation to the target variable for each question.\\n(Added by Soham Mundhada)',\n",
       "    'section': '2. Machine Learning for Regression',\n",
       "    'question': 'Caution for applying log transformation in Week-2 2023 cohort homework'},\n",
       "   {'text': 'Version 0.24.2 and Python 3.8.11\\n(Added by Diego Giraldo)',\n",
       "    'section': '3. Machine Learning for Classification',\n",
       "    'question': 'What sklearn version is Alexey using in the youtube videos?'},\n",
       "   {'text': 'Week 3 HW: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/03-classification/homework.md\\nSubmit HW Week 3: https://docs.google.com/forms/d/e/1FAIpQLSeXS3pqsv_smRkYmVx-7g6KIZDnG29g2s7pdHo-ASKNqtfRFQ/viewform\\nAll HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\\nEvaluation Matrix: https://docs.google.com/spreadsheets/d/e/2PACX-1vQCwqAtkjl07MTW-SxWUK9GUvMQ3Pv_fF8UadcuIYLgHa0PlNu9BRWtfLgivI8xSCncQs82HDwGXSm3/pubhtml\\nGitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\\nYoutube Link: 3.X --- https://www.youtube.com/watch?v=0Zw04wdeTQo&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=29\\n~~Nukta Bhatia~~',\n",
       "    'section': '3. Machine Learning for Classification',\n",
       "    'question': 'How do I get started with Week 3?'},\n",
       "   {'text': \"The error message “could not convert string to float: ‘Nissan’” typically occurs when a machine learning model or function is expecting numerical input, but receives a string instead. In this case, it seems like the model is trying to convert the car brand ‘Nissan’ into a numerical value, which isn’t possible.\\nTo resolve this issue, you can encode categorical variables like car brands into numerical values. One common method is one-hot encoding, which creates new binary columns for each category/label present in the original column.\\nHere’s an example of how you can perform one-hot encoding using pandas:\\nimport pandas as pd\\n# Assuming 'data' is your DataFrame and 'brand' is the column with car brands\\ndata_encoded = pd.get_dummies(data, columns=['brand'])\\nIn this code, pd.get_dummies() creates a new DataFrame where the ‘brand’ column is replaced with binary columns for each brand (e.g., ‘brand_Nissan’, ‘brand_Toyota’, etc.). Each row in the DataFrame has a 1 in the column that corresponds to its brand and 0 in all other brand columns.\\n-Mohammad Emad Sharifi-\",\n",
       "    'section': '3. Machine Learning for Classification',\n",
       "    'question': \"Could not convert string to float:’Nissan’rt string to float: 'Nissan'\"},\n",
       "   {'text': 'Solution: Mutual Information score calculates the relationship between categorical variables or discrete variables. So in the homework, because the target which is median_house_value is continuous, we had to change it to binary format which in other words, makes its values discrete as either 0 or 1. If we allowed it to remain in the continuous variable format, the mutual information score could be calculated, but the algorithm would have to divide the continuous variables into bins and that would be highly subjective. That is why continuous variables are not used for mutual information score calculation.\\n—Odimegwu David—-',\n",
       "    'section': '3. Machine Learning for Classification',\n",
       "    'question': 'Why did we change the targets to binary format when calculating mutual information score in the homework?'},\n",
       "   {'text': \"Q2 asks about correlation matrix and converting median_house_value from numeric to binary. Just to make sure here we are only dealing with df_train not df_train_full, right? As the question explicitly mentions the train dataset.\\nYes. I think it is only on df_train. The reason behind this is that df_train_full also contains the validation dataset, so at this stage we don't want to make conclusions based on the validation data, since we want to test how we did without using that portion of the data.\\nPastor Soto\",\n",
       "    'section': '3. Machine Learning for Classification',\n",
       "    'question': 'What data should we use for correlation matrix'},\n",
       "   {'text': \"The background of any dataframe can be colored (not only the correlation matrix) based on the numerical values the dataframe contains by using the method pandas.io.formats.style.Styler.background_graident.\\nHere an example on how to color the correlation matrix. A color map of choice can get passed, here ‘viridis’ is used.\\n# ensure to have only numerical values in the dataframe before calling 'corr'\\ncorr_mat = df_numerical_only.corr()\\ncorr_mat.style.background_gradient(cmap='viridis')\\nHere is an example of how the coloring will look like using a dataframe containing random values and applying “background_gradient” to it.\\nnp.random.seed = 3\\ndf_random = pd.DataFrame(data=np.random.random(3*3).reshape(3,3))\\ndf_random.style.background_gradient(cmap='viridis')\\nAdded by Sylvia Schmitt\",\n",
       "    'section': '3. Machine Learning for Classification',\n",
       "    'question': 'Coloring the background of the pandas.DataFrame.corr correlation matrix directly'},\n",
       "   {'text': 'data_corr = pd.DataFrame(data_num.corr().round(3).abs().unstack().sort_values(ascending=False))\\ndata_corr.head(10)\\nAdded by Harish Balasundaram\\nYou can also use seaborn to create a heatmap with the correlation. The code for doing that:\\nsns.heatmap(df[numerical_features].corr(),\\nannot=True,\\nsquare=True,\\nfmt=\".2g\",\\ncmap=\"crest\")\\nAdded by Cecile Guillot\\nYou can refine your heatmap and plot only a triangle, with a blue to red color gradient, that will show every correlation between your numerical variables without redundant information with this function:\\nWhich outputs, in the case of churn dataset:\\n(Mélanie Fouesnard)',\n",
       "    'section': '3. Machine Learning for Classification',\n",
       "    'question': 'Identifying highly correlated feature pairs easily through unstack'},\n",
       "   {'text': \"Should we perform EDA on the base of train or train+validation or train+validation+test dataset?\\nIt's indeed good practice to only rely on the train dataset for EDA. Including validation might be okay. But we aren't supposed to touch the test dataset, even just looking at it isn't a good idea. We indeed pretend that this is the future unseen data\\nAlena Kniazeva\",\n",
       "    'section': '3. Machine Learning for Classification',\n",
       "    'question': 'What data should be used for EDA?'},\n",
       "   {'text': 'Validation dataset helps to validate models and prediction on unseen data. This helps get an estimate on its performance on fresh data. It helps optimize the model.\\nEdidiong Esu\\nBelow is an extract of Alexey\\'s book explaining this point. Hope is useful\\nWhen we apply the fit method, this method is looking at the content of the df_train dictionaries we are passing to the DictVectorizer instance, and fit is figuring out (training) how to map the values of these dictionaries. If categorical, applies one-hot encoding, if numerical it will leave it as it is.\\nWith this context, if we apply the fit to the validation model, we are \"giving the answers\" and we are not letting the \"fit\" do its job for data that we haven\\'t seen. By not applying the fit to the validation model we can know how well it was trained.\\nBelow is an extract of Alexey\\'s book explaining this point.\\nHumberto Rodriguez\\nThere is no need to initialize another instance of dictvectorizer after fitting it on the train set as it will overwrite what it learnt from being fit on the train data.\\nThe correct way is to fit_transform the train set, and only transform the validation and test sets.\\nMemoona Tahira',\n",
       "    'section': '3. Machine Learning for Classification',\n",
       "    'question': 'Fitting DictVectorizer on validation'},\n",
       "   {'text': 'For Q5 in homework, should we calculate the smallest difference in accuracy in real values (i.e. -0.001 is less than -0.0002) or in absolute values (i.e. 0.0002 is less than 0.001)?\\nWe should select the “smallest” difference, and not the “lowest”, meaning we should reason in absolute values.\\nIf the difference is negative, it means that the model actually became better when we removed the feature.',\n",
       "    'section': '3. Machine Learning for Classification',\n",
       "    'question': 'Feature elimination'},\n",
       "   {'text': \"Instead use the method “.get_feature_names_out()” from DictVectorizer function and the warning will be resolved , but we need not worry about the waning as there won't be any warning\\nSanthosh Kumar\",\n",
       "    'section': '3. Machine Learning for Classification',\n",
       "    'question': 'FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2'},\n",
       "   {'text': 'Fitting the logistic regression takes a long time / kernel crashes when calling predict() with the fitted model.\\nMake sure that the target variable for the logistic regression is binary.\\nKonrad Muehlberg',\n",
       "    'section': '3. Machine Learning for Classification',\n",
       "    'question': 'Logistic regression crashing Jupyter kernel'},\n",
       "   {'text': 'Ridge regression is a linear regression technique used to mitigate the problem of multicollinearity (when independent variables are highly correlated) and prevent overfitting in predictive modeling. It adds a regularization term to the linear regression cost function, penalizing large coefficients.\\nsag Solver: The sag solver stands for \"Stochastic Average Gradient.\" It\\'s particularly suitable for large datasets, as it optimizes the regularization term using stochastic gradient descent (SGD). sag can be faster than some other solvers for large datasets.\\nAlpha: The alpha parameter  controls the strength of the regularization in Ridge regression. A higher alpha value leads to stronger regularization, which means the model will have smaller coefficient values, reducing the risk of overfitting.\\nfrom sklearn.linear_model import Ridge\\nridge = Ridge(alpha=alpha, solver=\\'sag\\', random_state=42)\\nridge.fit(X_train, y_train)\\nAminat Abolade',\n",
       "    'section': '3. Machine Learning for Classification',\n",
       "    'question': 'Understanding Ridge'},\n",
       "   {'text': 'DictVectorizer(sparse=True) produces CSR format, which is both more memory efficient and converges better during fit(). Basically it stores non-zero values and indices instead of adding a column for each class of each feature (models of cars produced 900+ columns alone in the current task).\\nUsing “sparse” format like on the picture above, both via pandas.get_dummies() and DictVectorizer(sparse=False) - is slower (around 6-8min for Q6 task - Linear/Ridge Regression) for high amount of classes (like models of cars for eg) and gives a bit “worse” results in both Logistic and Linear/Ridge Regression, while also producing convergence warnings for Linear/Ridge Regression.\\nLarkin Andrii',\n",
       "    'section': '3. Machine Learning for Classification',\n",
       "    'question': 'pandas.get_dummies() and DictVectorizer(sparse=False) produce the same type of one-hot encodings:'},\n",
       "   {'text': 'Ridge with sag solver requires feature to be of the same scale. You may get the following warning: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\\nPlay with different scalers. See notebook-scaling-ohe.ipynb\\nDmytro Durach\\n(Oscar Garcia)  Use a StandardScaler for the numeric fields and OneHotEncoder (sparce = False) for the categorical features.  This help with the warning. Separate the features (num/cat) without using the encoder first and see if that helps.',\n",
       "    'section': '3. Machine Learning for Classification',\n",
       "    'question': 'Convergence Problems in W3Q6'},\n",
       "   {'text': \"When encountering convergence errors during the training of a Ridge regression model, consider the following steps:\\nFeature Normalization: Normalize your numerical features using techniques like MinMaxScaler or StandardScaler. This ensures that numerical features are on a \\tsimilar scale, preventing convergence issues.\\nCategorical Feature Encoding: If your dataset includes categorical features, apply \\tcategorical encoding techniques such as OneHotEncoder (OHE) to \\tconvert them into a numerical format. OHE is commonly used to represent categorical variables as binary vectors, making them compatible with regression models like Ridge.\\nCombine Features: After \\tnormalizing numerical features and encoding categorical features using OneHotEncoder, combine them to form a single feature matrix (X_train). This combined dataset serves as the input for training the Ridge regression model.\\nBy following these steps, you can address convergence errors and enhance the stability of your Ridge model training process. It's important to note that the choice of encoding method, such as OneHotEncoder, is appropriate for handling categorical features in this context.\\nYou can find an example here.\\n \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tOsman Ali\",\n",
       "    'section': '3. Machine Learning for Classification',\n",
       "    'question': 'Dealing with Convergence in Week 3 q6'},\n",
       "   {'text': 'A sparse matrix is more memory-efficient because it only stores the non-zero values and their positions in memory. This is particularly useful when working with large datasets with many zero or missing values.\\nThe default DictVectorizer configuration is a sparse matrix. For week3 Q6 using the default sparse is an interesting option because of the size of the matrix. Training the model was also more performant and didn’t give an error message like dense mode.\\n \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tQuinn Avila',\n",
       "    'section': '3. Machine Learning for Classification',\n",
       "    'question': 'Sparse matrix compared dense matrix'},\n",
       "   {'text': 'The warnings on the jupyter notebooks can be disabled/ avoided with the following comments:\\nImport warnings\\nwarnings.filterwarnings(“ignore”)\\nKrishna Anand',\n",
       "    'section': '3. Machine Learning for Classification',\n",
       "    'question': 'How  to Disable/avoid Warnings in Jupyter Notebooks'},\n",
       "   {'text': 'Question: Regarding RMSE, how do we decide on the correct score to choose? In the study group discussion    about week two homework, all of us got it wrong and one person had the lowest score selected as well.\\nAnswer: You need to find RMSE for each alpha. If RMSE scores  are equal, you will select the lowest alpha.\\nAsia Saeed',\n",
       "    'section': '3. Machine Learning for Classification',\n",
       "    'question': 'How to select the alpha parameter in Q6'},\n",
       "   {'text': 'Question: Could you please help me with HW3 Q3: \"Calculate the mutual information score with the (binarized) price for the categorical variable that we have. Use the training set only.\" What is the second variable that we need to use to calculate the mutual information score?\\nAnswer: You need to calculate the mutual info score between the binarized price (above_average) variable & ocean_proximity, the only original categorical variable in the dataset.\\nAsia Saeed',\n",
       "    'section': '3. Machine Learning for Classification',\n",
       "    'question': 'Second variable that we need to use to calculate the mutual information score'},\n",
       "   {'text': 'Do we need to train the model only with the features: total_rooms, total_bedrooms, population and households? or with all the available features and then pop once at a time each of the previous features and train the model to make the accuracy comparison?\\nYou need to create a list of all features in this question and evaluate the model one time to obtain the accuracy, this will be the original accuracy, and then remove one feature each time, and in each time, train the model, find the accuracy and the difference between the original accuracy and the found accuracy. Finally, find out which feature has the smallest absolute accuracy difference.\\nWhile calculating differences between accuracy scores while training on the whole model, versus dropping one feature at a time and comparing its accuracy to the model to judge impact of the feature on the accuracy of the model, do we take the smallest difference or smallest absolute difference?\\nSince order of subtraction between the two accuracy scores can result in a negative number, we will take its absolute value as we are interested in the smallest value difference, not the lowest difference value. Case in point, if difference is -4 and -2, the smallest difference is abs(-2), and not abs(-4)',\n",
       "    'section': '3. Machine Learning for Classification',\n",
       "    'question': 'Features for homework Q5'},\n",
       "   {'text': 'Both work in similar ways. That is, to convert categorical features to numerical variables for use in training the model. But the difference lies in the input. OneHotEncoder uses an array as input while DictVectorizer uses a dictionary.\\nBoth will produce the same result. But when we use OneHotEncoder, features are sorted alphabetically. When you use DictVectorizer you stack features that you want.\\nTanya Mard',\n",
       "    'section': '3. Machine Learning for Classification',\n",
       "    'question': 'What is the difference between OneHotEncoder and DictVectorizer?'},\n",
       "   {'text': 'They are basically the same. There are some key differences with regards to their input/output types, handling of missing values, etc, but they are both techniques to one-hot-encode categorical variables with identical results. The biggest difference is get_dummies are a convenient choice when you are working with Pandas Dataframes, while if you are building a scikit-learn-based machine learning pipeline and need to handle categorical data as part of that pipeline, OneHotEncoder is a more suitable choice. [Abhirup Ghosh]',\n",
       "    'section': '3. Machine Learning for Classification',\n",
       "    'question': 'What is the difference between pandas get_dummies and sklearn OnehotEncoder?'},\n",
       "   {'text': \"For the test_train_split question on week 3's homework, are we supposed to use 42 as the random_state in both splits or only the 1st one?\\nAnswer: for both splits random_state = 42 should be used\\n(Bhaskar Sarma)\",\n",
       "    'section': '3. Machine Learning for Classification',\n",
       "    'question': 'Use of random seed in HW3'},\n",
       "   {'text': 'Should correlation be calculated after splitting or before splitting. And lastly I know how to find the correlation but how do i find the two most correlated features.\\nAnswer: Correlation matrix of your train dataset. Thus, after splitting. Two most correlated features are the ones having the highest correlation coefficient in terms of absolute values.',\n",
       "    'section': '3. Machine Learning for Classification',\n",
       "    'question': 'Correlation before or after splitting the data'},\n",
       "   {'text': 'Make sure that the features used in ridge regression model are only NUMERICAL ones not categorical.\\nDrop all categorical features first before proceeding.\\n(Aileah Gotladera)\\nWhile it is True that ridge regression accepts only numerical values, the categorical ones can be useful for your model. You have to transform them using one-hot encoding before training the model. To avoid the error of non convergence, put sparse=True when doing so.\\n(Erjon)',\n",
       "    'section': '3. Machine Learning for Classification',\n",
       "    'question': 'Features in Ridge Regression Model'},\n",
       "   {'text': \"You need to use all features. and price for target. Don't include the average variable we created before.\\nIf you use DictVectorizer then make sure to use sparce=True to avoid convergence errors\\nI also used StandardScalar for numerical variable you can try running with or without this\\n(Peter Pan)\",\n",
       "    'section': '3. Machine Learning for Classification',\n",
       "    'question': 'Handling Column Information for Homework 3 Question 6'},\n",
       "   {'text': 'Use sklearn.preprocessing encoders and scalers, e.g. OneHotEncoder, OrdinalEncoder, and StandardScaler.',\n",
       "    'section': '3. Machine Learning for Classification',\n",
       "    'question': 'Transforming Non-Numerical Columns into Numerical Columns'},\n",
       "   {'text': 'These both methods receive the dictionary as an input. While the DictVectorizer will store the big vocabulary and takes more memory. FeatureHasher create a vectors with predefined length. They are both used for categorical features.\\nWhen you have a high cardinality for categorical features better to use FeatureHasher. If you want to preserve feature names in transformed data and have a small number of unique values is DictVectorizer. But your choice will dependence on your data.\\nYou can read more by follow the link https://scikit-learn.org/stable/auto_examples/text/plot_hashing_vs_dict_vectorizer.html\\nOlga Rudakova',\n",
       "    'section': '3. Machine Learning for Classification',\n",
       "    'question': 'What is the better option FeatureHasher or DictVectorizer'},\n",
       "   {'text': '(Question by Connie S.)\\nThe reason it\\'s good/recommended practice to do it after splitting is to avoid data leakage - you don\\'t want any data from the test set influencing the training stage (similarly from the validation stage in the initial training). See e.g. scikit-learn documentation on \"Common pitfalls and recommended practices\": https://scikit-learn.org/stable/common_pitfalls.html\\nAnswered/added by Rileen Sinha',\n",
       "    'section': '3. Machine Learning for Classification',\n",
       "    'question': \"Isn't it easier to use DictVertorizer or get dummies before splitting the data into train/val/test? Is there a reason we wouldn't do this? Or is it the same either way?\"},\n",
       "   {'text': 'If you are getting 1.0 as accuracy then there is a possibility you have overfitted the model. Dropping the column msrp/price can help you solve this issue.\\nAdded by Akshar Goyal',\n",
       "    'section': '3. Machine Learning for Classification',\n",
       "    'question': 'HW3Q4 I am getting 1.0 as accuracy. Should I use the closest option?'},\n",
       "   {'text': 'We can use sklearn & numpy packages to calculate Root Mean Squared Error\\nfrom sklearn.metrics import mean_squared_error\\nimport numpy as np\\nRmse = np.sqrt(mean_squared_error(y_pred, y_val/ytest)\\nAdded by Radikal Lukafiardi\\nYou can also refer to Alexey’s notebook for Week 2:\\nhttps://github.com/alexeygrigorev/mlbookcamp-code/blob/master/chapter-02-car-price/02-carprice.ipynb\\nwhich includes the following code:\\ndef rmse(y, y_pred):\\nerror = y_pred - y\\nmse = (error ** 2).mean()\\nreturn np.sqrt(mse)\\n(added by Rileen Sinha)',\n",
       "    'section': '3. Machine Learning for Classification',\n",
       "    'question': 'How to calculate Root Mean Squared Error?'},\n",
       "   {'text': 'The solution is to use “get_feature_names_out” instead. See details: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html\\nGeorge Chizhmak',\n",
       "    'section': '3. Machine Learning for Classification',\n",
       "    'question': \"AttributeError: 'DictVectorizer' object has no attribute 'get_feature_names'\"},\n",
       "   {'text': 'To use RMSE without math or numpy, ‘sklearn.metrics’ has a mean_squared_error function with a squared kwarg (defaults to True). Setting squared to False will return the RMSE.\\nfrom sklearn.metrics import mean_squared_error\\nrms = mean_squared_error(y_actual, y_predicted, squared=False)\\nSee details: https://stackoverflow.com/questions/17197492/is-there-a-library-function-for-root-mean-square-error-rmse-in-python\\nAhmed Okka',\n",
       "    'section': '3. Machine Learning for Classification',\n",
       "    'question': 'Root Mean Squared Error'},\n",
       "   {'text': 'This article explains different encoding techniques used https://towardsdatascience.com/all-about-categorical-variable-encoding-305f3361fd02\\nHrithik Kumar Advani',\n",
       "    'section': '3. Machine Learning for Classification',\n",
       "    'question': 'Encoding Techniques'},\n",
       "   {'text': \"I got this error multiple times here is the code:\\n“accuracy_score(y_val, y_pred >= 0.5)”\\nTypeError: 'numpy.float64' object is not callable\\nI solve it using\\nfrom sklearn import metrics\\nmetrics.accuracy_score(y_train, y_pred>= 0.5)\\nOMAR Wael\",\n",
       "    'section': '4. Evaluation Metrics for Classification',\n",
       "    'question': 'Error in use of accuracy_score from sklearn in jupyter (sometimes)'},\n",
       "   {'text': 'Week 4 HW: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/04-evaluation/homework.md\\nAll HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\\nEvaluation Matrix: https://docs.google.com/spreadsheets/d/e/2PACX-1vQCwqAtkjl07MTW-SxWUK9GUvMQ3Pv_fF8UadcuIYLgHa0PlNu9BRWtfLgivI8xSCncQs82HDwGXSm3/pubhtml\\nGitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\\nYouTube Link: 4.X --- https://www.youtube.com/watch?v=gmg5jw1bM8A&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=40\\nSci-Kit Learn on Evaluation:\\nhttps://scikit-learn.org/stable/model_selection.html\\n~~Nukta Bhatia~~',\n",
       "    'section': '4. Evaluation Metrics for Classification',\n",
       "    'question': 'How do I get started with Week 4?'},\n",
       "   {'text': 'https://datatalks-club.slack.com/archives/C0288NJ5XSA/p1696475675887119\\nMetrics can be used on a series or a dataframe\\n~~Ella Sahnan~~',\n",
       "    'section': '4. Evaluation Metrics for Classification',\n",
       "    'question': 'Using a variable to score'},\n",
       "   {'text': 'Ie particularly in module-04 homework Qn2 vs Qn5. https://datatalks-club.slack.com/archives/C0288NJ5XSA/p1696760905214979\\nRefer to the sklearn docs, random_state is to ensure the “randomness” that is used to shuffle dataset is reproducible, and it usually requires both random_state and shuffle params to be set accordingly.\\n~~Ella Sahnan~~',\n",
       "    'section': '4. Evaluation Metrics for Classification',\n",
       "    'question': 'Why do we sometimes use random_state and not at other times?'},\n",
       "   {'text': 'How to get classification metrics - precision, recall, f1 score, accuracy simultaneously\\nUse classification_report from sklearn. For more info check here.\\nAbhishek N',\n",
       "    'section': '4. Evaluation Metrics for Classification',\n",
       "    'question': 'How to get all classification metrics?'},\n",
       "   {'text': 'I am getting multiple thresholds with the same F1 score, does this indicate I am doing something wrong or is there a method for choosing? I would assume just pick the lowest?\\nChoose the one closest to any of the options\\nAdded by Azeez Enitan Edunwale\\nYou can always use scikit-learn (or other standard libraries/packages) to verify results obtained using your own code, e.g. you can use  “classification_report” (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) to obtain precision, recall and F1-score.\\nAdded by Rileen Sinha',\n",
       "    'section': '4. Evaluation Metrics for Classification',\n",
       "    'question': 'Multiple thresholds for Q4'},\n",
       "   {'text': \"Solution description: duplicating the\\ndf.churn = (df.churn == 'yes').astype(int)\\nThis is causing you to have only 0's in your churn column. In fact, match with the error you are getting:  ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0.\\nIt is telling us that it only contains 0's.\\nDelete one of the below cells and you will get the accuracy\\nHumberto Rodriguez\",\n",
       "    'section': '4. Evaluation Metrics for Classification',\n",
       "    'question': 'ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0'},\n",
       "   {'text': 'Use Yellowbrick. Yellowbrick in a library that combines scikit-learn with matplotlib to produce visualizations for your models. It produces colorful classification reports.\\nKrishna Annad',\n",
       "    'section': '4. Evaluation Metrics for Classification',\n",
       "    'question': 'Method to get beautiful classification report'},\n",
       "   {'text': 'That’s fine, use the closest option',\n",
       "    'section': '4. Evaluation Metrics for Classification',\n",
       "    'question': 'I’m not getting the exact result in homework'},\n",
       "   {'text': 'Check the solutions from the 2021 iteration of the course. You should use roc_auc_score.',\n",
       "    'section': '4. Evaluation Metrics for Classification',\n",
       "    'question': 'Use AUC to evaluate feature importance of numerical variables'},\n",
       "   {'text': 'When calculating the ROC AUC score using sklearn.metrics.roc_auc_score the function expects two parameters “y_true” and “y_score”. So for each numerical value in the dataframe it will be passed as the “y_score” to the function and the target variable will get passed a “y_true” each time.\\nSylvia Schmitt',\n",
       "    'section': '4. Evaluation Metrics for Classification',\n",
       "    'question': 'Help with understanding: “For each numerical value, use it as score and compute AUC”'},\n",
       "   {'text': 'You must use the `dt_val` dataset to compute the metrics asked in Question 3 and onwards, as you did in Question 2.\\nDiego Giraldo',\n",
       "    'section': '4. Evaluation Metrics for Classification',\n",
       "    'question': 'What dataset should I use to compute the metrics in Question 3'},\n",
       "   {'text': \"What does this line do?\\nKFold(n_splits=n_splits, shuffle=True, random_state=1)\\nIf I do it inside the loop [0.01, 0.1, 1, 10] or outside the loop in Q6, HW04 it doesn't make any difference to my answers. I am wondering why and what is the right way, although it doesn't make a difference!\\nDid you try using a different random_state? From my understanding, KFold just makes N (which is equal to n_splits) separate pairs of datasets (train+val).\\nhttps://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\\nIn my case changing random state changed results\\n(Arthur Minakhmetov)\\nChanging the random state makes a difference in my case too, but not whether it is inside or outside the for loop. I think I have got the answer. kFold = KFold(n_splits=n_splits, shuffle = True, random_state = 1)  is just a generator object and it contains only the information n_splits, shuffle and random_state. The k-fold splitting actually happens in the next for loop for train_idx, val_idx in kFold.split(df_full_train): . So it doesn't matter where we generate the object, before or after the first loop. It will generate the same information. But from the programming point of view, it is better to do it before the loop. No point doing it again and again inside the loop\\n(Bhaskar Sarma)\\nIn case of KFold(n_splits=n_splits, shuffle=True, random_state=1) and C= [0.01, 0.1, 1, 10], it is better to loop through the different values of Cs as the video explained. I had separate train() and predict() functions, which were reused after dividing the dataset via KFold. The model ran about 10 minutes and provided a good score.\\n(Ani Mkrtumyan)\",\n",
       "    'section': '4. Evaluation Metrics for Classification',\n",
       "    'question': 'What does KFold do?'},\n",
       "   {'text': \"I’m getting “ValueError: multi_class must be in ('ovo', 'ovr')” when using roc_auc_score to evaluate feature importance of numerical variables in question 1.\\nI was getting this error because I was passing the parameters to roc_auc_score incorrectly (df_train[col] , y_train) . The correct way is to pass the parameters in this way: roc_auc_score(y_train, df_train[col])\\nAsia Saeed\",\n",
       "    'section': '4. Evaluation Metrics for Classification',\n",
       "    'question': \"ValueError: multi_class must be in ('ovo', 'ovr')\"},\n",
       "   {'text': 'from tqdm.auto import tqdm\\nTqdm - terminal progress bar\\nKrishna Anand',\n",
       "    'section': '4. Evaluation Metrics for Classification',\n",
       "    'question': 'Monitoring Wait times and progress of the code execution can be done with:'},\n",
       "   {'text': 'Inverting or negating variables with ROC AUC scores less than the threshold is a valuable technique to improve feature importance and model performance when dealing with negatively correlated features. It helps ensure that the direction of the correlation aligns with the expectations of most machine learning algorithms.\\nAileah Gotladera',\n",
       "    'section': '4. Evaluation Metrics for Classification',\n",
       "    'question': 'What is the use of inverting or negating the variables less than the threshold?'},\n",
       "   {'text': 'In case of using predict(X) for this task we are getting the binary classification predictions which are 0 and 1. This may lead to incorrect evaluation values.\\nThe solution is to use predict_proba(X)[:,1], where we get the probability that the value belongs to one of the classes.\\nVladimir Yesipov\\nPredict_proba shows probailites per class.\\nAni Mkrtumyan',\n",
       "    'section': '4. Evaluation Metrics for Classification',\n",
       "    'question': 'Difference between predict(X) and predict_proba(X)[:, 1]'},\n",
       "   {'text': 'For churn/not churn predictions, I need help to interpret the following scenario please, what is happening when:\\nThe threshold is 1.0\\nFPR is 0.0\\nAnd TPR is 0.0\\nWhen the threshold is 1.0, the condition for belonging to the positive class (churn class) is g(x)>=1.0 But g(x) is a sigmoid function for a binary classification problem. It has values between 0 and 1. This function  never becomes equal to outermost values, i.e. 0 and 1.\\nThat is why there is no object, for which churn-condition could be satisfied. And that is why there is no any positive  (churn) predicted value (neither true positive, nor false positive), if threshold is equal to 1.0\\nAlena Kniazeva',\n",
       "    'section': '4. Evaluation Metrics for Classification',\n",
       "    'question': 'Why are FPR and TPR equal to 0.0, when threshold = 1.0?'},\n",
       "   {'text': \"Matplotlib has a cool method to annotate where you could provide an X,Y point and annotate with an arrow and text. For example this will show an arrow pointing to the x,y point optimal threshold.\\nplt.annotate(f'Optimal Threshold: {optimal_threshold:.2f}\\\\nOptimal F1 Score: {optimal_f1_score:.2f}',\\nxy=(optimal_threshold, optimal_f1_score),\\nxytext=(0.3, 0.5),\\ntextcoords='axes fraction',\\narrowprops=dict(facecolor='black', shrink=0.05))\\nQuinn Avila\",\n",
       "    'section': '4. Evaluation Metrics for Classification',\n",
       "    'question': 'How can I annotate a graph?'},\n",
       "   {'text': \"It's a complex and abstract topic and it requires some time to understand. You can move on without fully understanding the concept.\\nNonetheless, it might be useful for you to rewatch the video, or even watch videos/lectures/notes by other people on this topic, as the ROC AUC is one of the most important metrics used in Binary Classification models.\",\n",
       "    'section': '4. Evaluation Metrics for Classification',\n",
       "    'question': 'I didn’t fully understand the ROC curve. Can I move on?'},\n",
       "   {'text': 'One main reason behind that, is the way of splitting data. For example, we want to split data into train/validation/test with the ratios 60%/20%/20% respectively.\\nAlthough the following two options end up with the same ratio, the data itself is a bit different and not 100% matching in each case.\\n1)\\ndf_train, df_temp = train_test_split(df, test_size=0.4, random_state=42)\\ndf_val, df_test = train_test_split(df_temp, test_size=0.5, random_state=42)\\n2)\\ndf_full_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\\ndf_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=42)\\nTherefore, I would recommend using the second method which is more consistent with the lessons and thus the homeworks.\\nIbraheem Taha',\n",
       "    'section': '4. Evaluation Metrics for Classification',\n",
       "    'question': 'Why do I have different values of accuracy than the options in the homework?'},\n",
       "   {'text': 'You can find the intercept between these two curves using numpy diff (https://numpy.org/doc/stable/reference/generated/numpy.diff.html ) and sign (https://numpy.org/doc/stable/reference/generated/numpy.sign.html):\\nI suppose here that you have your df_scores ready with your three columns ‘threshold’, ‘precision’ and ‘recall’:\\nYou want to know at which index (or indices) you have your intercept between precision and recall (namely: where the sign of the difference between precision and recall changes):\\nidx = np.argwhere(\\nnp.diff(\\nnp.sign(np.array(df_scores[\"precision\"]) - np.array(df_scores[\"recall\"]))\\n)\\n).flatten()\\nYou can print the result to easily read it:\\nprint(\\nf\"The precision and recall curves intersect at a threshold equal to {df_scores.loc[idx][\\'threshold\\']}.\"\\n)\\n(Mélanie Fouesnard)',\n",
       "    'section': '4. Evaluation Metrics for Classification',\n",
       "    'question': 'How to find the intercept between precision and recall curves by using numpy?'},\n",
       "   {'text': \"In the demonstration video, we are shown how to calculate the precision and recall manually. You can use the Scikit Learn library to calculate the confusion matrix. precision, recall, f1_score without having to first define true positive, true negative, false positive, and false negative.\\nfrom sklearn.metrics import precision_score, recall_score, f1_score\\nprecision_score(y_true, y_pred, average='binary')\\nrecall_score(y_true, y_pred, average='binary')\\nf1_score(y_true, y_pred, average='binary')\\nRadikal Lukafiardi\",\n",
       "    'section': '4. Evaluation Metrics for Classification',\n",
       "    'question': 'Compute Recall, Precision, and F1 Score using scikit-learn library'},\n",
       "   {'text': 'Cross-validation evaluates the performance of a model and chooses the best hyperparameters. Cross-validation does this by splitting the dataset into multiple parts (folds), typically 5 or 10. It then trains and evaluates your model multiple times, each time using a different fold as the validation set and the remaining folds as the training set.\\n\"C\" is a hyperparameter that is typically associated with regularization in models like Support Vector Machines (SVM) and logistic regression.\\nSmaller \"C\" values: They introduce more regularization, which means the model will try to find a simpler decision boundary, potentially underfitting the data. This is because it penalizes the misclassification of training examples more severely.\\nLarger \"C\" values: They reduce the regularization effect, allowing the model to fit the training data more closely, potentially overfitting. This is because it penalizes misclassification less severely, allowing the model to prioritize getting training examples correct.\\nAminat Abolade',\n",
       "    'section': '4. Evaluation Metrics for Classification',\n",
       "    'question': 'Why do we use cross validation?'},\n",
       "   {'text': \"Model evaluation metrics can be easily computed using off the shelf calculations available in scikit learn library. This saves a lot of time and more precise compared to our own calculations from the scratch using numpy and pandas libraries.\\nfrom sklearn.metrics import (accuracy_score,\\nprecision_score,\\nrecall_score,\\nf1_score,\\nroc_auc_score\\n)\\naccuracy = accuracy_score(y_val, y_pred)\\nprecision = precision_score(y_val, y_pred)\\nrecall = recall_score(y_val, y_pred)\\nf1 = f1_score(y_val, y_pred)\\nroc_auc = roc_auc_score(y_val, y_pred)\\nprint(f'Accuracy: {accuracy}')\\nprint(f'Precision: {precision}')\\nprint(f'Recall: {recall}')\\nprint(f'F1-Score: {f1}')\\nprint(f'ROC AUC: {roc_auc}')\\n(Harish Balasundaram)\",\n",
       "    'section': '4. Evaluation Metrics for Classification',\n",
       "    'question': 'Evaluate the Model using scikit learn metrics'},\n",
       "   {'text': 'Scikit-learn offers another way: precision_recall_fscore_support\\nExample:\\nfrom sklearn.metrics import precision_recall_fscore_support\\nprecision, recall, fscore, support = precision_recall_fscore_support(y_val, y_val_pred, zero_division=0)\\n(Gopakumar Gopinathan)',\n",
       "    'section': '4. Evaluation Metrics for Classification',\n",
       "    'question': 'Are there other ways to compute Precision, Recall and F1 score?'},\n",
       "   {'text': '- ROC curves are appropriate when the observations are balanced between each class, whereas precision-recall curves are appropriate for imbalanced datasets.\\n- The reason for this recommendation is that ROC curves present an optimistic picture of the model on datasets with a class imbalance.\\n-This is because of the use of true negatives in the False Positive Rate in the ROC Curve and the careful avoidance of this rate in the Precision-Recall curve.\\n- If the proportion of positive to negative instances changes in a test set, the ROC curves will not change. Metrics such as accuracy, precision, lift and F scores use values from both columns of the confusion matrix. As a class distribution changes these measures will change as well, even if the fundamental classifier performance does not. ROC graphs are based upon TP rate and FP rate, in which each dimension is a strict columnar ratio, so cannot give an accurate picture of performance when there is class imbalance.\\n(Anudeep Vanjavakam)',\n",
       "    'section': '4. Evaluation Metrics for Classification',\n",
       "    'question': 'When do I use ROC vs Precision-Recall curves?'},\n",
       "   {'text': 'You can use roc_auc_score function from sklearn.metrics module and pass the vector of the target variable (‘above_average’) as the first argument and the vector of feature values as the second one. This function will return AUC score for the feature that was passed as a second argument.\\n(Denys Soloviov)',\n",
       "    'section': '4. Evaluation Metrics for Classification',\n",
       "    'question': 'How to evaluate feature importance for numerical variables with AUC?'},\n",
       "   {'text': 'Precision-recall curve, and thus the score, explicitly depends on the ratio  of positive to negative test cases. This means that comparison of the F-score across different problems with differing class ratios is problematic. One way to address this issue is to use a standard class ratio  when making such comparisons.\\n(George Chizhmak)',\n",
       "    'section': '4. Evaluation Metrics for Classification',\n",
       "    'question': 'Dependence of the F-score on class imbalance'},\n",
       "   {'text': \"We can import precision_recall_curve from scikit-learn and plot the graph as follows:\\nfrom sklearn.metrics import precision_recall_curve\\nprecision, recall, thresholds = precision_recall_curve(y_val, y_predict)\\nplt.plot(thresholds, precision[:-1], label='Precision')\\nplt.plot(thresholds, recall[:-1], label='Recall')\\nplt.legend()\\nHrithik Kumar Advani\",\n",
       "    'section': '4. Evaluation Metrics for Classification',\n",
       "    'question': 'Quick way to plot Precision-Recall Curve'},\n",
       "   {'text': 'For multiclass classification it is important to keep class balance when you split the data set. In this case Stratified k-fold returns folds that contains approximately the sme percentage of samples of each classes.\\nPlease check the realisation in sk-learn library:\\nhttps://scikit-learn.org/stable/modules/cross_validation.html#stratified-k-fold\\nOlga Rudakova',\n",
       "    'section': '5. Deploying Machine Learning Models',\n",
       "    'question': 'What is Stratified k-fold?'},\n",
       "   {'text': 'Week 5 HW: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/05-deployment/homework.md\\nAll HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\\nHW 3 Solution: https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/course-zoomcamp/cohorts/2022/03-classification/homework_3.ipynb\\nEvaluation Matrix: https://docs.google.com/spreadsheets/d/e/2PACX-1vQCwqAtkjl07MTW-SxWUK9GUvMQ3Pv_fF8UadcuIYLgHa0PlNu9BRWtfLgivI8xSCncQs82HDwGXSm3/pubhtml\\nGitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\\nYouTube Link: 5.X --- https://www.youtube.com/watch?v=agIFak9A3m8&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=49\\n~~~ Nukta Bhatia ~~~',\n",
       "    'section': '5. Deploying Machine Learning Models',\n",
       "    'question': 'How do I get started with Week 5?'},\n",
       "   {'text': 'While weeks 1-4 can relatively easily be followed and the associated homework completed with just about any default environment / local setup, week 5 introduces several layers of abstraction and dependencies.\\nIt is advised to prepare your “homework environment” with a cloud provider of your choice. A thorough step-by-step guide for doing so for an AWS EC2 instance is provided in an introductory video taken from the MLOPS course here:\\nhttps://www.youtube.com/watch?v=IXSiYkP23zo\\nNote that (only) small AWS instances can be run for free, and that larger ones will be billed hourly based on usage (but can and should be stopped when not in use).\\nAlternative ways are sketched here:\\nhttps://github.com/alexeygrigorev/mlbookcamp-code/blob/master/course-zoomcamp/01-intro/06-environment.md',\n",
       "    'section': '5. Deploying Machine Learning Models',\n",
       "    'question': 'Errors related to the default environment: WSL, Ubuntu, proper Python version, installing pipenv etc.'},\n",
       "   {'text': \"You’ll need a kaggle account\\nGo to settings, API and click `Create New Token`. This will download a `kaggle.json` file which contains your `username` and `key` information\\nIn the same location as your Jupyter NB, place the `kaggle.json` file\\nRun `!chmod 600 <ENTER YOUR FILEPATH>/kaggle.json`\\nMake sure to import os via `import os` and then run:\\nos.environ['KAGGLE_CONFIG_DIR'] = <STRING OF YOUR FILE PATH>\\nFinally you can run directly in your NB: `!kaggle datasets download -d kapturovalexander/bank-credit-scoring`\\nAnd then you can unzip the file and access the CSV via: `!unzip -o bank-credit-scoring.zip`\\n>>> Michael Fronda <<<\",\n",
       "    'section': '5. Deploying Machine Learning Models',\n",
       "    'question': 'How to download CSV data via Jupyter NB and the Kaggle API, for one seamless experience'},\n",
       "   {'text': 'Cd .. (go back)\\nLs (see current folders)\\nCd ‘path’/ (go to this path)\\nPwd (home)\\nCat “file name’ --edit txt file in ubuntu\\nAileah Gotladera',\n",
       "    'section': '5. Deploying Machine Learning Models',\n",
       "    'question': 'Basic Ubuntu Commands:'},\n",
       "   {'text': 'Open terminal and type the code below to check the version on your laptop\\npython3 --version\\nFor windows,\\nVisit the official python website at  https://www.python.org/downloads/ to download the python version you need for installation\\nRun the installer and  ensure to check the box that says “Add Python to PATH” during installation and complete the installation by following the prompts\\nOr\\nFor Python 3,\\nOpen your command prompt or terminal and run the following command:\\npip install --upgrade python\\nAminat Abolade',\n",
       "    'section': '5. Deploying Machine Learning Models',\n",
       "    'question': 'Installing and updating to the python version 3.10 and higher'},\n",
       "   {'text': 'It is quite simple, and you can follow these instructions here:\\nhttps://www.youtube.com/watch?v=qYlgUDKKK5A&ab_channel=NeuralNine\\nMake sure that you have “Virtual Machine Platform” feature activated in your Windows “Features”. To do that, search “features” in the research bar and see if the checkbox is selected. You also need to make sure that your system (in the bios) is able to virtualize. This is usually the case.\\nIn the Microsoft Store: look for ‘Ubuntu’ or ‘Debian’ (or any linux distribution you want) and install it\\nOnce it is downloaded, open the app and choose a username and a password (secured one). When you type your password, nothing will show in the window, which is normal: the writing is invisible.\\nYou are now inside of your linux system. You can test some commands such as “pwd”. You are not in your Windows system.\\nTo go to your windows system: you need to go back two times with cd ../.. And then go to the “mnt” directory with cd mnt. If you list here your files, you will see your disks. You can move to the desired folder, for example here I moved to the ML_Zoomcamp folder:\\nPython should be already installed but you can check it by running sudo apt install python3 command.\\nYou can make your actual folder your default folder when you open your Ubuntu terminal with this command : echo \"cd ../../mnt/your/folder/path\" >> ~/.bashrc\\nYou can disable bell sounds (when you type something that does not exist for example) by modifying the inputrc file with this command: sudo vim /etc/inputrc\\nYou have to uncomment the set bell-style none line -> to do that, press the “i” keyboard letter (for insert) and go with your keyboard to this line. Delete the # and then press the Escape keyboard touch and finally press “:wq” to write (it saves your modifications) then quit.\\nYou can check that your modifications are taken into account by opening a new terminal (you can pin it to your task bar so you do not have to go to the Microsoft app each time).\\nYou will need to install pip by running this command sudo apt install python3-pip\\nNB: I had this error message when trying to install pipenv (https://github.com/microsoft/WSL/issues/5663):\\n/sbin/ldconfig.real: Can\\'t link /usr/lib/wsl/lib/libnvoptix_loader.so.1 to libnvoptix.so.1\\n/sbin/ldconfig.real: /usr/lib/wsl/lib/libcuda.so.1 is not a symbolic link\\nSo I had to create the following symbolic link:\\nsudo ln -s /usr/lib/wsl/lib/libcuda.so.1 /usr/lib64/libcuda.so\\n(Mélanie Fouesnard)',\n",
       "    'section': '5. Deploying Machine Learning Models',\n",
       "    'question': 'How to install WSL on Windows 10 and 11 ?'},\n",
       "   {'text': \"Do you get errors building the Docker image on the Mac M1 chipset?\\nThe error I was getting was:\\nCould not open '/lib64/ld-linux-x86-64.so.2': No such file or directory\\nThe fix (from here): vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv\\nOpen mlbookcamp-code/course-zoomcamp/01-intro/environment/Dockerfile\\nReplace line 1 with\\nFROM --platform=linux/amd64 ubuntu:latest\\nNow build the image as specified. In the end it took over 2 hours to build the image but it did complete in the end.\\nDavid Colton\",\n",
       "    'section': '5. Deploying Machine Learning Models',\n",
       "    'question': 'Error building Docker images on Mac with M1 silicon'},\n",
       "   {'text': 'Import waitress\\nprint(waitress.__version__)\\nKrishna Anand',\n",
       "    'section': '5. Deploying Machine Learning Models',\n",
       "    'question': 'Method to find the version of any install python libraries in jupyter notebook'},\n",
       "   {'text': 'Working on getting Docker installed - when I try running hello-world I am getting the error.\\nDocker: Cannot connect to the docker daemon at unix:///var/run/docker.sock. Is the Docker daemon running ?\\nSolution description\\nIf you’re getting this error on WSL, re-install your docker: remove the docker installation from WSL and install Docker Desktop on your host machine (Windows).\\nOn Linux, start the docker daemon with either of these commands:\\nsudo dockerd\\nsudo service docker start\\nAdded by Ugochukwu Onyebuchi',\n",
       "    'section': '5. Deploying Machine Learning Models',\n",
       "    'question': 'Cannot connect to the docker daemon. Is the Docker daemon running?'},\n",
       "   {'text': 'After using the command “docker build -t churn-prediction .” to build the Docker image, the above error is raised and the image is not created.\\nIn your Dockerfile, change the Python version in the first line the Python version installed in your system:\\nFROM python:3.7.5-slim\\nTo find your python version, use the command python --version. For example:\\npython --version\\n>> Python 3.9.7\\nThen, change it on your Dockerfile:\\nFROM python:3.9.7-slim\\nAdded by Filipe Melo',\n",
       "    'section': '5. Deploying Machine Learning Models',\n",
       "    'question': \"The command '/bin/sh -c pipenv install --deploy --system &&  rm -rf /root/.cache' returned a non-zero code: 1\"},\n",
       "   {'text': 'When the facilitator was adding sklearn to the virtual environment in the lectures, he used sklearn==0.24.1 and it ran smoothly. But while doing the homework and you are asked to use the 1.0.2 version of sklearn, it gives errors.\\nThe solution is to use the full name of sklearn. That is, run it as “pipenv install scikit-learn==1.0.2” and the error will go away, allowing you to install sklearn for the version in your virtual environment.\\nOdimegwu David\\nHomework asks you to install 1.3.1\\nPipenv install scikit-learn==1.3.1\\nUse Pipenv to install Scikit-Learn version 1.3.1\\nGopakumar Gopinathan',\n",
       "    'section': '5. Deploying Machine Learning Models',\n",
       "    'question': 'Running “pipenv install sklearn==1.0.2” gives errors. What should I do?'},\n",
       "   {'text': 'What is the reason we don’t want to keep the docker image in our system and why do we need to run docker containers with `--rm` flag?\\nFor best practice, you don’t want to have a lot of abandoned docker images in your system. You just update it in your folder and trigger the build one more time.\\nThey consume extra space on your disk. Unless you don’t want to re-run the previously existing containers, it is better to use the `--rm` option.\\nThe right way to say: “Why do we remove the docker container in our system?”. Well the docker image is still kept; it is the container that is not kept. Upon execution, images are not modified; only containers are.\\nThe option `--rm` is for removing containers. The images remain until you remove them manually. If you don’t specify a version when building an image, it will always rebuild and replace the latest tag. `docker images` shows you all the image you have pulled or build so far.\\nDuring development and testing you usually specify `--rm` to get the containers auto removed upon exit. Otherwise they get accumulated in a stopped state, taking up space. `docker ps -a` shows you all the containers you have in your host. Each time you change Pipfile (or any file you baked into the container), you rebuild the image under the same tag or a new tag. It’s important to understand the difference between the term “docker image” and “docker container”. Image is what we build with all the resources baked in. You can move it around, maintain it in a repository, share it. Then we use the image to spin up instances of it and they are called containers.\\nAdded by Muhammad Awon',\n",
       "    'section': '5. Deploying Machine Learning Models',\n",
       "    'question': 'Why do we need the --rm flag'},\n",
       "   {'text': 'When you create the dockerfile the name should be dockerfile and needs to be without extension. One of the problems we can get at this point is to create the dockerfile as a dockerfile extension Dockerfile.dockerfile which creates an error when we build the docker image. Instead we just need to create the file without extension: Dockerfile and will run perfectly.\\nAdded by Pastor Soto',\n",
       "    'section': '5. Deploying Machine Learning Models',\n",
       "    'question': 'Failed to read Dockerfile'},\n",
       "   {'text': 'Refer to the page https://docs.docker.com/desktop/install/mac-install/ remember to check if you have apple chip or intel chip.',\n",
       "    'section': '5. Deploying Machine Learning Models',\n",
       "    'question': 'Install docker on MacOS'},\n",
       "   {'text': 'Problem: When I am trying to pull the image with the docker pull svizor/zoomcamp-model command I am getting an error:\\nUsing default tag: latest\\nError response from daemon: manifest for svizor/zoomcamp-model:latest not found: manifest unknown: manifest unknown\\nSolution: The docker by default uses the latest tag to avoid this use the correct tag from image description. In our case use command:\\ndocker pull svizor/zoomcamp-model:3.10.12-slim\\nAdded by Vladimir Yesipov',\n",
       "    'section': '5. Deploying Machine Learning Models',\n",
       "    'question': 'I cannot pull the image with docker pull command'},\n",
       "   {'text': 'Using the command docker images or docker image ls will dump all information for all local Docker images. It is possible to dump the information only for a specified image by using:\\ndocker image ls <image name>\\nOr alternatively:\\ndocker images <image name>\\nIn action to that it is possible to only dump specific information provided using the option --format which will dump only the size for the specified image name when using the command below:\\ndocker image ls --format \"{{.Size}}\" <image name>\\nOr alternatively:\\ndocker images --format \"{{.Size}}\" <image name>\\nSylvia Schmitt',\n",
       "    'section': '5. Deploying Machine Learning Models',\n",
       "    'question': 'Dumping/Retrieving only the size of for a specific Docker image'},\n",
       "   {'text': \"It creates them in\\nOSX/Linux: ~/.local/share/virtualenvs/folder-name_cyrptic-hash\\nWindows: C:\\\\Users\\\\<USERNAME>\\\\.virtualenvs\\\\folder-name_cyrptic-hash\\nEg: C:\\\\Users\\\\Ella\\\\.virtualenvs\\\\code-qsdUdabf (for module-05 lesson)\\nThe environment name is the name of the last folder in the folder directory where we used the pipenv install command (or any other pipenv command). E.g. If you run any pipenv command in folder path ~/home/user/Churn-Flask-app, it will create an environment named Churn-Flask-app-some_random_characters, and it's path will be like this: /home/user/.local/share/virtualenvs/churn-flask-app-i_mzGMjX.\\nAll libraries of this environment will be installed inside this folder. To activate this environment, I will need to cd into the project folder again, and type pipenv shell. In short, the location of the project folder acts as an identifier for an environment, in place of any name.\\n(Memoona Tahira)\",\n",
       "    'section': '5. Deploying Machine Learning Models',\n",
       "    'question': 'Where does pipenv create environments and how does it name them?'},\n",
       "   {'text': 'Launch the container image in interactive mode and overriding the entrypoint, so that it starts a bash command.\\ndocker run -it --entrypoint bash <image>\\nIf the container is already running, execute a command in the specific container:\\ndocker ps (find the container-id)\\ndocker exec -it <container-id> bash\\n(Marcos MJD)',\n",
       "    'section': '5. Deploying Machine Learning Models',\n",
       "    'question': 'How do I debug a docker container?'},\n",
       "   {'text': \"$ docker exec -it 1e5a1b663052 bash\\nthe input device is not a TTY.  If you are using mintty, try prefixing the command with 'winpty'\\nFix:\\nwinpty docker exec -it 1e5a1b663052 bash\\nA TTY is a terminal interface that supports escape sequences, moving the cursor around, etc.\\nWinpty is a Windows software package providing an interface similar to a Unix pty-master for communicating with Windows console programs.\\nMore info on terminal, shell, console applications hi and so on:\\nhttps://conemu.github.io/en/TerminalVsShell.html\\n(Marcos MJD)\",\n",
       "    'section': '5. Deploying Machine Learning Models',\n",
       "    'question': 'The input device is not a TTY when running docker in interactive mode (Running Docker on Windows in GitBash)'},\n",
       "   {'text': 'Initially, I did not assume there was a model2. I copied the original model1.bin and dv.bin. Then when I tried to load using\\nCOPY [\"model2.bin\", \"dv.bin\", \"./\"]\\nthen I got the error above in MINGW64 (git bash) on Windows.\\nThe temporary solution I found was to use\\nCOPY [\"*\", \"./\"]\\nwhich I assume combines all the files from the original docker image and the files in your working directory.\\nAdded by Muhammed Tan',\n",
       "    'section': '5. Deploying Machine Learning Models',\n",
       "    'question': 'Error: failed to compute cache key: \"/model2.bin\" not found: not found'},\n",
       "   {'text': 'Create a virtual environment using the Cmd command (command) and use pip freeze command to write the requirements in the text file\\nKrishna Anand',\n",
       "    'section': '5. Deploying Machine Learning Models',\n",
       "    'question': 'Failed to write the dependencies to pipfile and piplock file'},\n",
       "   {'text': 'f-String not properly keyed in: does anyone knows why i am getting error after import pickle?\\nThe first error showed up because your f-string is using () instead of {} around C. So, should be: f’model_C={C}.bin’\\nThe second error as noticed by Sriniketh, your are missing one parenthesis it should be pickle.dump((dv, model), f_out)\\n(Humberto R.)',\n",
       "    'section': '5. Deploying Machine Learning Models',\n",
       "    'question': 'f-strings'},\n",
       "   {'text': \"This error happens because pipenv is already installed but you can't access it from the path.\\nThis error comes out if you run.\\npipenv  --version\\npipenv shell\\nSolution for Windows\\nOpen this option\\nClick here\\nClick in Edit Button\\nMake sure the next two locations are on the PATH, otherwise, add it.\\nC:\\\\Users\\\\AppData\\\\....\\\\Python\\\\PythonXX\\\\\\nC:\\\\Users\\\\AppData\\\\....\\\\Python\\\\PythonXX\\\\Scripts\\\\\\nAdded by Alejandro Aponte\\nNote: this answer assumes you don’t use Anaconda. For Windows, using Anaconda would be a better choice and less prone to errors.\",\n",
       "    'section': '5. Deploying Machine Learning Models',\n",
       "    'question': \"'pipenv' is not recognized as an internal or external command, operable program or batch file.\"},\n",
       "   {'text': 'Following the instruction from video week-5.6, using pipenv to install python libraries throws below error\\nSolution to this error is to make sure that you are working with python==3.9 (as informed in the very first lesson of the zoomcamp) and not python==3.10.\\nAdded by Hareesh Tummala',\n",
       "    'section': '5. Deploying Machine Learning Models',\n",
       "    'question': 'AttributeError: module ‘collections’ has no attribute ‘MutableMapping’'},\n",
       "   {'text': 'After entering `pipenv shell` don’t forget to use `exit` before `pipenv --rm`, as it may cause errors when trying to install packages, it is unclear whether you are “in the shell”(using Windows) at the moment as there are no clear markers for it.\\nIt can also mess up PATH, if that’s the case, here’s terminal commands for fixing that:\\n# for Windows\\nset VIRTUAL_ENV \"\"\\n# for Unix\\nexport VIRTUAL_ENV=\"\"\\nAlso manually re-creating removed folder at `C:\\\\Users\\\\username\\\\.virtualenvs\\\\removed-envname` can help, removed-envname can be seen at the error message.\\nAdded by Andrii Larkin',\n",
       "    'section': '5. Deploying Machine Learning Models',\n",
       "    'question': \"Q: ValueError: Path not found or generated: WindowsPath('C:/Users/username/.virtualenvs/envname/Scripts')\"},\n",
       "   {'text': 'Set the host to ‘0.0.0.0’ on the flask app and dockerfile then RUN the url using localhost.\\n(Theresa S.)',\n",
       "    'section': '5. Deploying Machine Learning Models',\n",
       "    'question': \"ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\"},\n",
       "   {'text': 'Solution:\\nThis error occurred because I used single quotes around the filenames. Stick to double quotes',\n",
       "    'section': '5. Deploying Machine Learning Models',\n",
       "    'question': 'docker  build ERROR [x/y] COPY …'},\n",
       "   {'text': 'I tried the first solution on Stackoverflow which recommended running `pipenv lock` to update the Pipfile.lock. However, this didn’t resolve it. But the following switch to the pipenv installation worked\\nRUN pipenv install --system --deploy --ignore-pipfile',\n",
       "    'section': '5. Deploying Machine Learning Models',\n",
       "    'question': 'Fix error during installation of Pipfile inside Docker container'},\n",
       "   {'text': 'Solution\\nThis error was because there was another instance of gunicorn running. So I thought of removing this along with the zoomcamp_test image. However, it didn’t let me remove the orphan container. So I did the following\\nRunning the following commands\\ndocker ps -a <to list all docker containers>\\ndocker images <to list images>\\ndocker stop <container ID>\\ndocker rm <container ID>\\ndocker rmi image\\nI rebuilt the Docker image, and ran it once again; this time it worked correctly and I was able to serve the test script to the endpoint.',\n",
       "    'section': '5. Deploying Machine Learning Models',\n",
       "    'question': 'How to fix error after running the Docker run command'},\n",
       "   {'text': 'I was getting the below error when I rebuilt the docker image although the port was not allocated, and it was working fine.\\nError message:\\nError response from daemon: driver failed programming external connectivity on endpoint beautiful_tharp (875be95c7027cebb853a62fc4463d46e23df99e0175be73641269c3d180f7796): Bind for 0.0.0.0:9696 failed: port is already allocated.\\nSolution description\\nIssue has been resolved running the following command:\\ndocker kill $(docker ps -q)\\nhttps://github.com/docker/for-win/issues/2722\\nAsia Saeed',\n",
       "    'section': '5. Deploying Machine Learning Models',\n",
       "    'question': 'Bind for 0.0.0.0:9696 failed: port is already allocated'},\n",
       "   {'text': 'I was getting the error on client side with this\\nClient Side:\\nFile \"C:\\\\python\\\\lib\\\\site-packages\\\\urllib3\\\\connectionpool.py\", line 703, in urlopen …………………..\\nraise ConnectionError(err, request=request)\\nrequests.exceptions.ConnectionError: (\\'Connection aborted.\\', RemoteDisconnected(\\'Remote end closed connection without response\\'))\\nSevrer Side:\\nIt showed error for gunicorn\\nThe waitress  cmd was running smoothly from server side\\nSolution:\\nUse the ip-address as 0.0.0.0:8000 or 0.0.0.0:9696.They are the ones which do work max times\\nAamir Wani',\n",
       "    'section': '5. Deploying Machine Learning Models',\n",
       "    'question': 'Bind for 127.0.0.1:5000 showing error'},\n",
       "   {'text': 'Install it by using command\\n% brew install md5sha1sum\\nThen run command to check hash for file to check if they the same with the provided\\n% md5sum model1.bin dv.bin\\nOlga Rudakova',\n",
       "    'section': '5. Deploying Machine Learning Models',\n",
       "    'question': 'Installing md5sum on Macos'},\n",
       "   {'text': 'Problem description:\\nI started a web-server in terminal (command window, powershell, etc.). How can I run another python script, which makes a request to this server?\\nSolution description:\\nJust open another terminal (command window, powershell, etc.) and run a python script.\\nAlena Kniazeva',\n",
       "    'section': '5. Deploying Machine Learning Models',\n",
       "    'question': 'How to run a script while a web-server is working?'},\n",
       "   {'text': \"Problem description:\\nIn video 5.5 when I do pipenv shell and then pipenv run gunicorn --bind 0.0.0.0:9696 predict:app, I get the following warning:\\nUserWarning: Trying to unpickle estimator DictVectorizer from version 1.1.1 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\\nSolution description:\\nWhen you create a virtual env, you should use the same version of Scikit-Learn that you used for training the model on this case it's 1.1.1. There is version conflicts so we need to make sure our model and dv files are created from the version we are using for the project.\\nBhaskar Sarma\",\n",
       "    'section': '5. Deploying Machine Learning Models',\n",
       "    'question': 'Version-conflict in pipenv'},\n",
       "   {'text': \"If you install packages via pipenv install, and get an error that ends like this:\\npipenv.vendor.plette.models.base.ValidationError: {'python_version': '3.9', 'python_full_version': '3.9.13'}\\npython_full_version: 'python_version' must not be present with 'python_full_version'\\npython_version: 'python_full_version' must not be present with 'python_version'\\nDo this:\\nopen Pipfile in nano editor, and remove either the python_version or python_full_version line, press CTRL+X, type Y and click Enter to save changed\\nType pipenv lock to create the Pipfile.lock.\\nDone. Continue what you were doing\",\n",
       "    'section': '5. Deploying Machine Learning Models',\n",
       "    'question': 'Python_version and Python_full_version error after running pipenv install:'},\n",
       "   {'text': 'If during running the  docker build command, you get an error like this:\\nYour Pipfile.lock (221d14) is out of date. Expected: (939fe0).\\nUsage: pipenv install [OPTIONS] [PACKAGES]...\\nERROR:: Aborting deploy\\nOption 1: Delete the pipfile.lock via rm Pipfile, and then rebuild the lock via  pipenv lock from the terminal before retrying the docker build command.\\nOption 2:  If it still doesn’t work, remove the pipenv environment, Pipfile and Pipfile.lock, and create a new one before building docker again. Commands to remove pipenv environment and removing pipfiles:\\npipenv  --rm\\nrm Pipfile*',\n",
       "    'section': '5. Deploying Machine Learning Models',\n",
       "    'question': 'Your Pipfile.lock (221d14) is out of date (during Docker build)'},\n",
       "   {'text': 'Ans: Pip uninstall waitress mflow. Then reinstall just mlflow. By this time you should have successfully built your docker image so you dont need to reinstall waitress. All good. Happy learning.\\nAdded by 🅱🅻🅰🆀',\n",
       "    'section': '5. Deploying Machine Learning Models',\n",
       "    'question': 'You are using windows. Conda environment. You then use waitress instead of gunicorn. After a few runs, suddenly mlflow server fails to run.'},\n",
       "   {'text': \"Ans: so you have created the env. You need to make sure you're in eu-west-1 (ireland) when you check the EB environments. Maybe you're in a different region in your console.\\nAdded by Edidiong Esu\",\n",
       "    'section': '5. Deploying Machine Learning Models',\n",
       "    'question': 'Completed creating the environment locally but could not find the environment on AWS.'},\n",
       "   {'text': 'Running \\'pip install waitress\\' as a command on GitBash was not downloading the executable file \\'waitress-serve.exe\\'. You need this file to be able to run commands with waitress in Git Bash. To solve this:\\nopen a Jupyter notebook and run the same command \\' pip install waitress\\'. This way the executable file will be downloaded. The notebook may give you this warning : \\'WARNING: The script waitress-serve.exe is installed in \\'c:\\\\Users\\\\....\\\\anaconda3\\\\Scripts\\' which is not on PATH. Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\\'\\nAdd the path where \\'waitress-serve.exe\\' is installed into gitbash\\'s PATH as such:\\nenter the following command in gitbash: nano ~/.bashrc\\nadd the path to \\'waitress-serve.exe\\' to PATH using this command: export PATH=\"/path/to/waitress:$PATH\"\\nclose gitbash and open it again and you should be good to go\\nAdded by Bachar Kabalan',\n",
       "    'section': '5. Deploying Machine Learning Models',\n",
       "    'question': 'Installing waitress on Windows via GitBash: “waitress-serve” command not found'},\n",
       "   {'text': 'Q2.1: Use Pipenv to install Scikit-Learn version 1.3.1\\nThis is an error I got while executing the above step in the ml-zoomcamp conda environment. The error is not fatal and just warns you that explicit language specifications are not set out in our bash profile. A quick-fix is here:\\nhttps://stackoverflow.com/questions/49436922/getting-error-while-trying-to-run-this-command-pipenv-install-requests-in-ma\\nBut one can proceed without addressing it.\\nAdded by Abhirup Ghosh',\n",
       "    'section': '5. Deploying Machine Learning Models',\n",
       "    'question': 'Warning: the environment variable LANG is not set!'},\n",
       "   {'text': 'The provided image FROM svizor/zoomcamp-model:3.10.12-slim has a model and dictvectorizer that should be used for question 6. \"model2.bin\", \"dv.bin\"\\nAdded by Quinn Avila',\n",
       "    'section': '5. Deploying Machine Learning Models',\n",
       "    'question': 'Module5 HW Question 6'},\n",
       "   {'text': 'https://apps.microsoft.com/detail/windows-terminal/9N0DX20HK701?hl=es-419&gl=CO\\nAdded by Dawuta Smit',\n",
       "    'section': '5. Deploying Machine Learning Models',\n",
       "    'question': 'Terminal Used in Week 5 videos:'},\n",
       "   {'text': \"Question:\\nWhen running\\npipenv run waitress-serve --listen=localhost:9696 q4-predict:app\\nI get the following:\\nThere was an exception (ValueError) importing your module.\\nIt had these arguments:\\n1. Malformed application 'q4-predict:app'\\nAnswer:\\nWaitress doesn’t accept a dash in the python file name.\\nThe solution is to rename the file replacing a dash with something else for instance with an underscore eg q4_predict.py\\nAdded by Alex Litvinov\",\n",
       "    'section': '5. Deploying Machine Learning Models',\n",
       "    'question': 'waitress-serve shows Malformed application'},\n",
       "   {'text': 'I wanted to have a fast and simple way to check if the HTTP POST requests are working just running a request from command line. This can be done running ‘curl’. \\n(Used with WSL2 on Windows, should also work on Linux and MacOS)\\ncurl --json \\'<json data>\\' <url>\\n# piping the structure to the command\\ncat <json file path> | curl --json @- <url>\\necho \\'<json data>\\' | curl --json @- <url>\\n# example using piping\\necho \\'{\"job\": \"retired\", \"duration\": 445, \"poutcome\": \"success\"}\\'\\\\\\n| curl --json @- http://localhost:9696/predict\\nAdded by Sylvia Schmitt',\n",
       "    'section': '5. Deploying Machine Learning Models',\n",
       "    'question': 'Testing HTTP POST requests from command line using curl'},\n",
       "   {'text': 'Question:\\nWhen executing\\neb local run  --port 9696\\nI get the following error:\\nERROR: NotSupportedError - You can use \"eb local\" only with preconfigured, generic and multicontainer Docker platforms.\\nAnswer:\\nThere are two options to fix this:\\nRe-initialize by running eb init -i and choosing the options from a list (the first default option for docker platform should be fine).\\nEdit the ‘.elasticbeanstalk/config.yml’ directly changing the default_platform from Docker to default_platform: Docker running on 64bit Amazon Linux 2023\\nThe disadvantage of the second approach is that the option might not be available the following years\\nAdded by Alex Litvinov',\n",
       "    'section': '5. Deploying Machine Learning Models',\n",
       "    'question': 'NotSupportedError - You can use \"eb local\" only with preconfigured, generic and multicontainer Docker platforms.'},\n",
       "   {'text': \"You need to include the protocol scheme: 'http://localhost:9696/predict'.\\nWithout the http:// part, requests has no idea how to connect to the remote server.\\nNote that the protocol scheme must be all lowercase; if your URL starts with HTTP:// for example, it won’t find the http:// connection adapter either.\\nAdded by George Chizhmak\",\n",
       "    'section': '5. Deploying Machine Learning Models',\n",
       "    'question': \"Requests Error: No connection adapters were found for 'localhost:9696/predict'.\"},\n",
       "   {'text': 'While running the docker image if you get the same result check which model you are using.\\nRemember you are using a model downloading model + python version so remember to change the model in your file when running your prediction test.\\nAdded by Ahmed Okka',\n",
       "    'section': '5. Deploying Machine Learning Models',\n",
       "    'question': 'Getting the same result'},\n",
       "   {'text': 'Ensure that you used pipenv to install the necessary modules including gunicorn. As pipfiles for virtual environments, you can use pipenv shell and then build+run your docker image. - Akshar Goyal',\n",
       "    'section': '5. Deploying Machine Learning Models',\n",
       "    'question': 'Trying to run a docker image I built but it says it’s unable to start the container process'},\n",
       "   {'text': \"You can copy files from your local machine into a Docker container using the docker cp command. Here's how to do it:\\nTo copy a file or directory from your local machine into a running Docker container, you can use the `docker cp command`. The basic syntax is as follows:\\ndocker cp /path/to/local/file_or_directory container_id:/path/in/container\\nHrithik Kumar Advani\",\n",
       "    'section': '5. Deploying Machine Learning Models',\n",
       "    'question': 'How do I copy files from my local machine to docker container?'},\n",
       "   {'text': 'You can copy files from your local machine into a Docker container using the docker cp command. Here\\'s how to do it:\\nIn the Dockerfile, you can provide the folder containing the files that you want to copy over. The basic syntax is as follows:\\nCOPY [\"src/predict.py\", \"models/xgb_model.bin\", \"./\"]\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tGopakumar Gopinathan',\n",
       "    'section': '5. Deploying Machine Learning Models',\n",
       "    'question': 'How do I copy files from a different folder into docker container’s working directory?'},\n",
       "   {'text': 'I struggled with the command :\\neb init -p docker tumor-diagnosis-serving -r eu-west-1\\nWhich resulted in an error when running : eb local run --port 9696\\nERROR: NotSupportedError - You can use \"eb local\" only with preconfigured, generic and multicontainer Docker platforms.\\nI replaced it with :\\neb init -p \"Docker running on 64bit Amazon Linux 2\" tumor-diagnosis-serving -r eu-west-1\\nThis allowed the recognition of the Dockerfile and the build/run of the docker container.\\nAdded by Mélanie Fouesnard',\n",
       "    'section': '5. Deploying Machine Learning Models',\n",
       "    'question': 'I can’t create the environment on AWS Elastic Beanstalk with the command proposed during the video'},\n",
       "   {'text': \"I had this error when creating a AWS ElasticBean environment: eb create tumor-diagnosis-env\\nERROR   Instance deployment: Both 'Dockerfile' and 'Dockerrun.aws.json' are missing in your source bundle. Include at least one of them. The deployment failed.\\nI did not committed the files used to build the container, particularly the Dockerfile. After a git add and git commit of the modified files, the command works.\\nAdded by Mélanie Fouesnard\",\n",
       "    'section': '6. Decision Trees and Ensemble Learning',\n",
       "    'question': 'Dockerfile missing when creating the AWS ElasticBean environment'},\n",
       "   {'text': 'Week 6 HW: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/06-trees/homework.md\\nAll HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\\nHW 4 Solution: https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/course-zoomcamp/cohorts/2022/04-evaluation/homework_4.ipynb\\nEvaluation Matrix: https://docs.google.com/spreadsheets/d/e/2PACX-1vQCwqAtkjl07MTW-SxWUK9GUvMQ3Pv_fF8UadcuIYLgHa0PlNu9BRWtfLgivI8xSCncQs82HDwGXSm3/pubhtml\\nGitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\\nYouTube Link: 6.X --- https://www.youtube.com/watch?v=GJGmlfZoCoU&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=57\\nFAQs: https://docs.google.com/document/d/1LpPanc33QJJ6BSsyxVg-pWNMplal84TdZtq10naIhD8/edit#heading=h.lpz96zg7l47j\\n~~~Nukta Bhatia~~~',\n",
       "    'section': '6. Decision Trees and Ensemble Learning',\n",
       "    'question': 'How to get started with Week 6?'},\n",
       "   {'text': 'During the XGBoost lesson, we created a parser to extract the training and validation auc from the standard output. However, we can accomplish that in a more straightforward way.\\nWe can use the evals_result  parameters, which takes an empty dictionary and updates it for each tree. Additionally, you can store the data in a dataframe and plot it in an easier manner.\\nAdded by Daniel Coronel',\n",
       "    'section': '6. Decision Trees and Ensemble Learning',\n",
       "    'question': 'How to get the training and validation metrics from XGBoost?'},\n",
       "   {'text': 'You should create sklearn.ensemble.RandomForestRegressor object. It’s rather similar to sklearn.ensemble.RandomForestClassificator for classification problems. Check https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html for more information.\\nAlena Kniazeva',\n",
       "    'section': '6. Decision Trees and Ensemble Learning',\n",
       "    'question': 'How to solve regression problems with random forest in scikit-learn?'},\n",
       "   {'text': 'In question 6, I was getting ValueError: feature_names must be string, and may not contain [, ] or < when I was creating DMatrix for train and validation\\nSolution description\\nThe cause of this error is that some of the features names contain special characters like = and <, and I fixed the error by removing them as  follows:\\nfeatures= [i.replace(\"=<\", \"_\").replace(\"=\",\"_\") for i in features]\\nAsia Saeed\\nAlternative Solution:\\nIn my case the equal sign “=” was not a problem, so in my opinion the first part of Asias solution features= [i.replace(\"=<\", \"_\") should work as well.\\nFor me this works:\\nfeatures = []\\nfor f in dv.feature_names_:\\nstring = f.replace(“=<”, “-le”)\\nfeatures.append(string)\\nPeter Ernicke',\n",
       "    'section': '6. Decision Trees and Ensemble Learning',\n",
       "    'question': 'ValueError: feature_names must be string, and may not contain [, ] or <'},\n",
       "   {'text': 'If you’re getting this error, It is likely because the feature names in dv.get_feature_names_out() are a np.ndarray instead of a list so you have to convert them into a list by using the to_list() method.\\nAli Osman',\n",
       "    'section': '6. Decision Trees and Ensemble Learning',\n",
       "    'question': \"`TypeError: Expecting a sequence of strings for feature names, got: <class 'numpy.ndarray'> ` when training xgboost model.\"},\n",
       "   {'text': \"If you’re getting TypeError:\\n“TypeError: Expecting a sequence of strings for feature names, got: <class 'numpy.ndarray'>”,\\nprobably you’ve done this:\\nfeatures = dv.get_feature_names_out()\\nIt gets you np.ndarray instead of list. Converting to list list(features) will not fix this, read below.\\nIf you’re getting ValueError:\\n“ValueError: feature_names must be string, and may not contain [, ] or <”,\\nprobably you’ve either done:\\nfeatures = list(dv.get_feature_names_out())\\nor:\\nfeatures = dv.feature_names_\\nreason is what you get from DictVectorizer here looks like this:\\n['households',\\n'housing_median_age',\\n'latitude',\\n'longitude',\\n'median_income',\\n'ocean_proximity=<1H OCEAN',\\n'ocean_proximity=INLAND',\\n'population',\\n'total_bedrooms',\\n'total_rooms']\\nit has symbols XGBoost doesn’t like ([, ] or <).\\nWhat you can do, is either do not specify “feature_names=” while creating xgb.DMatrix or:\\nimport re\\nfeatures = dv.feature_names_\\npattern = r'[\\\\[\\\\]<>]'\\nfeatures = [re.sub(pattern, '  ', f) for f in features]\\nAdded by Andrii Larkin\",\n",
       "    'section': '6. Decision Trees and Ensemble Learning',\n",
       "    'question': 'Q6: ValueError or TypeError while setting xgb.DMatrix(feature_names=)'},\n",
       "   {'text': 'To install Xgboost, use the code below directly in your jupyter notebook:\\n(Pip 21.3+ is required)\\npip install xgboost\\nYou can update your pip by using the code below:\\npip install --upgrade pip\\nFor more about xgbboost and installation, check here:\\nhttps://xgboost.readthedocs.io/en/stable/install.html\\nAminat Abolade',\n",
       "    'section': '6. Decision Trees and Ensemble Learning',\n",
       "    'question': 'How to Install Xgboost'},\n",
       "   {'text': 'Sometimes someone might wonder what eta means in the tunable hyperparameters of XGBoost and how it helps the model.\\nETA is the learning rate of the model. XGBoost uses gradient descent to calculate and update the model. In gradient descent, we are looking for the minimum weights that help the model to learn the data very well. This minimum weights for the features is updated each time the model passes through the features and learns the features during training. Tuning the learning rate helps you tell the model what speed it would use in deriving the minimum for the weights.',\n",
       "    'section': '6. Decision Trees and Ensemble Learning',\n",
       "    'question': 'What is eta in XGBoost'},\n",
       "   {'text': 'For ensemble algorithms, during the week 6, one bagging algorithm and one boosting algorithm were presented: Random Forest and XGBoost, respectively.\\nRandom Forest trains several models in parallel. The output can be, for example, the average value of all the outputs of each model. This is called bagging.\\nXGBoost trains several models sequentially: the previous model error is used to train the following model. Weights are used to ponderate the models such as the best models have higher weights and are therefore favored for the final output. This method is called boosting.\\nNote that boosting is not necessarily better than bagging.\\nMélanie Fouesnard\\nBagging stands for “Bootstrap Aggregation” - it involves taking multiple samples with replacement to derive multiple training datasets from the original training dataset (bootstrapping), training a classifier (e.g. decision trees or stumps for Random Forests) on each such training dataset, and then combining the the predictions (aggregation) to obtain the final prediction. For classification, predictions are combined via voting; for regression, via averaging. Bagging can be done in parallel, since the various classifiers are independent. Bagging decreases variance (but not bias) and is robust against overfitting.\\nBoosting, on the other hand, is sequential - each model learns from the mistakes of its predecessor. Observations are given different weights - observations/samples misclassified by the previous classifier are given a higher weight, and this process is continued until a stopping condition is reached (e.g. max. No. of models is reached, or error is acceptably small, etc.). Boosting reduces bias & is generally more accurate than bagging, but can be prone to overfitting.\\nRileen',\n",
       "    'section': '6. Decision Trees and Ensemble Learning',\n",
       "    'question': 'What is the difference between bagging and boosting?'},\n",
       "   {'text': 'I wanted to directly capture the output from the xgboost training for multiple eta values to a dictionary without the need to run the same cell multiple times and manually editing the eta value in between or copy the code for a second eta value.\\nUsing the magic cell command “%%capture output” I was only able to capture the complete output for all iterations for the loop, but. I was able to solve this using the following approach. This is just a code sample to grasp the idea.\\n# This would be the content of the Jupyter Notebook cell\\nfrom IPython.utils.capture import capture_output\\nimport sys\\ndifferent_outputs = {}\\nfor i in range(3):\\nwith capture_output(sys.stdout) as output:\\nprint(i)\\nprint(\"testing capture\")\\ndifferent_outputs[i] = output.stdout\\n# different_outputs\\n# {0: \\'0\\\\ntesting capture\\\\n\\',\\n#  1: \\'1\\\\ntesting capture\\\\n\\',\\n#  2: \\'2\\\\ntesting capture\\\\n\\'}\\nAdded by Sylvia Schmitt',\n",
       "    'section': '6. Decision Trees and Ensemble Learning',\n",
       "    'question': 'Capture stdout for each iterations of a loop separately'},\n",
       "   {'text': 'Calling roc_auc_score() to get auc is throwing the above error.\\nSolution to this issue is to make sure that you pass y_actuals as 1st argument and y_pred as 2nd argument.\\nroc_auc_score(y_train, y_pred)\\nHareesh Tummala',\n",
       "    'section': '6. Decision Trees and Ensemble Learning',\n",
       "    'question': 'ValueError: continuous format is not supported'},\n",
       "   {'text': 'When rmse stops improving means, when it stops to decrease or remains almost similar.\\nPastor Soto',\n",
       "    'section': '6. Decision Trees and Ensemble Learning',\n",
       "    'question': 'Question 3 of homework 6 if i see that rmse goes up at a certain number of n_estimators but then goes back down lower than it was before, should the answer be the number of n_estimators after which rmse initially went up, or the number after which it was its overall lowest value?'},\n",
       "   {'text': 'dot_data = tree.export_graphviz(regr, out_file=None,\\nfeature_names=boston.feature_names,\\nfilled=True)\\ngraphviz.Source(dot_data, format=\"png\")\\nKrishna Anand\\nfrom sklearn import tree\\ntree.plot_tree(dt,feature_names=dv.feature_names_)\\nAdded By Ryan Pramana',\n",
       "    'section': '6. Decision Trees and Ensemble Learning',\n",
       "    'question': 'One of the method to visualize the decision trees'},\n",
       "   {'text': 'Solution: This problem happens because you use DecisionTreeClassifier instead of DecisionTreeRegressor. You should check if you want to use a Decision tree for classification or regression.\\nAlejandro Aponte',\n",
       "    'section': '6. Decision Trees and Ensemble Learning',\n",
       "    'question': \"ValueError: Unknown label type: 'continuous'\"},\n",
       "   {'text': 'When I run dt = DecisionTreeClassifier() in jupyter in same laptop, each time I re-run it or do (restart kernel + run) I get different values of auc. Some of them are 0.674, 0.652, 0.642, 0.669 and so on.  Anyone knows why it could be? I am referring to 7:40-7:45 of video 6.3.\\nSolution: try setting the random seed e.g\\ndt = DecisionTreeClassifier(random_state=22)\\nBhaskar Sarma',\n",
       "    'section': '6. Decision Trees and Ensemble Learning',\n",
       "    'question': 'Different values of auc, each time code is re-run'},\n",
       "   {'text': \"They both do the same, it's just less typing from the script.\\nAsked by Andrew Katoch, Added by Edidiong Esu\",\n",
       "    'section': '6. Decision Trees and Ensemble Learning',\n",
       "    'question': 'Does it matter if we let the Python file create the server or if we run gunicorn directly?'},\n",
       "   {'text': 'When I tried to run example from the video using function ping and can not import it. I use import ping and it was unsuccessful. To fix it I use the statement:\\n\\nfrom [file name] import ping\\nOlga Rudakova',\n",
       "    'section': '6. Decision Trees and Ensemble Learning',\n",
       "    'question': 'No module named ‘ping’?'},\n",
       "   {'text': 'The DictVectorizer has a function to get the feature names get_feature_names_out(). This is helpful for example if you need to analyze feature importance but use the dict vectorizer for one hot encoding. Just keep in mind it does return a numpy array so you may need to convert this to a list depending on your usage for example dv.get_feature_names_out() will return a ndarray array of string objects. list(dv.get_feature_names_out()) will convert to a standard list of strings. Also keep in mind that you first need to fit the predictor and response arrays before you have access to the feature names.\\nQuinn Avila',\n",
       "    'section': '6. Decision Trees and Ensemble Learning',\n",
       "    'question': 'DictVectorizer feature names'},\n",
       "   {'text': \"They both do the same, it's just less typing from the script.\",\n",
       "    'section': '6. Decision Trees and Ensemble Learning',\n",
       "    'question': 'Does it matter if we let the Python file create the server or if we run gunicorn directly?'},\n",
       "   {'text': 'This error occurs because the list of feature names contains some characters like \"<\" that are not supported. To fix this issue, you can replace those problematic characters with supported ones. If you want to create a consistent list of features with no special characters, you can achieve it like this:\\nYou can address this error by replacing problematic characters in the feature names with underscores, like so:\\nfeatures = [f.replace(\\'=<\\', \\'_\\').replace(\\'=\\', \\'_\\') for f in features]\\nThis code will go through the list of features and replace any instances of \"=<\" with \"\", as well as any \"=\" with \"\", ensuring that the feature names only consist of supported characters.',\n",
       "    'section': '6. Decision Trees and Ensemble Learning',\n",
       "    'question': 'ValueError: feature_names must be string, and may not contain [, ] or <'},\n",
       "   {'text': \"To make it easier for us to determine which features are important, we can use a horizontal bar chart to illustrate feature importance sorted by value.\\n1. # extract the feature importances from the model\\nfeature_importances = list(zip(features_names, rdr_model.feature_importances_))\\nimportance_df = pd.DataFrame(feature_importances, columns=['feature_names', 'feature_importances'])\\n2. # sort descending the dataframe by using feature_importances value\\nimportance_df = importance_df.sort_values(by='feature_importances', ascending=False)\\n3. # create a horizontal bar chart\\nplt.figure(figsize=(8, 6))\\nsns.barplot(x='feature_importances', y='feature_names', data=importance_df, palette='Blues_r')\\nplt.xlabel('Feature Importance')\\nplt.ylabel('Feature Names')\\nplt.title('Feature Importance Chart')\\nRadikal Lukafiardi\",\n",
       "    'section': '6. Decision Trees and Ensemble Learning',\n",
       "    'question': 'Visualize Feature Importance by using horizontal bar chart'},\n",
       "   {'text': 'Instead of using np.sqrt() as the second step. You can extract it using like this way :\\nmean_squared_error(y_val, y_predict_val,squared=False)\\nAhmed Okka',\n",
       "    'section': '6. Decision Trees and Ensemble Learning',\n",
       "    'question': 'RMSE using metrics.root_meas_square()'},\n",
       "   {'text': 'I like this visual implementation of features importance in scikit-learn library:\\nhttps://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html\\nIt actually adds std.errors to features importance -> so that you can trace stability of features (important for a model’s explainability) over the different params of the model.\\nIvan Brigida',\n",
       "    'section': '6. Decision Trees and Ensemble Learning',\n",
       "    'question': 'Features Importance graph'},\n",
       "   {'text': 'Expanded error says: xgboost.core.XGBoostError: sklearn needs to be installed in order to use this module. So, sklearn in requirements solved the problem.\\nGeorge Chizhmak',\n",
       "    'section': '6. Decision Trees and Ensemble Learning',\n",
       "    'question': 'xgboost.core.XGBoostError: This app has encountered an error. The original error message is redacted to prevent data leaks.'},\n",
       "   {'text': 'Information gain  in Y due to X, or the mutual information of Y and X\\nWhere  is the entropy of Y. \\n\\nIf X is completely uninformative about Y:\\nIf X is completely informative about Y: )\\nHrithik Kumar Advani',\n",
       "    'section': '6. Decision Trees and Ensemble Learning',\n",
       "    'question': 'Information Gain'},\n",
       "   {'text': 'Filling in missing values using an entire dataset before splitting for training/testing/validation causes',\n",
       "    'section': '6. Decision Trees and Ensemble Learning',\n",
       "    'question': 'Data Leakage'},\n",
       "   {'text': 'Save model by calling ‘booster.save_model’, see eg\\nLoad model:\\nDawuta Smit\\nThis section is moved to Projects',\n",
       "    'section': '8. Neural Networks and Deep Learning',\n",
       "    'question': 'Serialized Model Xgboost error'},\n",
       "   {'text': 'TODO',\n",
       "    'section': '8. Neural Networks and Deep Learning',\n",
       "    'question': 'How to get started with Week 8?'},\n",
       "   {'text': 'Create or import your notebook into Kaggle.\\nClick on the Three dots at the top right hand side\\nClick on Accelerator\\nChoose T4 GPU\\nKhurram Majeed',\n",
       "    'section': '8. Neural Networks and Deep Learning',\n",
       "    'question': 'How to use Kaggle for Deep Learning?'},\n",
       "   {'text': 'Create or import your notebook into Google Colab.\\nClick on the Drop Down at the top right hand side\\nClick on “Change runtime type”\\nChoose T4 GPU\\nKhurram Majeed',\n",
       "    'section': '8. Neural Networks and Deep Learning',\n",
       "    'question': 'How to use Google Colab for Deep Learning?'},\n",
       "   {'text': 'Connecting your GPU on Saturn Cloud to Github repository is not compulsory, since you can just download the notebook and copy it to the Github folder. But if you like technology to do things for you, then follow the solution description below:\\nSolution description: Follow the instructions in these github docs to create an SSH private and public key:\\nhttps://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-ke\\ny-and-adding-it-to-the-ssh-agenthttps://docs.github.com/en/authentication/connecting-to-github-with-ssh/adding-a-new-ssh-key-to-your-github-account?tool=webui\\nThen the second video on this module about saturn cloud would show you how to add the ssh keys to secrets and authenticate through a terminal.\\nOr alternatively, you could just use the public keys provided by Saturn Cloud by default. To do so, follow these steps:\\nClick on your username and on manage\\nDown below you will see the Git SSH keys section.\\nCopy the default public key provided by Saturn Cloud\\nPaste these key into the SSH keys section of your github repo\\nOpen a terminal on Saturn Cloud and run this command “ssh -T git@github.com”\\nYou will receive a successful authentication notice.\\nOdimegwu David',\n",
       "    'section': '8. Neural Networks and Deep Learning',\n",
       "    'question': 'How do I push from Saturn Cloud to Github?'},\n",
       "   {'text': 'This template is referred to in the video 8.1b Setting up the Environment on Saturn Cloud\\nbut the location shown in the video is no longer correct.\\nThis template has been moved to “python deep learning tutorials’ which is shown on the Saturn Cloud home page.\\nSteven Christolis',\n",
       "    'section': '8. Neural Networks and Deep Learning',\n",
       "    'question': 'Where is the Python TensorFlow template on Saturn Cloud?'},\n",
       "   {'text': 'The above error happens since module scipy is not installed in the saturn cloud tensorflow image. While creating the Jupyter server resource, in the “Extra Packages” section under pip in the textbox write scipy. Below the textbox, the pip install scipy command will be displayed. This will ensure when the resource spins up, the scipy package will be automatically installed. This approach can also be followed for additional python packages.\\nSumeet Lalla',\n",
       "    'section': '8. Neural Networks and Deep Learning',\n",
       "    'question': 'Getting error module scipy not found during model training in Saturn Cloud tensorflow image'},\n",
       "   {'text': 'Problem description: Uploading the data to saturn cloud from kaggle can be time saving, specially if the dataset is large.\\nYou can just download to your local machine and then upload to a folder on saturn cloud, but there is a better solution that needs to be set once and you have access to all kaggle datasets in saturn cloud.\\nOn your notebook run:\\n!pip install -q kaggle\\nGo to Kaggle website (you need to have an account for this):\\nClick on your profile image -> Account\\nScroll down to the API box\\nClick on Create New API token\\nIt will download a json file with the name kaggle.json store on your local computer. We need to upload this file in the .kaggle folder\\nOn the notebook click on folder icon on the left upper corner\\nThis will take you to the root folder\\nClick on the .kaggle folder\\nOnce inside of the .kaggle folder upload the kaggle.json file that you downloaded\\nRun this command on your notebook:\\n!chmod 600 /home/jovyan/.kaggle/kaggle.json\\nDownload the data using this command:\\n!kaggle datasets download -d agrigorev/dino-or-dragon\\nCreate a folder to unzip your files:\\n!mkdir data\\nUnzip your files inside that folder\\n!unzip dino-or-dragon.zip -d data\\nPastor Soto',\n",
       "    'section': '8. Neural Networks and Deep Learning',\n",
       "    'question': 'How to upload kaggle data to Saturn Cloud?'},\n",
       "   {'text': 'In order to run tensorflow with gpu on your local machine you’ll need to setup cuda and cudnn.\\nThe process can be overwhelming. Here’s a simplified guide\\nOsman Ali',\n",
       "    'section': '8. Neural Networks and Deep Learning',\n",
       "    'question': 'How to install CUDA & cuDNN on Ubuntu 22.04'},\n",
       "   {'text': 'Problem description:\\nWhen loading saved model getting error: ValueError: Unable to load weights saved in HDF5 format into a subclassed Model which has not created its variables yet. Call the Model first, then load the weights.\\nSolution description:\\nBefore loading model need to evaluate the model on input data: model.evaluate(train_ds)\\nAdded by Vladimir Yesipov',\n",
       "    'section': '8. Neural Networks and Deep Learning',\n",
       "    'question': 'Error: (ValueError: Unable to load weights saved in HDF5 format into a subclassed Model which has not created its variables yet. Call the Model first, then load the weights.) when loading model.'},\n",
       "   {'text': 'Problem description:\\nWhen follow module 8.1b video to setup git in Saturn Cloud, run `ssh -T git@github.com` lead error `git@github.com: Permission denied (publickey).`\\nSolution description:\\nAlternative way, we can setup git in our Saturn Cloud env with generate SSH key in our Saturn Cloud and add it to our git account host. After it, we can access/manage our git through Saturn’s jupyter server. All steps detailed on this following tutorial: https://saturncloud.io/docs/using-saturn-cloud/gitrepo/\\nAdded by Ryan Pramana',\n",
       "    'section': '8. Neural Networks and Deep Learning',\n",
       "    'question': 'Getting error when connect git on Saturn Cloud: permission denied'},\n",
       "   {'text': \"Problem description:\\nGetting an error using <git clone git@github.com:alexeygrigorev/clothing-dataset-small.git>\\nThe error:\\nCloning into 'clothing-dataset'...\\nHost key verification failed.\\nfatal: Could not read from remote repository.\\nPlease make sure you have the correct access rights\\nand the repository exists.\\nSolution description:\\nwhen cloning the repo, you can also chose https - then it should work. This happens when you don't have your ssh key configured.\\n<git clone https://github.com/alexeygrigorev/clothing-dataset-small.git>\\nAdded by Gregory Morris\",\n",
       "    'section': '8. Neural Networks and Deep Learning',\n",
       "    'question': 'Host key verification failed.'},\n",
       "   {'text': \"Problem description\\nThe accuracy and the loss are both still the same or nearly the same while training.\\nSolution description\\nIn the homework, you should set class_mode='binary' while reading the data.\\nAlso, problem occurs when you choose the wrong optimizer, batch size, or learning rate\\nAdded by Ekaterina Kutovaia\",\n",
       "    'section': '8. Neural Networks and Deep Learning',\n",
       "    'question': 'The same accuracy on epochs'},\n",
       "   {'text': 'Problem:\\nWhen resuming training after augmentation, the loss skyrockets (1000+ during first epoch) and accuracy settles around 0.5 – i.e. the model becomes as good as a random coin flip.\\nSolution:\\nCheck that the augmented ImageDataGenerator still includes the option “rescale” as specified in the preceding step.\\nAdded by Konrad Mühlberg',\n",
       "    'section': '8. Neural Networks and Deep Learning',\n",
       "    'question': 'Model breaking after augmentation – high loss + bad accuracy'},\n",
       "   {'text': \"While doing:\\nimport tensorflow as tf\\nfrom tensorflow import keras\\nmodel = tf.keras.models.load_model('model_saved.h5')\\nIf you get an error message like this:\\nValueError: The channel dimension of the inputs should be defined. The input_shape received is (None, None, None, None), where axis -1 (0-based) is the channel dimension, which found to be `None`.\\nSolution:\\nSaving a model (either yourself via model.save() or via checkpoint when save_weights_only = False) saves two things: The trained model weights (for example the best weights found during training) and the model architecture.  If the number of channels is not explicitly specified in the Input layer of the model, and is instead defined as a variable, the model architecture will not have the value in the variable stored. Therefore when the model is reloaded, it will complain about not knowing the number of channels. See the code below, in the first line, you need to specify number of channels explicitly:\\n# model architecture:\\ninputs = keras.Input(shape=(input_size, input_size, 3))\\nbase = base_model(inputs, training=False)\\nvectors = keras.layers.GlobalAveragePooling2D()(base)\\ninner = keras.layers.Dense(size_inner, activation='relu')(vectors)\\ndrop = keras.layers.Dropout(droprate)(inner)\\noutputs = keras.layers.Dense(10)(drop)\\nmodel = keras.Model(inputs, outputs)\\n(Memoona Tahira)\",\n",
       "    'section': '8. Neural Networks and Deep Learning',\n",
       "    'question': 'Missing channel value error while reloading model:'},\n",
       "   {'text': \"Problem:\\nA dataset for homework is in a zipped folder. If you unzip it within a jupyter notebook by means of ! unzip command, you’ll see a huge amount of output messages about unzipping of each image. So you need to suppress this output\\nSolution:\\nExecute the next cell:\\n%%capture\\n! unzip zipped_folder_name.zip -d destination_folder_name\\nAdded by Alena Kniazeva\\nInside a Jupyter Notebook:\\nimport zipfile\\nlocal_zip = 'data.zip'\\nzip_ref = zipfile.ZipFile(local_zip, 'r')\\nzip_ref.extractall('data')\\nzip_ref.close()\",\n",
       "    'section': '8. Neural Networks and Deep Learning',\n",
       "    'question': 'How to unzip a folder with an image dataset and suppress output?'},\n",
       "   {'text': 'Problem:\\nWhen we run train_gen.flow_from_directory() as in video 8.5, it finds images belonging to 10 classes. Does it understand the names of classes from the names of folders? Or, there is already something going on deep behind?\\nSolution:\\nThe name of class is the folder name\\nIf you just create some random folder with the name \"xyz\", it will also be considered as a class!! The name itself is saying flow_from_directory\\na clear explanation below:\\nhttps://vijayabhaskar96.medium.com/tutorial-image-classification-with-keras-flow-from-directory-and-generators-95f75ebe5720\\nAdded by Bhaskar Sarma',\n",
       "    'section': '8. Neural Networks and Deep Learning',\n",
       "    'question': 'How keras flow_from_directory know the names of classes in images?'},\n",
       "   {'text': 'Problem:\\nI created a new environment in SaturnCloud and chose the image corresponding to Saturn with Tensorflow, but when I tried to fit the model it showed an error about the missing module: scipy\\nSolution:\\nInstall the module in a new cell: !pip install scipy\\nRestart the kernel and fit the model again\\nAdded by Erick Calderin',\n",
       "    'section': '8. Neural Networks and Deep Learning',\n",
       "    'question': 'Error with scipy missing module in SaturnCloud'},\n",
       "   {'text': 'The command to read folders in the dataset in the tensorflow source code is:\\nfor subdir in sorted(os.listdir(directory)):\\n…\\nReference: https://github.com/keras-team/keras/blob/master/keras/preprocessing/image.py, line 563\\nThis means folders will be read in alphabetical order. For example, in the case of a folder named dino, and another named dragon, dino will read first and will have class label 0, whereas dragon will be read in next and will have class label 1.\\nWhen a Keras model predicts binary labels, it will only return one value, and this is the probability of class 1 in case of sigmoid activation function in the last dense layer with 2 neurons. The probability of class 0 can be found out by:\\nprob(class(0)) = 1- prob(class(1))\\nIn case of using from_logits to get results, you will get two values for each of the labels.\\nA prediction of 0.8 is saying the probability that the image has class label 1 (in this case dragon), is 0.8, and conversely we can infer the probability that the image has class label 0 is 0.2.\\n(Added by Memoona Tahira)',\n",
       "    'section': '8. Neural Networks and Deep Learning',\n",
       "    'question': 'How are numeric class labels determined in flow_from_directroy using binary class mode and what is meant by the single probability predicted by a binary Keras model:'},\n",
       "   {'text': \"It's fine, some small changes are expected\\nAlexey Grigorev\",\n",
       "    'section': '8. Neural Networks and Deep Learning',\n",
       "    'question': 'Does the actual values matter after predicting with a neural network or it should be treated as like hood of falling in a class?'},\n",
       "   {'text': 'Problem:\\nI found running the wasp/bee model on my mac laptop had higher reported accuracy and lower std deviation than the HW answers. This may be because of the SGD optimizer. Running this on my mac printed a message about a new and legacy version that could be used.\\nSolution:\\nTry running the same code on google collab or another way. The answers were closer for me on collab. Another tip is to change the runtime to use T4 and the model run’s faster than just CPU\\nAdded by Quinn Avila',\n",
       "    'section': '8. Neural Networks and Deep Learning',\n",
       "    'question': 'What if your accuracy and std training loss don’t match HW?'},\n",
       "   {'text': 'When running “model.fit(...)” an additional parameter “workers” can be specified for speeding up the data loading/generation. The default value is “1”. Try out which value between 1 and the cpu count on your system performs best.\\nhttps://www.tensorflow.org/api_docs/python/tf/keras/Model#fit\\nAdded by Sylvia Schmitt',\n",
       "    'section': '8. Neural Networks and Deep Learning',\n",
       "    'question': 'Using multi-threading for data generation in “model.fit()”'},\n",
       "   {'text': 'Reproducibility for training runs can be achieved following these instructions: \\nhttps://www.tensorflow.org/versions/r2.8/api_docs/python/tf/config/experimental/enable_op_determinism\\nseed = 1234\\ntf.keras.utils.set_random_seed(seed)\\ntf.config.experimental.enable_op_determinism()\\nThis will work for a script, if this gets executed multiple times.\\nAdded by Sylvia Schmitt',\n",
       "    'section': '8. Neural Networks and Deep Learning',\n",
       "    'question': 'Reproducibility with TensorFlow using a seed point'},\n",
       "   {'text': 'Pytorch is also a deep learning framework that allows to do equivalent tasks as keras. Here is a tutorial to create a CNN from scratch using pytorch :\\nhttps://blog.paperspace.com/writing-cnns-from-scratch-in-pytorch/\\nThe functions have similar goals. The syntax can be slightly different. For the lessons and the homework, we use keras, but one can feel free to make a pull request with the equivalent with pytorch for lessons and homework!\\nMélanie Fouesnard',\n",
       "    'section': '8. Neural Networks and Deep Learning',\n",
       "    'question': 'Can we use pytorch for this lesson/homework ?'},\n",
       "   {'text': \"While training a Keras model you get the error “Failed to find data adapter that can handle input: <class 'keras.src.preprocessing.image.ImageDataGenerator'>, <class 'NoneType'>” you may have unintentionally passed the image generator instead of the dataset to the model\\ntrain_gen = ImageDataGenerator(rescale=1./255)\\ntrain_ds = train_gen.flow_from_directory(…)\\nhistory_after_augmentation = model.fit(\\ntrain_gen, # this should be train_ds!!!\\nepochs=10,\\nvalidation_data=test_gen # this should be test_ds!!!\\n)\\nThe fix is simple and probably obvious once pointed out, use the training and validation dataset (train_ds and val_ds) returned from flow_from_directory\\nAdded by Tzvi Friedman\",\n",
       "    'section': '8. Neural Networks and Deep Learning',\n",
       "    'question': 'Keras model training fails with “Failed to find data adapter”'},\n",
       "   {'text': 'The command ‘nvidia-smi’ has a built-in function which will run it in subsequently updating it every N seconds without the need of using the command ‘watch’.\\nnvidia-smi -l <N seconds>\\nThe following command will run ‘nvidia-smi’ every 2 seconds until interrupted using CTRL+C.\\nnvidia-smi -l 2\\nAdded by Sylvia Schmitt',\n",
       "    'section': '8. Neural Networks and Deep Learning',\n",
       "    'question': 'Running ‘nvidia-smi’ in a loop without using ‘watch’'},\n",
       "   {'text': 'The Python package ‘’ is an interactive GPU process viewer similar to ‘htop’ for CPU.\\nhttps://pypi.org/project//\\nImage source: https://pypi.org/project//\\nAdded by Sylvia Schmitt',\n",
       "    'section': '8. Neural Networks and Deep Learning',\n",
       "    'question': 'Checking GPU and CPU utilization using ‘nvitop’'},\n",
       "   {'text': \"Let’s say we define our Conv2d layer like this:\\n>> tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3))\\nIt means our input image is RGB (3 channels, 150 by 150 pixels), kernel is 3x3 and number of filters (layer’s width) is 32.\\nIf we check model.summary() we will get this:\\n_________________________________________________________________\\nLayer (type)                Output Shape              Param #\\n=================================================================\\nconv2d (Conv2D)             (None, 148, 148, 32)      896\\nSo where does 896 params come from? It’s computed like this:\\n>>> (3*3*3 +1) * 32\\n896\\n# 3x3 kernel, 3 channels RGB, +1 for bias, 32 filters\\nWhat about the number of “features” we get after the Flatten layer?\\nFor our homework model.summary() for last MaxPooling2d and Flatten layers looked like this:\\n_________________________________________________________________\\nLayer (type)                Output Shape              Param #\\n=================================================================\\nmax_pooling2d_3       (None, 7, 7, 128)         0\\nflatten (Flatten)           (None, 6272)              0\\nSo where do 6272 vectors come from? It’s computed like this:\\n>>> 7*7*128\\n6272\\n# 7x7 “image shape” after several convolutions and poolings, 128 filters\\nAdded by Andrii Larkin\",\n",
       "    'section': '8. Neural Networks and Deep Learning',\n",
       "    'question': 'Q: Where does the number of Conv2d layer’s params come from? Where does the number of “features” we get after the Flatten layer come from?'},\n",
       "   {'text': 'It’s quite useful to understand that all types of models in the course are a plain stack of layers where each layer has exactly one input tensor and one output tensor (Sequential model TF page, Sequential class).\\nYou can simply start from an “empty” model and add more and more layers in a sequential order.\\nThis mode is called “Sequential Model API”  (easier)\\nIn Alexey’s videos it is implemented as chained calls of different entities (“inputs”,“base”, “vectors”,  “outputs”) in a more advanced mode “Functional Model API”.\\nMaybe a more complicated way makes sense when you do Transfer Learning and want to separate “Base” model vs. rest, but in the HW you need to recreate the full model from scratch ⇒ I believe it is easier to work with a sequence of “similar” layers.\\nYou can read more about it in this TF2 tutorial.\\nA really useful Sequential model example is shared in the Kaggle’s “Bee or Wasp” dataset folder with code: notebook\\nAdded by Ivan Brigida\\nFresh Run on Neural Nets\\nWhile correcting an error on neural net architecture, it is advised to do fresh run by restarting kernel, else the model learns on top of previous runs.\\nAdded by Abhijit Chakraborty',\n",
       "    'section': '8. Neural Networks and Deep Learning',\n",
       "    'question': 'Sequential vs. Functional Model Modes in Keras (TF2)'},\n",
       "   {'text': \"I found this code snippet fixed my OOM errors, as I have an Nvidia GPU. Can't speak to OOM errors on CPU, though.\\nhttps://www.tensorflow.org/api_docs/python/tf/config/experimental/set_memory_growth\\n```\\nphysical_devices = tf.configlist_physical_devices('GPU')\\ntry:\\ntf.config.experimental.set_memory_growth(physical_devices[0],True)\\nexcept:\\n# Invalid device or cannot modify virtual devices once initialized.\\npass\\n```\",\n",
       "    'section': '8. Neural Networks and Deep Learning',\n",
       "    'question': 'Out of memory errors when running tensorflow'},\n",
       "   {'text': 'When training the models, in the fit function, you can specify the number of workers/threads.\\nThe number of threads apparently also works for GPUs, and came very handy in google colab for the T4 GPU, since it was very very slow, and workers default value is 1.\\nI changed the workers variable to 2560, following this thread in stackoverflow. I am using the free T4 GPU.  (https://stackoverflow.com/questions/68208398/how-to-find-the-number-of-cores-in-google-colabs-gpu)\\nAdded by Ibai Irastorza',\n",
       "    'section': '8. Neural Networks and Deep Learning',\n",
       "    'question': 'Model training very slow in google colab with T4 GPU'},\n",
       "   {'text': 'From the keras documentation:\\nDeprecated: tf.keras.preprocessing.image.ImageDataGenerator is not recommended for new code. Prefer loading images with tf.keras.utils.image_dataset_from_directory and transforming the output tf.data.Dataset with preprocessing layers. For more information, see the tutorials for loading images and augmenting images, as well as the preprocessing layer guide.\\nHrithik Kumar Advani',\n",
       "    'section': '9. Serverless Deep Learning',\n",
       "    'question': 'Using image_dataset_from_directory instead of ImageDataGeneratorn for loading images'},\n",
       "   {'text': 'TODO',\n",
       "    'section': '9. Serverless Deep Learning',\n",
       "    'question': 'How to get started with Week 9?'},\n",
       "   {'text': 'The week 9 uses a link to github to fetch the models.\\nThe original link was moved to here:\\nhttps://github.com/DataTalksClub/machine-learning-zoomcamp/releases',\n",
       "    'section': '9. Serverless Deep Learning',\n",
       "    'question': 'Where is the model for week 9?'},\n",
       "   {'text': 'Solution description\\nIn the unit 9.6, Alexey ran the command echo ${REMOTE_URI} which turned the URI address in the terminal. There workaround is to set a local variable (REMOTE_URI) and assign your URI address in the terminal and use it to login the registry, for instance, REMOTE_URI=2278222782.dkr.ecr.ap-south-1.amazonaws.com/clothing-tflite-images (fake address). One caveat is that you will lose this variable once the session is terminated.\\nI also had the same problem on Ubuntu terminal. I executed the following two commands:\\n$ export REMOTE_URI=1111111111.dkr.ecr.us-west-1.amazonaws.com/clothing-tflite-images:clothing-model-xception-v4-001\\n$ echo $REMOTE_URI\\n111111111.dkr.ecr.us-west-1.amazonaws.com/clothing-tflite-images:clothing-model-xception-v4-001\\nNote: 1. no curly brackets (e.g. echo ${REMOTE_URI}) needed unlike in video 9.6,\\n2. Replace REMOTE_URI with your URI\\n(Bhaskar Sarma)',\n",
       "    'section': '9. Serverless Deep Learning',\n",
       "    'question': 'Executing the command echo ${REMOTE_URI} returns nothing.'},\n",
       "   {'text': 'The command aws ecr get-login --no-include-email returns an invalid choice error:\\nThe solution is to use the following command instead:  aws ecr get-login-password\\nCould simplify the login process with, just replace the <ACCOUNT_NUMBER> and <REGION> with your values:\\nexport PASSWORD=`aws ecr get-login-password`\\ndocker login -u AWS -p $PASSWORD <ACCOUNT_NUMBER>.dkr.ecr.<REGION>.amazonaws.com/clothing-tflite-images\\nAdded by Martin Uribe',\n",
       "    'section': '9. Serverless Deep Learning',\n",
       "    'question': 'Getting a syntax error while trying to get the password from aws-cli'},\n",
       "   {'text': 'We can use the keras.models.Sequential() function to pass many parameters of the cnn at once.\\nKrishna Anand',\n",
       "    'section': '9. Serverless Deep Learning',\n",
       "    'question': 'Pass many parameters in the model at once'},\n",
       "   {'text': 'This error is produced sometimes when building your docker image from the Amazon python base image.\\nSolution description: The following could solve the problem.\\nUpdate your docker desktop if you haven’t done so.\\nOr restart docker desktop and terminal and then build the image all over again.\\nOr if all else fails, first run the following command: DOCKER_BUILDKIT=0  docker build .  then build your image.\\n(optional) Added by Odimegwu David',\n",
       "    'section': '9. Serverless Deep Learning',\n",
       "    'question': 'Getting  ERROR [internal] load metadata for public.ecr.aws/lambda/python:3.8'},\n",
       "   {'text': \"When trying to run the command  !ls -lh in windows jupyter notebook  , I was getting an error message that says “'ls' is not recognized as an internal or external command,operable program or batch file.\\nSolution description :\\nInstead of !ls -lh , you can use this command !dir , and you will get similar output\\nAsia Saeed\",\n",
       "    'section': '9. Serverless Deep Learning',\n",
       "    'question': \"Problem: 'ls' is not recognized as an internal or external command, operable program or batch file.\"},\n",
       "   {'text': 'When I run   import tflite_runtime.interpreter as tflite , I get an error message says “ImportError: generic_type: type \"InterpreterWrapper\" is already registered!”\\nSolution description\\nThis error occurs when you import both tensorflow  and tflite_runtime.interpreter  “import tensorflow as tf” and “import tflite_runtime.interpreter as tflite” in the same notebook.  To fix the issue, restart the kernel and import only tflite_runtime.interpreter \" import tflite_runtime.interpreter as tflite\".\\nAsia Saeed',\n",
       "    'section': '9. Serverless Deep Learning',\n",
       "    'question': 'ImportError: generic_type: type \"InterpreterWrapper\" is already registered!'},\n",
       "   {'text': 'Problem description:\\nIn command line try to do $ docker build -t dino_dragon\\ngot this Using default tag: latest\\n[2022-11-24T06:48:47.360149000Z][docker-credential-desktop][W] Windows version might not be up-to-date: The system cannot find the file specified.\\nerror during connect: This error may indicate that the docker daemon is not running.: Post\\n.\\nSolution description:\\nYou need to make sure that Docker is not stopped by a third-party program.\\nAndrei Ilin',\n",
       "    'section': '9. Serverless Deep Learning',\n",
       "    'question': 'Windows version might not be up-to-date'},\n",
       "   {'text': 'When running docker build -t dino-dragon-model it returns the above error\\nThe most common source of this error in this week is because Alex video shows a version of the wheel with python 8, we need to find a wheel with the version that we are working on. In this case python 9. Another common error is to copy the link, this will also produce the same error, we need to download the raw format:\\nhttps://github.com/alexeygrigorev/tflite-aws-lambda/raw/main/tflite/tflite_runtime-2.7.0-cp39-cp39-linux_x86_64.whl\\nPastor Soto',\n",
       "    'section': '9. Serverless Deep Learning',\n",
       "    'question': 'WARNING: You are using pip version 22.0.4; however, version 22.3.1 is available'},\n",
       "   {'text': 'Problem description:\\nIn video 9.6, after installing aswcli, we should configure it with aws configure . There it asks for Access Key ID, Secret Access Key, Default Region Name and also Default output format. What we should put for Default output format? Leaving it as  None is okay?\\nSolution description:\\nYes, in my I case I left everything as the provided defaults (except, obviously, the Access key and the secret access key)\\nAdded by Bhaskar Sarma',\n",
       "    'section': '9. Serverless Deep Learning',\n",
       "    'question': 'How to do AWS configure after installing awscli'},\n",
       "   {'text': 'Problem:\\nWhile passing local testing of the lambda function without issues, trying to test the same input with a running docker instance results in an error message like\\n{‘errorMessage’: ‘Unable to marshal response: Object of type float32 is not JSON serializable’, ‘errorType’: ‘Runtime.MarshalError’, ‘requestId’: ‘f155492c-9af2-4d04-b5a4-639548b7c7ac’, ‘stackTrace’: []}\\nThis happens when a model (in this case the dino vs dragon model) returns individual estimation values as numpy float32 values (arrays). They need to be converted individually to base-Python floats in order to become “serializable”.\\nSolution:\\nIn my particular case, I set up the dino vs dragon model in such a way as to return a label + predicted probability for each class as follows (below is a two-line extract of function predict() in the lambda_function.py):\\npreds = [interpreter.get_tensor(output_index)[0][0], \\\\\\n1-interpreter.get_tensor(output_index)[0][0]]\\nIn which case the above described solution will look like this:\\npreds = [float(interpreter.get_tensor(output_index)[0][0]), \\\\\\nfloat(1-interpreter.get_tensor(output_index)[0][0])]\\nThe rest can be made work by following the chapter 9 (and/or chapter 5!) lecture videos step by step.\\nAdded by Konrad Muehlberg',\n",
       "    'section': '9. Serverless Deep Learning',\n",
       "    'question': 'Object of type float32 is not JSON serializable'},\n",
       "   {'text': 'I had this error when running the command line : interpreter.set_tensor(input_index, x) that can be seen in the video 9.3 around 12 minutes.\\nValueError: Cannot set tensor: Got value of type UINT8 but expected type FLOAT32 for input 0, name: serving_default_conv2d_input:0\\nThis is because the X is an int but a float is expected.\\nSolution:\\nI found this solution from this question here https://stackoverflow.com/questions/76102508/valueerror-cannot-set-tensor-got-value-of-type-float64-but-expected-type-float :\\n# Need to convert to float32 before set_tensor\\nX = np.float32(X)\\nThen, it works. I work with tensorflow 2.15.0, maybe the fact that this version is more recent involves this change ?\\nAdded by Mélanie Fouesnard',\n",
       "    'section': '9. Serverless Deep Learning',\n",
       "    'question': 'Error with the line “interpreter.set_tensor(input_index, X”)'},\n",
       "   {'text': 'To check your file size using the powershell terminal, you can do the following command lines:\\n$File = Get-Item -Path path_to_file\\n$FileSize = (Get-Item -Path $FilePath).Length\\nNow you can check the size of your file, for example in MB:\\nWrite-host \"MB\":($FileSize/1MB)\\nSource: https://www.sharepointdiary.com/2020/10/powershell-get-file-size.html#:~:text=To%20get%20the%20size%20of,the%20file%2C%20including%20its%20size.\\nAdded by Mélanie Fouesnard',\n",
       "    'section': '9. Serverless Deep Learning',\n",
       "    'question': 'How to easily get file size in powershell terminal ?'},\n",
       "   {'text': 'I wanted to understand how lambda container images work in depth and how lambda functions are initialized, for this reason, I found the following documentation\\nhttps://docs.aws.amazon.com/lambda/latest/dg/images-create.html\\nhttps://docs.aws.amazon.com/lambda/latest/dg/runtimes-api.html\\nAdded by Alejandro aponte',\n",
       "    'section': '9. Serverless Deep Learning',\n",
       "    'question': 'How do Lambda container images work?'},\n",
       "   {'text': 'The docker image for aws lambda can be created and pushed to aws ecr and the same can be exposed as a REST API through APIGatewayService in a single go using AWS Serverless Framework. Refer the below article for a detailed walkthrough.\\nhttps://medium.com/hoonio/deploy-containerized-serverless-flask-to-aws-lambda-c0eb87c1404d\\nAdded by Sumeet Lalla',\n",
       "    'section': '9. Serverless Deep Learning',\n",
       "    'question': 'How to use AWS Serverless Framework to deploy on AWS Lambda and expose it as REST API through APIGatewayService?'},\n",
       "   {'text': 'Problem:\\nWhile trying to build docker image in Section 9.5 with the command:\\ndocker build -t clothing-model .\\nIt throws a pip install error for the tflite runtime whl\\nERROR: failed to solve: process \"/bin/sh -c pip install https://github.com/alexeygrigorev/tflite-aws-lambda/blob/main/tflite/tflite_runtime-2.14.0-cp310-cp310-linux_x86_64.whl\" did not complete successfully: exit code: 1\\nTry to use this link: https://github.com/alexeygrigorev/tflite-aws-lambda/raw/main/tflite/tflite_runtime-2.14.0-cp310-cp310-linux_x86_64.whl\\nIf the link above does not work:\\nThe problem is because of the arm architecture of the M1. You will need to run the code on a PC or Ubuntu OS.\\nOr try the code bellow.\\nAdded by Dashel Ruiz Perez\\nSolution:\\nTo build the Docker image, use the command:\\ndocker build --platform linux/amd64 -t clothing-model .\\nTo run the built image, use the command:\\ndocker run -it --rm -p 8080:8080 --platform linux/amd64 clothing-model:latest\\nAdded by Daniel Egbo',\n",
       "    'section': '9. Serverless Deep Learning',\n",
       "    'question': 'Error building docker image on M1 Mac'},\n",
       "   {'text': \"Problem: Trying to test API gateway in 9.7 - API Gateway: Exposing the Lambda Function, running: $ python test.py\\nWith error message:\\n{'message': 'Missing Authentication Token'}\\nSolution:\\nNeed to get the deployed API URL for the specific path you are invoking. Example:\\nhttps://<random string>.execute-api.us-east-2.amazonaws.com/test/predict\\nAdded by Andrew Katoch\",\n",
       "    'section': '9. Serverless Deep Learning',\n",
       "    'question': 'Error invoking API Gateway deploy API locally'},\n",
       "   {'text': 'Problem: When trying to install tflite_runtime with\\n!pip install --extra-index-url https://google-coral.github.io/py-repo/ tflite_runtime\\none gets an error message above.\\nSolution:\\nfflite_runtime is only available for the os-python version combinations that can be found here: https://google-coral.github.io/py-repo/tflite-runtime/\\nyour combination must be missing here\\nyou can see if any of these work for you https://github.com/alexeygrigorev/tflite-aws-lambda/tree/main/tflite\\nand install the needed one using pip\\neg\\npip install https://github.com/alexeygrigorev/tflite-aws-lambda/raw/main/tflite/tflite_runtime-2.7.0-cp38-cp38-linux_x86_64.whl\\nas it is done in the lectures code:\\nhttps://github.com/alexeygrigorev/mlbookcamp-code/blob/master/course-zoomcamp/09-serverless/code/Dockerfile#L4\\nAlternatively, use a virtual machine (with VM VirtualBox, for example) with a Linux system. The other way is to run a code at a virtual machine within cloud service, for example you can use Vertex AI Workbench at GCP (notebooks and terminals are provided there, so all tasks may be performed).\\nAdded by Alena Kniazeva, modified by Alex Litvinov',\n",
       "    'section': '9. Serverless Deep Learning',\n",
       "    'question': 'Error: Could not find a version that satisfies the requirement tflite_runtime (from versions:none)'},\n",
       "   {'text': 'docker: Error response from daemon: mkdir /var/lib/docker/overlay2/37be849565da96ac3fce34ee9eb2215bd6cd7899a63ebc0ace481fd735c4cb0e-init: read-only file system.\\nYou need to restart the docker services to get rid of the above error\\nKrishna Anand',\n",
       "    'section': '9. Serverless Deep Learning',\n",
       "    'question': 'Docker run error'},\n",
       "   {'text': 'The docker image can be saved/exported to tar format in local machine using the below command:\\ndocker image save <image-name> -o <name-of-tar-file.tar>\\nThe individual layers of the docker image for the filesystem content can be viewed by extracting the layer.tar present in the <name-of-tar-file.tar> created from above.\\nSumeet Lalla',\n",
       "    'section': '9. Serverless Deep Learning',\n",
       "    'question': 'Save Docker Image to local machine and view contents'},\n",
       "   {'text': 'On vscode running jupyter notebook. After I ‘pip install pillow’, my notebook did not recognize using the import for example from PIL import image. After restarting the jupyter notebook the imports worked.\\nQuinn Avila',\n",
       "    'section': '9. Serverless Deep Learning',\n",
       "    'question': 'Jupyter notebook not seeing package'},\n",
       "   {'text': 'Due to experimenting back and forth so much without care for storage, I just ran out of it on my 30-GB AWS instance. It turns out that deleting docker images does not actually free up any space as you might expect. After removing images, you also need to run docker system prune',\n",
       "    'section': '9. Serverless Deep Learning',\n",
       "    'question': 'Running out of space for AWS instance.'},\n",
       "   {'text': 'Using the 2.14 version with python 3.11 works fine.\\nIn case it doesn’t work, I tried with tensorflow 2.4.4 whl, however, make sure to run it on top of supported python versions like 3.8, else there will be issues installing tf==2.4.4\\nAdded by Abhijit Chakraborty',\n",
       "    'section': '9. Serverless Deep Learning',\n",
       "    'question': 'Using Tensorflow 2.15 for AWS deployment'},\n",
       "   {'text': 'see here',\n",
       "    'section': '9. Serverless Deep Learning',\n",
       "    'question': 'Command aws ecr get-login --no-include-email returns “aws: error: argument operation: Invalid choice…”'},\n",
       "   {'text': 'Sign in to the AWS Console: Log in to the AWS Console.\\nNavigate to IAM: Go to the IAM service by clicking on \"Services\" in the top left corner and selecting \"IAM\" under the \"Security, Identity, & Compliance\" section.\\nCreate a new policy: In the left navigation pane, select \"Policies\" and click on \"Create policy.\"\\nSelect the service and actions:\\nClick on \"JSON\" and copy and paste the JSON policy you provided earlier for the specific ECR actions.\\nReview and create the policy:\\nClick on \"Review policy.\"\\nProvide a name and description for the policy.\\nClick on \"Create policy.\"\\nJSON policy:\\n{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"VisualEditor0\",\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"ecr:CreateRepository\",\\n\"ecr:GetAuthorizationToken\",\\n\"ecr:BatchCheckLayerAvailability\",\\n\"ecr:BatchGetImage\",\\n\"ecr:InitiateLayerUpload\",\\n\"ecr:UploadLayerPart\",\\n\"ecr:CompleteLayerUpload\",\\n\"ecr:PutImage\"\\n],\\n\"Resource\": \"*\"\\n}\\n]\\n}\\nAdded by: Daniel Muñoz-Viveros\\nERROR: failed to solve: public.ecr.aws/lambda/python:3.10: error getting credentials - err: exec: \"docker-credential-desktop.exe\": executable file not found in $PATH, out: ``\\n(WSL2 system)\\nSolved: Delete the file ~/.docker/config.json\\nYishan Zhan',\n",
       "    'section': '9. Serverless Deep Learning',\n",
       "    'question': 'What IAM permission policy is needed to complete Week 9: Serverless?'},\n",
       "   {'text': 'Add the next lines to vim /etc/docker/daemon.json\\n{\\n\"dns\": [\"8.8.8.8\", \"8.8.4.4\"]\\n}\\nThen, restart docker:  sudo service docker restart\\nIbai Irastorza',\n",
       "    'section': '9. Serverless Deep Learning',\n",
       "    'question': 'Docker Temporary failure in name resolution'},\n",
       "   {'text': \"Solution: add compile = False to the load_model function\\nkeras.models.load_model('model_name.h5', compile=False)\\nNadia Paz\",\n",
       "    'section': '9. Serverless Deep Learning',\n",
       "    'question': 'Keras model *.h5 doesn’t load. Error: weight_decay is not a valid argument, kwargs should be empty  for `optimizer_experimental.Optimizer`'},\n",
       "   {'text': 'This deployment setup can be tested locally using AWS RIE (runtime interface emulator).\\nBasically, if your Docker image was built upon base AWS Lambda image (FROM public.ecr.aws/lambda/python:3.10) - just use certain ports for “docker run” and a certain “localhost link” for testing:\\ndocker run -it --rm -p 9000:8080 name\\nThis command runs the image as a container and starts up an endpoint locally at:\\nlocalhost:9000/2015-03-31/functions/function/invocations\\nPost an event to the following endpoint using a curl command:\\ncurl -XPOST \"http://localhost:9000/2015-03-31/functions/function/invocations\" -d \\'{}\\'\\nExamples of curl testing:\\n* windows testing:\\ncurl -XPOST \"http://localhost:9000/2015-03-31/functions/function/invocations\" -d \"{\\\\\"url\\\\\": \\\\\"https://habrastorage.org/webt/rt/d9/dh/rtd9dhsmhwrdezeldzoqgijdg8a.jpeg\\\\\"}\"\\n* unix testing:\\ncurl -XPOST \"http://localhost:9000/2015-03-31/functions/function/invocations\" -d \\'{\"url\": \"https://habrastorage.org/webt/rt/d9/dh/rtd9dhsmhwrdezeldzoqgijdg8a.jpeg\"}\\'\\nIf during testing you encounter an error like this:\\n# {\"errorMessage\": \"Unable to marshal response: Object of type float32 is not JSON serializable\", \"errorType\": \"Runtime.MarshalError\", \"requestId\": \"7ea5d17a-e0a2-48d5-b747-a16fc530ed10\", \"stackTrace\": []}\\njust turn your response at lambda_handler() to string - str(result).\\nAdded by Andrii Larkin',\n",
       "    'section': '9. Serverless Deep Learning',\n",
       "    'question': 'How to test AWS Lambda + Docker locally?'},\n",
       "   {'text': 'Make sure all codes in test.py dont have any dependencies with tensorflow library. One of most common reason that lead the this error is tflite still imported from tensorflow. Change import tensorflow.lite as tflite to import tflite_runtime.interpreter as tflite\\nAdded by Ryan Pramana',\n",
       "    'section': '9. Serverless Deep Learning',\n",
       "    'question': '\"Unable to import module \\'lambda_function\\': No module named \\'tensorflow\\'\" when run python test.py'},\n",
       "   {'text': 'I’ve tried to do everything in Google Colab. Here is a way to work with Docker in Google Colab:\\nhttps://gist.github.com/mwufi/6718b30761cd109f9aff04c5144eb885\\n\\uec03%%shell\\npip install udocker\\nudocker --allow-root install\\n\\uec02!udocker --allow-root run hello-world\\nAdded by Ivan Brigida\\nLambda API Gateway errors:\\n`Authorization header requires \\'Credential\\' parameter. Authorization header requires \\'Signature\\' parameter. Authorization header requires \\'SignedHeaders\\' parameter. Authorization header requires existence of either a \\'X-Amz-Date\\' or a \\'Date\\' header.`\\n`Missing Authentication Token`\\nimport boto3\\nclient = boto3.client(\\'apigateway\\')\\nresponse = client.test_invoke_method(\\nrestApiId=\\'your_rest_api_id\\',\\nresourceId=\\'your_resource_id\\',\\nhttpMethod=\\'POST\\',\\npathWithQueryString=\\'/test/predict\\', #depend how you set up the api\\nbody=\\'{\"url\": \"https://habrastorage.org/webt/rt/d9/dh/rtd9dhsmhwrdezeldzoqgijdg8a.jpeg\"}\\'\\n)\\nprint(response[\\'body\\'])\\nYishan Zhan\\nUnable to run pip install tflite_runtime from github wheel links?\\nTo overcome this issue, you can download the whl file to your local project folder and in the Docker file add the following lines:\\nCOPY <file-name> .\\nRUN pip install <file-name>\\nAbhijit Chakraborty',\n",
       "    'section': '10. Kubernetes and TensorFlow Serving',\n",
       "    'question': 'Install Docker (udocker) in Google Colab'},\n",
       "   {'text': 'TODO',\n",
       "    'section': '10. Kubernetes and TensorFlow Serving',\n",
       "    'question': 'How to get started with Week 10?'},\n",
       "   {'text': 'Running a CNN on your CPU can take a long time and once you’ve run out of free time on some cloud providers, it’s time to pay up. Both can be tackled by installing tensorflow with CUDA support on your local machine if you have the right hardware.\\nI was able to get it working by using the following resources:\\nCUDA on WSL :: CUDA Toolkit Documentation (nvidia.com)\\nInstall TensorFlow with pip\\nStart Locally | PyTorch\\nI included the link to PyTorch so that you can get that one installed and working too while everything is fresh on your mind. Just select your options, and for Computer Platform, I chose CUDA 11.7 and it worked for me.\\nAdded by Martin Uribe',\n",
       "    'section': '10. Kubernetes and TensorFlow Serving',\n",
       "    'question': 'How to install Tensorflow in Ubuntu WSL2'},\n",
       "   {'text': 'If you are running tensorflow on your own machine and you start getting the following errors:\\nAllocator (GPU_0_bfc) ran out of memory trying to allocate 6.88GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\\nTry adding this code in a cell at the beginning of your notebook:\\nconfig = tf.compat.v1.ConfigProto()\\nconfig.gpu_options.allow_growth = True\\nsession = tf.compat.v1.Session(config=config)\\nAfter doing this most of my issues went away. I say most because there was one instance when I still got the error once more, but only during one epoch. I ran the code again, right after it finished, and I never saw the error again.\\nAdded by Martin Uribe',\n",
       "    'section': '10. Kubernetes and TensorFlow Serving',\n",
       "    'question': 'Getting: Allocator ran out of memory errors?'},\n",
       "   {'text': 'In session 10.3, when creating the virtual environment with pipenv and trying to run the script gateway.py, you might get this error:\\nTypeError: Descriptors cannot not be created directly.\\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\\n1. Downgrade the protobuf package to 3.20.x or lower.\\n2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates\\nThis will happen if your version of protobuf is one of the newer ones. As a workaround, you can fix the protobuf version to an older one. In my case I got around the issue by creating the environment with:\\npipenv install --python 3.9.13 requests grpcio==1.42.0 flask gunicorn \\\\\\nkeras-image-helper tensorflow-protobuf==2.7.0 protobuf==3.19.6\\nAdded by Ángel de Vicente',\n",
       "    'section': '10. Kubernetes and TensorFlow Serving',\n",
       "    'question': 'Problem with recent version of protobuf'},\n",
       "   {'text': 'Due to the uncertainties associated with machines, sometimes you can get the error message like this when you try to run a docker command:\\n”Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?”\\nSolution: The solution is simple. The Docker Desktop might no longer be connecting to the WSL Linux distro. What you need to do is go to your Docker Desktop setting and then click on resources. Under resources, click on WSL Integration. You will get a tab like the image below:\\nJust enable additional distros. That’s all. Even if the additional distro is the same as the default WSL distro.\\nOdimegwu David',\n",
       "    'section': '10. Kubernetes and TensorFlow Serving',\n",
       "    'question': 'WSL Cannot Connect To Docker Daemon'},\n",
       "   {'text': 'In case the HPA instance does not run correctly even after installing the latest version of Metrics Server from the components.yaml manifest with:\\n>>kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\\nAnd the targets still appear as <unknown>\\nRun >>kubectl edit deploy -n kube-system metrics-server\\nAnd search for this line:\\nargs:\\n- --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname\\nAdd this line in the middle:  - --kubelet-insecure-tls\\nSo that it stays like this:\\nargs:\\n- --kubelet-insecure-tls\\n- --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname\\nSave and run again >>kubectl get hpa\\nAdded by Marilina Orihuela',\n",
       "    'section': '10. Kubernetes and TensorFlow Serving',\n",
       "    'question': 'HPA instance doesn’t run properly'},\n",
       "   {'text': 'In case the HPA instance does not run correctly even after installing the latest version of Metrics Server from the components.yaml manifest with:\\n>>kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\\nAnd the targets still appear as <unknown>\\nRun the following command:\\nkubectl apply -f https://raw.githubusercontent.com/Peco602/ml-zoomcamp/main/10-kubernetes/kube-config/metrics-server-deployment.yaml\\nWhich uses a metrics server deployment file already embedding the - --kubelet-insecure-tls option.\\nAdded by Giovanni Pecoraro',\n",
       "    'section': '10. Kubernetes and TensorFlow Serving',\n",
       "    'question': 'HPA instance doesn’t run properly (easier solution)'},\n",
       "   {'text': \"When I run pip install grpcio==1.42.0 tensorflow-serving-api==2.7.0 to install the libraries in windows machine,  I was getting the below error :\\nERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\\\\\Users\\\\\\\\Asia\\\\\\\\anaconda3\\\\\\\\Lib\\\\\\\\site-packages\\\\\\\\google\\\\\\\\protobuf\\\\\\\\internal\\\\\\\\_api_implementation.cp39-win_amd64.pyd'\\nConsider using the `--user` option or check the permissions.\\nSolution description :\\nI was able to install the libraries using below command:\\npip --user install grpcio==1.42.0 tensorflow-serving-api==2.7.0\\nAsia Saeed\",\n",
       "    'section': '10. Kubernetes and TensorFlow Serving',\n",
       "    'question': 'Could not install packages due to an OSError: [WinError 5] Access is denied'},\n",
       "   {'text': 'Problem description\\nI was getting the below error message when I run gateway.py after modifying the code & creating virtual environment in  video 10.3 :\\nFile \"C:\\\\Users\\\\Asia\\\\Data_Science_Code\\\\Zoompcamp\\\\Kubernetes\\\\gat.py\", line 9, in <module>\\nfrom tensorflow_serving.apis import predict_pb2\\nFile \"C:\\\\Users\\\\Asia\\\\.virtualenvs\\\\Kubernetes-Ge6Ts1D5\\\\lib\\\\site-packages\\\\tensorflow_serving\\\\apis\\\\predict_pb2.py\", line 14, in <module>\\nfrom tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2\\nFile \"C:\\\\Users\\\\Asia\\\\.virtualenvs\\\\Kubernetes-Ge6Ts1D5\\\\lib\\\\site-packages\\\\tensorflow\\\\core\\\\framework\\\\tensor_pb2.py\", line 14, in <module>\\nfrom tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2\\nFile \"C:\\\\Users\\\\Asia\\\\.virtualenvs\\\\Kubernetes-Ge6Ts1D5\\\\lib\\\\site-packages\\\\tensorflow\\\\core\\\\framework\\\\resource_handle_pb2.py\", line 14, in <module>\\nfrom tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\\nFile \"C:\\\\Users\\\\Asia\\\\.virtualenvs\\\\Kubernetes-Ge6Ts1D5\\\\lib\\\\site-packages\\\\tensorflow\\\\core\\\\framework\\\\tensor_shape_pb2.py\", line 36, in <module>\\n_descriptor.FieldDescriptor(\\nFile \"C:\\\\Users\\\\Asia\\\\.virtualenvs\\\\Kubernetes-Ge6Ts1D5\\\\lib\\\\site-packages\\\\google\\\\protobuf\\\\descriptor.py\", line 560, in __new__\\n_message.Message._CheckCalledFromGeneratedFile()\\nTypeError: Descriptors cannot not be created directly.\\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\\n1. Downgrade the protobuf package to 3.20.x or lower.\\n2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\\nSolution description:\\nIssue has been resolved by downgrading protobuf to version 3.20.1.\\npipenv install protobuf==3.20.1\\nAsia Saeed',\n",
       "    'section': '10. Kubernetes and TensorFlow Serving',\n",
       "    'question': 'TypeError: Descriptors cannot not be created directly.'},\n",
       "   {'text': 'To install kubectl on windows using the terminal in vscode (powershell), I followed this tutorial: https://medium.com/@ggauravsigra/install-kubectl-on-windows-af77da2e6fff\\nI first downloaded kubectl with curl, with these command lines: https://kubernetes.io/docs/tasks/tools/install-kubectl-windows/#install-kubectl-binary-with-curl-on-windows\\nAt step 3, I followed the tutorial with the copy of the exe file in a specific folder on C drive.\\nThen I added this folder path to PATH in my environment variables.\\nKind can be installed the same way with the curl command on windows, by specifying a folder that will be added to the path environment variable.\\nAdded by Mélanie Fouesnard',\n",
       "    'section': '10. Kubernetes and TensorFlow Serving',\n",
       "    'question': 'How to install easily kubectl on windows ?'},\n",
       "   {'text': \"First you need to launch a powershell terminal with administrator privilege.\\nFor this we need to install choco library first through the following syntax in powershell:\\nSet-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://chocolatey.org/install.ps1'))\\nKrishna Anand\",\n",
       "    'section': '10. Kubernetes and TensorFlow Serving',\n",
       "    'question': 'Install kind through choco library'},\n",
       "   {'text': 'If you are having challenges installing Kind through the Windows Powershell as provided on the website and Choco Library as I did, you can simply install Kind through Go.\\n> Download and Install Go (https://go.dev/doc/install)\\n> Confirm installation by typing the following in Command Prompt -  go version\\n> Proceed by installing Kind by following this command - go install sigs.k8s.io/kind@v0.20.0\\n>Confirm Installation kind --version\\nIt works perfectly.',\n",
       "    'section': '10. Kubernetes and TensorFlow Serving',\n",
       "    'question': 'Install Kind via Go package'},\n",
       "   {'text': \"I ran into an issue where kubectl wasn't working.\\nI kept getting the following error:\\nkubectl get service\\nThe connection to the server localhost:8080 was refused - did you specify the right host or port?\\nI searched online for a resolution, but everyone kept talking about creating an environment variable and creating some admin.config file in my home directory.\\nAll hogwash.\\nThe solution to my problem was to just start over.\\nkind delete cluster\\nrm -rf ~/.kube\\nkind create cluster\\nNow when I try the same command again:\\nkubectl get service\\nNAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\\nkubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   53s\\nAdded by Martin Uribe\",\n",
       "    'section': '10. Kubernetes and TensorFlow Serving',\n",
       "    'question': 'The connection to the server localhost:8080 was refused - did you specify the right host or port?'},\n",
       "   {'text': 'Problem description\\nDue to experimenting back and forth so much without care for storage, I just ran out of it on my 30-GB AWS instance.\\nMy first reflex was to remove some zoomcamp directories, but of course those are mostly code so it didn’t help much.\\nSolution description\\n> docker images\\nrevealed that I had over 20 GBs worth of superseded / duplicate models lying around, so I proceeded to > docker rmi\\na bunch of those — but to no avail!\\nIt turns out that deleting docker images does not actually free up any space as you might expect. After removing images, you also need to run\\n> docker system prune\\nSee also: https://stackoverflow.com/questions/36799718/why-removing-docker-containers-and-images-does-not-free-up-storage-space-on-wind\\nAdded by Konrad Mühlberg',\n",
       "    'section': '10. Kubernetes and TensorFlow Serving',\n",
       "    'question': 'Running out of storage after building many docker images'},\n",
       "   {'text': 'Yes, the question does require for you to specify values for CPU and memory in the yaml file, however the question that it is use in the form only refers to the port which do have a define correct value for this specific homework.\\nPastor Soto',\n",
       "    'section': '10. Kubernetes and TensorFlow Serving',\n",
       "    'question': 'In HW10 Q6 what does it mean “correct value for CPU and memory”? Aren’t they arbitrary?'},\n",
       "   {'text': 'In Kubernetes resource specifications, such as CPU requests and limits, the \"m\" stands for milliCPU, which is a unit of computing power. It represents one thousandth of a CPU core.\\ncpu: \"100m\" means the container is requesting 100 milliCPUs, which is equivalent to 0.1 CPU core.\\ncpu: \"500m\" means the container has a CPU limit of 500 milliCPUs, which is equivalent to 0.5 CPU core.\\nThese values are specified in milliCPUs to allow fine-grained control over CPU resources. It allows you to express CPU requirements and limits in a more granular way, especially in scenarios where your application might not need a full CPU core.\\nAdded by Andrii Larkin',\n",
       "    'section': '10. Kubernetes and TensorFlow Serving',\n",
       "    'question': 'Why cpu vals for Kubernetes deployment.yaml look like “100m” and “500m”? What does \"m\" mean?'},\n",
       "   {'text': 'Problem: Failing to load docker-image to cluster (when you’ved named a cluster)\\nkind load docker-image zoomcamp-10-model:xception-v4-001\\nERROR: no nodes found for cluster \"kind\"\\nSolution: Specify cluster name with -n\\nkind -n clothing-model load docker-image zoomcamp-10-model:xception-v4-001\\nAndrew Katoch',\n",
       "    'section': '10. Kubernetes and TensorFlow Serving',\n",
       "    'question': 'Kind cannot load docker image'},\n",
       "   {'text': \"Problem: I download kind from the next command:\\ncurl.exe -Lo kind-windows-amd64.exe https://kind.sigs.k8s.io/dl/v0.17.0/kind-windows-amd64\\nWhen I try\\nkind --version\\nI get: 'kind' is not recognized as an internal or external command, operable program or batch file\\nSolution: The default name of executable is kind-windows-amd64.exe, so that you have to rename this file to  kind.exe. Put this file in specific folder, and add it to PATH\\nAlejandro Aponte\",\n",
       "    'section': '10. Kubernetes and TensorFlow Serving',\n",
       "    'question': \"'kind' is not recognized as an internal or external command, operable program or batch file. (In Windows)\"},\n",
       "   {'text': 'Using kind with Rootless Docker or Rootless Podman requires some changes on the system (Linux), see kind – Rootless (k8s.io).\\nSylvia Schmitt',\n",
       "    'section': '10. Kubernetes and TensorFlow Serving',\n",
       "    'question': 'Running kind on Linux with Rootless Docker or Rootless Podman'},\n",
       "   {'text': 'Deploy and Access the Kubernetes Dashboard\\nLuke',\n",
       "    'section': '10. Kubernetes and TensorFlow Serving',\n",
       "    'question': 'Kubernetes-dashboard'},\n",
       "   {'text': 'Make sure you are on AWS CLI v2 (check with aws --version)\\nhttps://docs.aws.amazon.com/cli/latest/userguide/cliv2-migration-instructions.html',\n",
       "    'section': '10. Kubernetes and TensorFlow Serving',\n",
       "    'question': 'Correct AWS CLI version for eksctl'},\n",
       "   {'text': 'Problem Description:\\nIn video 10.3, when I was testing a flask service, I got the above error. I ran docker run .. in one terminal. When in second terminal I run python gateway.py, I get the above error.\\nSolution: This error has something to do with versions of Flask and Werkzeug. I got the same error, if I just import flask with from flask import Flask.\\nBy running pip freeze > requirements.txt,I found that their versions are Flask==2.2.2 and Werkzeug==2.2.2. This error appears while using an old version of werkzeug (2.2.2) with new version of flask (2.2.2). I solved it by pinning version of Flask into an older version with pipenv install Flask==2.1.3.\\nAdded by Bhaskar Sarma',\n",
       "    'section': '10. Kubernetes and TensorFlow Serving',\n",
       "    'question': \"TypeError: __init__() got an unexpected keyword argument 'unbound_message' while importing Flask\"},\n",
       "   {'text': 'As per AWS documentation:\\nhttps://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-push-ecr-image.html\\nYou need to do: (change the fields in red)\\naws ecr get-login-password --region region | docker login --username AWS --password-stdin aws_account_id.dkr.ecr.region.amazonaws.com\\nAlternatively you can run the following command without changing anything given you have a default region configured\\naws ecr get-login-password --region $(aws configure get region) | docker login --username AWS --password-stdin \"$(aws sts get-caller-identity --query \"Account\" --output text).dkr.ecr.$(aws configure get region).amazonaws.com\"\\nAdded by Humberto Rodriguez',\n",
       "    'section': '10. Kubernetes and TensorFlow Serving',\n",
       "    'question': 'Command aws ecr get-login --no-include-email returns “aws: error: argument operation: Invalid choice…”'},\n",
       "   {'text': 'While trying to run the docker code on M1:\\ndocker run --platform linux/amd64 -it --rm \\\\\\n-p 8500:8500 \\\\\\n-v $(pwd)/clothing-model:/models/clothing-model/1 \\\\\\n-e MODEL_NAME=\"clothing-model\" \\\\\\ntensorflow/serving:2.7.0\\nIt outputs the error:\\nError:\\nStatus: Downloaded newer image for tensorflow/serving:2.7.0\\n[libprotobuf FATAL external/com_google_protobuf/src/google/protobuf/generated_message_reflection.cc:2345] CHECK failed: file != nullptr:\\nterminate called after throwing an instance of \\'google::protobuf::FatalException\\'\\nwhat():  CHECK failed: file != nullptr:\\nqemu: uncaught target signal 6 (Aborted) - core dumped\\n/usr/bin/tf_serving_entrypoint.sh: line 3:     8 Aborted                 tensorflow_model_server --port=8500 --rest_api_port=8501 --model_name=${MODEL_NAME} --model_base_path=${MODEL_BASE_PATH}/${MODEL_NAME} \"$@\"\\nSolution\\ndocker pull emacski/tensorflow-serving:latest\\ndocker run -it --rm \\\\\\n-p 8500:8500 \\\\\\n-v $(pwd)/clothing-model:/models/clothing-model/1 \\\\\\n-e MODEL_NAME=\"clothing-model\" \\\\\\nemacski/tensorflow-serving:latest-linux_arm64\\nSee more here: https://github.com/emacski/tensorflow-serving-arm\\nAdded by Daniel Egbo',\n",
       "    'section': '10. Kubernetes and TensorFlow Serving',\n",
       "    'question': 'Error downloading  tensorflow/serving:2.7.0 on Apple M1 Mac'},\n",
       "   {'text': 'Similar to the one above but with a different solution the main reason is that emacski doesn’t seem to maintain the repo any more, the latest image is from 2 years ago at the time of writing (December 2023)\\nProblem:\\nWhile trying to run the docker code on Mac M2 apple silicon:\\ndocker run --platform linux/amd64 -it --rm \\\\\\n-p 8500:8500 \\\\\\n-v $(pwd)/clothing-model:/models/clothing-model/1 \\\\\\n-e MODEL_NAME=\"clothing-model\" \\\\\\ntensorflow/serving\\nYou get an error:\\n/usr/bin/tf_serving_entrypoint.sh: line 3:     7 Illegal instruction     tensorflow_model_server --port=8500 --rest_api_port=8501 --model_name=${MODEL_NAME} --model_base_path=${MODEL_BASE_PATH}/${MODEL_NAME} \"$@\"\\nSolution:\\nUse bitnami/tensorflow-serving base image\\nLaunch it either using docker run\\ndocker run -d \\\\\\n--name tf_serving \\\\\\n-p 8500:8500 \\\\\\n-p 8501:8501 \\\\\\n-v $(pwd)/clothing-model:/bitnami/model-data/1 \\\\\\n-e TENSORFLOW_SERVING_MODEL_NAME=clothing-model \\\\\\nbitnami/tensorflow-serving:2\\nOr the following docker-compose.yaml\\nversion: \\'3\\'\\nservices:\\ntf_serving:\\nimage: bitnami/tensorflow-serving:2\\nvolumes:\\n- ${PWD}/clothing-model:/bitnami/model-data/1\\nports:\\n- 8500:8500\\n- 8501:8501\\nenvironment:\\n- TENSORFLOW_SERVING_MODEL_NAME=clothing-model\\nAnd run it with\\ndocker compose up\\nAdded by Alex Litvinov',\n",
       "    'section': '10. Kubernetes and TensorFlow Serving',\n",
       "    'question': 'Illegal instruction error when running tensorflow/serving image on Mac M2 Apple Silicon (potentially on M1 as well)'},\n",
       "   {'text': 'Problem: CPU metrics Shows Unknown\\nNAME         REFERENCE           TARGETS         MINPODS   MAXPODS   REPLICAS   AGE\\ncredit-hpa   Deployment/credit   <unknown>/20%   1         3         1          18s\\nFailedGetResourceMetric       2m15s (x169 over 44m)  horizontal-pod-autoscaler  failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API:\\nSolution:\\n-> Delete HPA (kubectl delete hpa credit-hpa)\\n-> kubectl apply -f https://raw.githubusercontent.com/pythianarora/total-practice/master/sample-kubernetes-code/metrics-server.yaml\\n-> Create HPA\\nThis should solve the cpu metrics report issue.\\nAdded by Priya V',\n",
       "    'section': '11. KServe',\n",
       "    'question': 'HPA doesn’t show CPU metrics'},\n",
       "   {'text': 'Problem description:\\nRunning this:\\ncurl -s \"https://raw.githubusercontent.com/kserve/kserve/release-0.9/hack/quick_install.sh\" | bash\\nFails with errors because of istio failing to update resources, and you are on kubectl > 1.25.0.\\nCheck kubectl version with kubectl version\\nSolution description\\nEdit the file “quick_install.bash” by downloading it with curl without running bash. Edit the versions of Istio and Knative as per the matrix on the KServe website.\\nRun the bash script now.\\nAdded by Andrew Katoch',\n",
       "    'section': '11. KServe',\n",
       "    'question': 'Errors with istio during installation'},\n",
       "   {'text': 'Problem description\\nSolution description\\n(optional) Added by Name',\n",
       "    'section': 'Projects (Midterm and Capstone)',\n",
       "    'question': 'Problem title'},\n",
       "   {'text': 'Answer: You can see them here (it’s taken from the 2022 cohort page). Go to the cohort folder for your own cohort’s deadline.',\n",
       "    'section': 'Projects (Midterm and Capstone)',\n",
       "    'question': 'What are the project deadlines?'},\n",
       "   {'text': 'Answer: All midterms and capstones are meant to be solo projects. [source @Alexey]',\n",
       "    'section': 'Projects (Midterm and Capstone)',\n",
       "    'question': 'Are projects solo or collaborative/group work?'},\n",
       "   {'text': 'Answer: Ideally midterms up to module-06, capstones include all modules in that cohort’s syllabus. But you can include anything extra that you want to feature. Just be sure to document anything not covered in class.\\nAlso watch office hours from previous cohorts. Go to DTC youtube channel and click on Playlists and search for {course yyyy}. ML Zoomcamp was first launched in 2021.\\nMore discussions:\\n[source1] [source2] [source3]',\n",
       "    'section': 'Projects (Midterm and Capstone)',\n",
       "    'question': 'What modules, topics, problem-sets should a midterm/capstone project cover? Can I do xyz?'},\n",
       "   {'text': \"These links apply to all projects, actually. Again, for some cohorts, the modules/syllabus might be different, so always check in your cohort’s folder as well for additional or different instructions, if any.\\nMidterm Project Sample: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp/cohorts/2021/07-midterm-project\\nMidTerm Project Deliverables: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp/projects\\nSubmit MidTerm Project: https://docs.google.com/forms/d/e/1FAIpQLSfgmOk0QrmHu5t0H6Ri1Wy_FDVS8I_nr5lY3sufkgk18I6S5A/viewform\\nDatasets:\\nhttps://www.kaggle.com/datasets and https://www.kaggle.com/competitions\\nhttps://archive.ics.uci.edu/ml/index.php\\nhttps://data.europa.eu/en\\nhttps://www.openml.org/search?type=data\\nhttps://newzealand.ai/public-data-sets\\nhttps://datasetsearch.research.google.com\\nWhat to do and Deliverables\\nThink of a problem that's interesting for you and find a dataset for that\\nDescribe this problem and explain how a model could be used\\nPrepare the data and doing EDA, analyze important features\\nTrain multiple models, tune their performance and select the best model\\nExport the notebook into a script\\nPut your model into a web service and deploy it locally with Docker\\nBonus points for deploying the service to the cloud\",\n",
       "    'section': 'Projects (Midterm and Capstone)',\n",
       "    'question': 'Crucial Links'},\n",
       "   {'text': 'Answer: Previous cohorts projects page has instructions (youtube).\\nhttps://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2022/projects.md#midterm-project\\nAlexey and his team will compile a g-sheet with links to submitted projects with our hashed emails (just like when we check leaderboard for homework) that are ours to review within the evaluation deadline.\\n~~~ Added by Nukta Bhatia ~~~',\n",
       "    'section': 'Projects (Midterm and Capstone)',\n",
       "    'question': 'How to conduct peer reviews for projects?'},\n",
       "   {'text': 'See the answer here.',\n",
       "    'section': 'Projects (Midterm and Capstone)',\n",
       "    'question': 'Computing the hash for project review'},\n",
       "   {'text': 'For the learning in public for this midterm project it seems that has a total value of 14!, Does this mean that we need make 14 posts?, Or the regular seven posts for each module and each one with a value of 2?, Or just one with a total value of 14?\\n14 posts, one for each day',\n",
       "    'section': 'Projects (Midterm and Capstone)',\n",
       "    'question': 'Learning in public links for the projects'},\n",
       "   {'text': 'You can use git-lfs (https://git-lfs.com/) for upload large file to github repository.\\nRyan Pramana',\n",
       "    'section': 'Projects (Midterm and Capstone)',\n",
       "    'question': \"My dataset is too large and I can't loaded in GitHub , do anyone knows about a solution?\"},\n",
       "   {'text': 'If you have submitted two projects (and peer-reviewed at least 3 course-mates’ projects for each submission), you will get the certificate for the course. According to the course coordinator, Alexey Grigorev, only two projects are needed to get the course certificate.\\n(optional) David Odimegwu',\n",
       "    'section': 'Projects (Midterm and Capstone)',\n",
       "    'question': 'What If I submitted only two projects and failed to submit the third?'},\n",
       "   {'text': 'Yes. You only need to review peers when you submit your project.\\nConfirmed on Slack by Alexey Grigorev (added by Rileen Sinha)',\n",
       "    'section': 'Projects (Midterm and Capstone)',\n",
       "    'question': \"I did the first two projects and skipped the last one so I wouldn't have two peer review in second capstone right?\"},\n",
       "   {'text': 'Regarding Point 4 in the midterm deliverables, which states, \"Train multiple models, tune their performance, and select the best model,\" you might wonder, how many models should you train? The answer is simple: train as many as you can. The term \"multiple\" implies having more than one model, so as long as you have more than one, you\\'re on the right track.',\n",
       "    'section': 'Projects (Midterm and Capstone)',\n",
       "    'question': 'How many models should I train?'},\n",
       "   {'text': 'I am not sure how the project evaluate assignment works? Where do I find this? I have access to all the capstone 2 project, perhaps, I can randomly pick any to review.\\nAnswer:\\nThe link provided for example (2023/Capstone link ): https://docs.google.com/forms/d/e/1FAIpQLSdgoepohpgbM4MWTAHWuXa6r3NXKnxKcg4NDOm0bElAdXdnnA/viewform contains a list of all submitted projects to be evaluated. More specific, you are to review 3 assigned peer projects. In the spreadsheet are 3 hash values of your assigned peer projects. However, you need to derive the your hash value of your email address and find the value on the spreadsheet under the (reviewer_hash) heading.\\nTo calculate your hash value run the python code below:\\nfrom hashlib import sha1\\ndef compute_hash(email):\\nreturn sha1(email.lower().encode(\\'utf-8\\')).hexdigest()\\n# Example usage **** enter your email below (Example1@gmail.com)****\\nemail = \"Example1@gmail.com\"\\nhashed_email = compute_hash(email)\\nprint(\"Original Email:\", email)\\nprint(\"Hashed Email (SHA-1):\", hashed_email)\\nEdit the above code to replace Example1@gmail.com as your email address\\nStore and run the above python code from your terminal. See below as the Hashed Email (SHA-1) value\\nYou then go to the link: https://docs.google.com/spreadsheets/d/e/2PACX-1vR-7RRtq7AMx5OzI-tDbkzsbxNLm-NvFOP5OfJmhCek9oYcDx5jzxtZW2ZqWvBqc395UZpHBv1of9R1/pubhtml?gid=876309294&single=true\\nLastly, copy the “Hashed Email (SHA-1): bd9770be022dede87419068aa1acd7a2ab441675” value and search for 3 identical entries. There you should see your peer project to be reviewed.\\nBy Emmanuel Ayeni',\n",
       "    'section': 'Projects (Midterm and Capstone)',\n",
       "    'question': 'How does the project evaluation work for you as a peer reviewer?'},\n",
       "   {'text': 'Alexey Grigorev: “It’s based on all the scores to make sure most of you pass.”                                                   By Annaliese Bronz\\nOther course-related questions that don’t fall into any of the categories above or can apply to more than one category/module',\n",
       "    'section': 'Miscellaneous',\n",
       "    'question': 'Do you pass a project based on the average of everyone else’s scores or based on the total score you earn?'},\n",
       "   {'text': 'Answer: The train.py file will be used by your peers to review your midterm project. It is for them to cross-check that your training process works on someone else’s system. It should also be included in the environment in conda or with pipenv.\\nOdimegwu David',\n",
       "    'section': 'Miscellaneous',\n",
       "    'question': 'Why do I need to provide a train.py file when I already have the notebook.ipynb file?'},\n",
       "   {'text': \"Pip install pillow - install pillow library\\nfrom PIL import Image\\nimg = Image.open('aeroplane.png')\\nFrom numpy import asarray\\nnumdata=asarray(img)\\nKrishna Anand\",\n",
       "    'section': 'Miscellaneous',\n",
       "    'question': 'Loading the Image with PILLOW library and converting to numpy array'},\n",
       "   {'text': \"Ans: train.py has to be a python file. This is because running a python script for training a model is much simpler than running a notebook and that's how training jobs usually look like in real life.\",\n",
       "    'section': 'Miscellaneous',\n",
       "    'question': 'Is a train.py file necessary when you have a train.ipynb file in your midterm project directory?'},\n",
       "   {'text': 'Yes, you can create a mobile app or interface that manages these forms and validations. But you should also perform validations on backend.\\nYou can also check Streamlit: https://github.com/DataTalksClub/project-of-the-week/blob/main/2022-08-14-frontend.md\\nAlejandro Aponte',\n",
       "    'section': 'Miscellaneous',\n",
       "    'question': 'Is there a way to serve up a form for users to enter data for the model to crunch on?'},\n",
       "   {'text': \"Using model.feature_importances_ can gives you an error:\\nAttributeError: 'Booster' object has no attribute 'feature_importances_'\\nAnswer: if you train the model like this: model = xgb.train you should use get_score() instead\\nEkaterina Kutovaia\",\n",
       "    'section': 'Miscellaneous',\n",
       "    'question': 'How to get feature importance for XGboost model'},\n",
       "   {'text': 'In the Elastic Container Service task log, error “[Errno 12] Cannot allocate memory” showed up.\\nJust increase the RAM and CPU in your task definition.\\nHumberto Rodriguez',\n",
       "    'section': 'Miscellaneous',\n",
       "    'question': '[Errno 12] Cannot allocate memory in AWS Elastic Container Service'},\n",
       "   {'text': \"When running a docker container with waitress serving the app.py for making predictions, pickle will throw an error that can't get attribute <name_of_class> on module __main__.\\nThis does not happen when Flask is used directly, i.e. not through waitress.\\nThe problem is that the model uses a custom column transformer class, and when the model was saved, it was saved from the __main__ module (e.g. python train.py). Pickle will reference the class in the global namespace (top-level code): __main__.<custom_class>.\\nWhen using waitress, waitress will load the predict_app module and this will call pickle.load, that will try to find __main__.<custom_class> that does not exist.\\nSolution:\\nPut the class into a separate module and import it in both the script that saves the model (e.g. train.py) and the script that loads the model (e.g. predict.py)\\nNote: If Flask is used (no waitress) in predict.py, and predict.py has the definition of the class, When  it is run: python predict.py, it will work because the class is in the same namespace as the one used when the model was saved (__main__).\\nDetailed info: https://stackoverflow.com/questions/27732354/unable-to-load-files-using-pickle-and-multiple-modules\\nMarcos MJD\",\n",
       "    'section': 'Miscellaneous',\n",
       "    'question': 'Pickle error: can’t get attribute XXX on module __main__'},\n",
       "   {'text': 'There are different techniques, but the most common used are the next:\\nDataset transformation (for example, log transformation)\\nClipping high values\\nDropping these observations\\nAlena Kniazeva',\n",
       "    'section': 'Miscellaneous',\n",
       "    'question': 'How to handle outliers in a dataset?'},\n",
       "   {'text': 'I was getting the below error message when I was trying to create docker image using bentoml\\n[bentoml-cli] `serve` failed: Failed loading Bento from directory /home/bentoml/bento: Failed to import module \"service\": No module named \\'sklearn\\'\\nSolution description\\nThe cause was because , in bentofile.yaml, I wrote sklearn instead of scikit-learn. Issue was fixed after I modified the packages list as below.\\npackages: # Additional pip packages required by the service\\n- xgboost\\n- scikit-learn\\n- pydantic\\nAsia Saeed',\n",
       "    'section': 'Miscellaneous',\n",
       "    'question': 'Failed loading Bento from directory /home/bentoml/bento: Failed to import module \"service\": No module named \\'sklearn\\''},\n",
       "   {'text': \"You might see a long error message with something about sparse matrices, and in the swagger UI, you get a code 500 error with “” (empty string) as output.\\nPotential reason: Setting DictVectorizer or OHE to sparse while training, and then storing this in a pipeline or custom object in the benotml model saving stage in train.py. This means that when the custom object is called in service.py, it will convert each input to a different sized sparse matrix, and this can't be batched due to inconsistent length. In this case, bentoml model signatures should have batchable set to False for production during saving the bentoml mode in train.py.\\n(Memoona Tahira)\",\n",
       "    'section': 'Miscellaneous',\n",
       "    'question': 'BentoML not working with –production flag at any stage: e.g. with bentoml serve and while running the bentoml container'},\n",
       "   {'text': 'Problem description:\\nDo we have to run everything?\\nYou are encouraged, if you can, to run them. As this provides another opportunity to learn from others.\\nNot everyone will be able to run all the files, in particular the neural networks.\\nSolution description:\\nAlternatively, can you see that everything you need to reproduce is there: the dataset is there, the instructions are there, are there any obvious errors and so on.\\nRelated slack conversation here.\\n(Gregory Morris)',\n",
       "    'section': 'Miscellaneous',\n",
       "    'question': 'Reproducibility'},\n",
       "   {'text': \"If your model is too big for github one option is to try and compress the model using joblib. For example joblib.dump(model, model_filename, compress=('zlib', 6) will use zlib to compress the model. Just note this could take a few moments as the model is being compressed.\\nQuinn Avila\",\n",
       "    'section': 'Miscellaneous',\n",
       "    'question': 'Model too big'},\n",
       "   {'text': \"When you try to push the docker image to Google Container Registry and get this message “unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials.”, type this below on console, but first install https://cloud.google.com/sdk/docs/install, this is to be able to use gcloud in console:\\ngcloud auth configure-docker\\n(Jesus Acuña)\",\n",
       "    'section': 'Miscellaneous',\n",
       "    'question': 'Permissions to push docker to Google Container Registry'},\n",
       "   {'text': 'I am getting this error message when I tried to install tflite in a pipenv environment\\nError:  An error occurred while installing tflite_runtime!\\nError text:\\nERROR: Could not find a version that satisfies the requirement tflite_runtime (from versions: none)\\nERROR: No matching distribution found for tflite_runtime\\nThis version of tflite do not run on python 3.10, the way we can make it work is by install python 3.9, after that it would install the tflite_runtime without problem.\\nPastor Soto\\nCheck all available versions here:\\nhttps://google-coral.github.io/py-repo/tflite-runtime/\\nIf you don’t find a combination matching your setup, try out the options at\\nhttps://github.com/alexeygrigorev/tflite-aws-lambda/tree/main/tflite\\nwhich you can install as shown in the lecture, e.g.\\npip install https://github.com/alexeygrigorev/tflite-aws-lambda/raw/main/tflite/tflite_runtime-2.7.0-cp38-cp38-linux_x86_64.whl\\nFinally, if nothing works, use the TFLite included in TensorFlow for local development, and use Docker for testing Lambda.\\nRileen Sinha (based on discussions on Slack)',\n",
       "    'section': 'Miscellaneous',\n",
       "    'question': 'Tflite_runtime unable to install'},\n",
       "   {'text': \"Error: ImageDataGenerator name 'scipy' is not defined.\\nCheck that scipy is installed in your environment.\\nRestart jupyter kernel and try again.\\nMarcos MJD\",\n",
       "    'section': 'Miscellaneous',\n",
       "    'question': 'Error when running ImageDataGenerator.flow_from_dataframe'},\n",
       "   {'text': 'Tim from BentoML has prepared a dedicated video tutorial wrt this use case here:\\nhttps://www.youtube.com/watch?v=7gI1UH31xb4&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=97\\nKonrad Muehlberg',\n",
       "    'section': 'Miscellaneous',\n",
       "    'question': 'How to pass BentoML content / docker container to Amazon Lambda'},\n",
       "   {'text': \"In deploying model part, I wanted to test my model locally on a test-image data and I had this silly error after the following command:\\nurl = 'https://github.com/bhasarma/kitchenware-classification-project/blob/main/test-image.jpg'\\nX = preprocessor.from_url(url)\\nI got the error:\\nUnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x7f797010a590>\\nSolution:\\nAdd ?raw=true after .jpg in url. E.g. as below\\nurl = ‘https://github.com/bhasarma/kitchenware-classification-project/blob/main/test-image.jpg?raw=true’\\nBhaskar Sarma\",\n",
       "    'section': 'Miscellaneous',\n",
       "    'question': 'Error UnidentifiedImageError: cannot identify image file'},\n",
       "   {'text': 'Problem: If you run pipenv install and get this message. Maybe manually change Pipfile and Pipfile.lock.\\nSolution: Run: ` pipenv lock` for fix this problem and dependency files\\nAlejandro Aponte',\n",
       "    'section': 'Miscellaneous',\n",
       "    'question': '[pipenv.exceptions.ResolutionFailure]: Warning: Your dependencies could not be resolved. You likely have a mismatch in your sub-dependencies'},\n",
       "   {'text': 'Problem: In the course this function worked to get the features from the dictVectorizer instance: dv.get_feature_names(). But in my computer did not work. I think it has to do with library versions and but apparently that function will be deprecated soon:\\nOld: https://scikit-learn.org/0.22/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.get_feature_names\\nNew: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.get_feature_names\\nSolution: change the line dv.get_feature_names() to list(dv.get_feature_names_out))\\nIbai Irastorza',\n",
       "    'section': 'Miscellaneous',\n",
       "    'question': 'Get_feature_names() not found'},\n",
       "   {'text': 'Problem happens when contacting the server waiting to send your predict-test and your data here in the correct shape.\\nThe problem was the format input to the model wasn’t in the right shape. Server receives the data in json format (dict) which is not suitable for the model. U should convert it to like numpy arrays.\\nAhmed Okka',\n",
       "    'section': 'Miscellaneous',\n",
       "    'question': 'Error decoding JSON response: Expecting value: line 1 column 1 (char 0)'},\n",
       "   {'text': \"Q: Hii folks, I tried deploying my docker image on Render, but it won't I get SIGTERM everytime.\\nI think .5GB RAM is not enough, is there any other free alternative available ?\\nA: aws (amazon), gcp (google), saturn.\\nBoth aws and gcp give microinstance for free for a VERY long time, and a bunch more free stuff.\\nSaturn even provides free GPU instances. Recent promo link from mlzoomcamp for Saturn:\\n“You can sign up here: https://bit.ly/saturn-mlzoomcamp\\nWhen you sign up, write in the chat box that you're an ML Zoomcamp student and you should get extra GPU hours (something like 150)”\\nAdded by Andrii Larkin\",\n",
       "    'section': 'Miscellaneous',\n",
       "    'question': 'Free cloud alternatives'},\n",
       "   {'text': \"Problem description: I have one column day_of_the_month . It has values 1, 2, 20, 25 etc. and int . I have a second column month_of_the_year. It has values jan, feb, ..dec. and are string. I want to convert these two columns into one column day_of_the_year and I want them to be int. 2 and jan should give me 2, i.e. 2nd day of the year, 1 and feb should give me 32, i.e. 32 nd day of the year. What is the simplest pandas-way to do it?\\nSolution description:\\nconvert dtype in day_of_the_month column from int to str with df['day_of_the_month'] = df['day_of_the_month'].map(str)\\nconvert month_of_the_year column in jan, feb ...,dec into 1,2, ..,12 string using map()\\nconvert day and month into a datetime object with:\\ndf['date_formatted'] = pd.to_datetime(\\ndict(\\nyear='2055',\\nmonth=df['month'],\\nday=df['day']\\n)\\n)\\nget day of year with: df['day_of_year']=df['date_formatted'].dt.dayofyear\\n(Bhaskar Sarma)\",\n",
       "    'section': 'Miscellaneous',\n",
       "    'question': 'Getting day of the year from day and month column'},\n",
       "   {'text': 'How to visualize the predictions per classes after training a neural net\\nSolution description\\nclasses, predictions = zip(*dict(zip(classes, predictions)).items())\\nplt.figure(figsize=(12, 3))\\nplt.bar(classes, predictions)\\nLuke',\n",
       "    'section': 'Miscellaneous',\n",
       "    'question': 'Chart for classes and predictions'},\n",
       "   {'text': 'You can convert the prediction output values to a datafarme using \\ndf = pd.DataFrame.from_dict(dict, orient=\\'index\\' , columns=[\"Prediction\"])\\nEdidiong Esu',\n",
       "    'section': 'Miscellaneous',\n",
       "    'question': 'Convert dictionary values to Dataframe table'},\n",
       "   {'text': 'The image dataset for the competition was in a different layout from what we used in the dino vs dragon lesson. Since that’s what was covered, some folks were more comfortable with that setup, so I wrote a script that would generate it for them\\nIt can be found here: kitchenware-dataset-generator | Kaggle\\nMartin Uribe',\n",
       "    'section': 'Miscellaneous',\n",
       "    'question': 'Kitchenware Classification Competition Dataset Generator'},\n",
       "   {'text': 'Install Nvidia drivers: https://www.nvidia.com/download/index.aspx.\\nWindows:\\nInstall Anaconda prompt https://www.anaconda.com/\\nTwo options:\\nInstall package ‘tensorflow-gpu’ in Anaconda\\nInstall the Tensorflow way https://www.tensorflow.org/install/pip#windows-native\\nWSL/Linux:\\nWSL: Use the Windows Nvida drivers, do not touch that.\\nTwo options:\\nInstall the Tensorflow way https://www.tensorflow.org/install/pip#linux_1\\nMake sure to follow step 4 to install CUDA by environment\\nAlso run:\\necho ‘export XLA_FLAGS=--xla_gpu_cuda_data_dir=$CONDA_PREFIX/lib/> $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh\\nInstall CUDA toolkit 11.x.x https://developer.nvidia.com/cuda-toolkit-archive\\nInstall https://developer.nvidia.com/rdp/cudnn-download\\nNow you should be able to do training/inference with GPU in Tensorflow\\n(Learning in public links Links to social media posts where you share your progress with others (LinkedIn, Twitter, etc). Use #mlzoomcamp tag. The scores for this part will be capped at 7 points. Please make sure the posts are valid URLs starting with \"https://\" Does it mean that I should provide my linkedin link? or it means that I should write a post that I have completed my first assignement? (\\nANS (by ezehcp7482@gmail.com): Yes, provide the linkedIN link to where you posted.\\nezehcp7482@gmail.com:\\nPROBLEM: Since I had to put up a link to a public repository, I had to use Kaggle and uploading the dataset therein was a bit difficult; but I had to ‘google’ my way out.\\nANS: See this link for a guide (https://www.kaggle.com/code/dansbecker/finding-your-files-in-kaggle-kernels/notebook)',\n",
       "    'section': 'Miscellaneous',\n",
       "    'question': 'CUDA toolkit and cuDNN Install for Tensorflow'},\n",
       "   {'text': 'When multiplying matrices, the order of multiplication is important.\\nFor example:\\nA (m x n) * B (n x p) = C (m x p)\\nB (n x p) * A (m x n) = D (n x n)\\nC and D are matrices of different sizes and usually have different values. Therefore the order is important in matrix multiplication and changing the order changes the result.\\nBaran Akın',\n",
       "    'section': 'Miscellaneous',\n",
       "    'question': 'About getting the wrong result when multiplying matrices'},\n",
       "   {'text': 'Refer to https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/01-intro/06-environment.md\\n(added by Rileen Sinha)',\n",
       "    'section': 'Miscellaneous',\n",
       "    'question': 'None of the videos have how to install the environment in Mac, does someone have instructions for Mac with M1 chip?'},\n",
       "   {'text': \"Depends on whether the form will still be open. If you're lucky and it's open, you can submit your homework and it will be evaluated. if closed - it's too late.\\n(Added by Rileen Sinha, based on answer by Alexey on Slack)\",\n",
       "    'section': 'Miscellaneous',\n",
       "    'question': 'I may end up submitting the assignment late. Would it be evaluated?'},\n",
       "   {'text': 'Yes. Whoever corrects the homework will only be able to access the link if the repository is public.\\n(added by Tano Bugelli)\\nHow to install Conda environment in my local machine?\\nWhich ide is recommended for machine learning?',\n",
       "    'section': 'Miscellaneous',\n",
       "    'question': 'Does the github repository need to be public?'},\n",
       "   {'text': 'Install w get:\\n!which wget\\nDownload data:\\n!wget -P /content/drive/My\\\\ Drive/Downloads/ URL\\n(added by Paulina Hernandez)',\n",
       "    'section': 'Miscellaneous',\n",
       "    'question': 'How to use wget with Google Colab?'},\n",
       "   {'text': \"Features (X) must always be formatted as a 2-D array to be accepted by scikit-learn.\\nUse reshape to reshape a 1D array to a 2D.\\n\\t\\t\\t\\t\\t\\t\\t(-Aileah) :>\\n(added by Tano\\nfiltered_df = df[df['ocean_proximity'].isin(['<1H OCEAN', 'INLAND'])]\\n# Select only the desired columns\\nselected_columns = [\\n'latitude',\\n'longitude',\\n'housing_median_age',\\n'total_rooms',\\n'total_bedrooms',\\n'population',\\n'households',\\n'median_income',\\n'median_house_value'\\n]\\nfiltered_df = filtered_df[selected_columns]\\n# Display the first few rows of the filtered DataFrame\\nprint(filtered_df.head())\",\n",
       "    'section': 'Miscellaneous',\n",
       "    'question': 'Features in scikit-learn?'},\n",
       "   {'text': 'FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead',\n",
       "    'section': 'Miscellaneous',\n",
       "    'question': 'When I plotted using Matplot lib to check if median has a tail, I got the error below how can one bypass?'},\n",
       "   {'text': 'When trying to rerun the docker file in Windows, as opposed to developing in WSL/Linux, I got the error of:\\n```\\nWarning: Python 3.11 was not found on your system…\\nNeither ‘pipenv’ nor ‘asdf’ could be found to install Python.\\nYou can specify specific versions of Python with:\\n$ pipenv –python path\\\\to\\\\python\\n```\\nThe solution was to add Python311 installation folder to the PATH and restart the system and run the docker file again. That solved the error.\\n(Added by Abhijit Chakraborty)',\n",
       "    'section': 'Miscellaneous',\n",
       "    'question': 'Reproducibility in different OS'},\n",
       "   {'text': 'You may quickly deploy your project to DigitalOcean App Cloud. The process is relatively straightforward. The deployment costs about 5 USD/month. The container needs to be up until the end of the project evaluation.\\nSteps:\\nRegister in DigitalOcean\\nGo to Apps -> Create App.\\nYou will need to choose GitHub as a service provider.\\nEdit Source Directory (if your project is not in the repo root)\\nIMPORTANT: Go to settings -> App Spec and edit the Dockerfile path so it looks like ./project/Dockerfile path relative to your repo root\\nRemember to add model files if they are not built automatically during the container build process.\\nBy Dmytro Durach',\n",
       "    'section': 'Miscellaneous',\n",
       "    'question': 'Deploying to Digital Ocean'},\n",
       "   {'text': \"I’m just looking back at the lessons in week 3 (churn prediction project), and lesson 3.6 talks about Feature Importance for categorical values. At 8.12, the mutual info scores show that the some features are more important than others, but then in lesson 3.10 the Logistic Regression model is trained on all of the categorical variables (see 1:35). Once we have done feature importance, is it best to train your model only on the most important features?\\nNot necessarily - rather, any feature that can offer additional predictive value should be included (so, e.g. predict with & without including that feature; if excluding it drops performance, keep it, else drop it). A few individually important features might in fact be highly correlated with others, & dropping some might be fine. There are many feature selection algorithms, it might be interesting to read up on them (among the methods we've learned so far in this course, L1 regularization (Lasso) implicitly does feature selection by shrinking some weights all the way to zero).\\nBy Rileen Sinha\",\n",
       "    'section': 'Miscellaneous',\n",
       "    'question': 'Is it best to train your model only on the most important features?'},\n",
       "   {'text': 'You can consider several different approaches:\\nSampling: In the exploratory phase, you can use random samples of the data.\\nChunking: When you do need all the data, you can read and process it in chunks that do fit in the memory.\\nOptimizing data types: Pandas’ automatic data type inference (when reading data in) might result in e.g. float64 precision being used to represent integers, which wastes space. You might achieve substantial memory reduction by optimizing the data types.\\nUsing Dask, an open-source python project which parallelizes Numpy and Pandas.\\n(see, e.g. https://www.vantage-ai.com/en/blog/4-strategies-how-to-deal-with-large-datasets-in-pandas)\\nBy Rileen Sinha',\n",
       "    'section': 'Miscellaneous',\n",
       "    'question': 'How can I work with very large datasets, e.g. the New York Yellow Taxi dataset, with over a million rows?'},\n",
       "   {'text': 'Technically, yes. Advisable? Not really. Reasons:\\nSome homework(s) asks for specific python library versions.\\nAnswers may not match in MCQ options if using different languages other than Python 3.10 (the recommended version for 2023 cohort)\\nAnd as for midterms/capstones, your peer-reviewers may not know these other languages. Do you want to be penalized for others not knowing these other languages?\\nYou can create a separate repo using course’s lessons but written in other languages for your own learnings, but not advisable for submissions.\\ntx[source]',\n",
       "    'section': 'Miscellaneous',\n",
       "    'question': 'Can I do the course in other languages, like R or Scala?'},\n",
       "   {'text': 'Yes, it’s allowed (as per Alexey).\\nAdded By Rileen Sinha',\n",
       "    'section': 'Miscellaneous',\n",
       "    'question': 'Is use of libraries like fast.ai or huggingface allowed in the capstone and competition, or are they considered to be \"too much help\"?'},\n",
       "   {'text': 'The TF and TF Serving versions have to match (as per solution from the slack channel)\\nAdded by Chiedu Elue',\n",
       "    'section': 'Miscellaneous',\n",
       "    'question': 'Flask image was built and tested successfully, but tensorflow serving image was built and unable to test successfully. What could be the problem?'},\n",
       "   {'text': 'I’ve seen LinkedIn users list DataTalksClub as Experience with titles as:\\nMachine Learning Fellow\\nMachine Learning Student\\nMachine Learning Participant\\nMachine Learning Trainee\\nPlease note it is best advised that you do not list the experience as an official “job” or “internship” experience since DataTalksClub did not hire you, nor financially compensate you.\\nOther ways you can incorporate the experience in the following sections:\\nOrganizations\\nProjects\\nSkills\\nFeatured\\nOriginal posts\\nCertifications\\nCourses\\nBy Annaliese Bronz\\nInteresting question, I put the link of my project into my CV as showcase and make posts to show my progress.\\nBy Ani Mkrtumyan',\n",
       "    'section': 'Miscellaneous',\n",
       "    'question': 'Any advice for adding the Machine Learning Zoomcamp experience to your LinkedIn profile?'}]},\n",
       " {'course': 'mlops-zoomcamp',\n",
       "  'documents': [{'text': 'MLOps Zoomcamp FAQ\\nThe purpose of this document is to capture frequently asked technical questions.\\nWe did this for our data engineering course, and it worked quite well. Check this document for inspiration on how to structure your questions and answers:\\nData Engineering Zoomcamp FAQ\\n[Problem description]\\n[Solution description]\\n(optional) Added by Name',\n",
       "    'section': '+-General course questions',\n",
       "    'question': 'Format for questions: [Problem title]'},\n",
       "   {'text': 'Approximately 3 months. For each module, about 1 week with possible deadline extensions (in total 6~9 weeks), 2 weeks for working on the capstone project and 1 week for peer review.',\n",
       "    'section': '+-General course questions',\n",
       "    'question': 'What is the expected duration of this course or that for each module?'},\n",
       "   {'text': 'The difference is the Orchestration and Monitoring modules. Those videos will be re-recorded. The rest should mostly be the same.\\nAlso all of the homeworks will be changed for the 2023 cohort.',\n",
       "    'section': '+-General course questions',\n",
       "    'question': 'What’s the difference between the 2023 and 2022 course?'},\n",
       "   {'text': 'Yes, it will start in May 2024',\n",
       "    'section': '+-General course questions',\n",
       "    'question': 'Will there be a 2024 Cohort? When will the 2024 cohort start?'},\n",
       "   {'text': 'Please choose the closest one to your answer. Also do not post your answer in the course slack channel.',\n",
       "    'section': '+-General course questions',\n",
       "    'question': 'What if my answer is not exactly the same as the choices presented?'},\n",
       "   {'text': 'Please pick up a problem you want to solve yourself. Potential datasets can be found on either Kaggle, Hugging Face, Google, AWS, or the UCI Machine Learning Datasets Repository.',\n",
       "    'section': '+-General course questions',\n",
       "    'question': 'Are we free to choose our own topics for the final project?'},\n",
       "   {'text': 'In order to obtain the certificate, completion of the final capstone project is mandatory. The completion of weekly homework assignments is optional, but they can contribute to your overall progress and ranking on the top 100 leaderboard.',\n",
       "    'section': '+-General course questions',\n",
       "    'question': 'Can I still graduate when I didn’t complete homework for week x?'},\n",
       "   {'text': 'You can get a few cloud points by using kubernetes even if you deploy it only locally. Or you can use local stack too to mimic AWS\\nAdded by Ming Jun, Asked by Ben Pacheco, Answered by Alexey Grigorev',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'For the final project, is it required to be put on the cloud?'},\n",
       "   {'text': 'For those who are not using VSCode (or other similar IDE), you can automate port-forwarding for Jupyter Notebook by adding the following line of code to your\\n~/.ssh/config file (under the mlops-zoomcamp host):\\nLocalForward 127.0.0.1:8899 127.0.0.1:8899\\nThen you can launch Jupyter Notebook using the following command: jupyter notebook --port=8899 --no-browser and copy paste the notebook URL into your browser.\\nAdded by Vishal',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'Port-forwarding without Visual Studio'},\n",
       "   {'text': 'You can install the Jupyter extension to open notebooks in VSCode.\\nAdded by Khubaib',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'Opening Jupyter in VSCode'},\n",
       "   {'text': 'In case one would like to set a github repository (e.g. for Homeworks), one can follow 2 great tutorials that helped a lot\\nSetting up github on AWS instance - this\\nSetting up keys on AWS instance - this\\nThen, one should be able to push to its repo\\nAdded by Daniel Hen (daniel8hen@gmail.com)',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'Configuring Github to work from the remote VM'},\n",
       "   {'text': \"Faced issue while setting up JUPYTER NOTEBOOK on AWS. I was unable to access it from my desktop. (I am not using visual studio and hence faced problem)\\nRun\\njupyter notebook --generate-config\\nEdit file /home/ubuntu/.jupyter/jupyter_notebook_config.py to add following line:\\nNotebookApp.ip = '*'\\nAdded by Atul Gupta (samatul@gmail.com)\",\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'Opening Jupyter in AWS'},\n",
       "   {'text': 'If you wish to use WSL on your windows machine, here are the setup instructions:\\nCommand: Sudo apt install wget\\nGet Anaconda download address here. wget <download address>\\nTurn on Docker Desktop WFree Download | AnacondaSL2\\nCommand: git clone <github repository address>\\nVSCODE on WSL\\nJupyter: pip3 install jupyter\\nAdded by Gregory Morris (gwm1980@gmail.com)\\nAll in all softwares at one shop:\\nYou can use anaconda which has all built in services like pycharm, jupyter\\nAdded by Khaja Zaffer (khajazaffer@aln.iseg.ulisboa.pt)\\nFor windows “wsl --install” in Powershell\\nAdded by Vadim Surin (vdmsurin@gmai.com)',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'WSL instructions'},\n",
       "   {'text': 'If you create a folder data and download datasets or raw files in your local repository. Then to push all your code to remote repository without this files or folder please use gitignore file. The simple way to create it do the following steps\\n1. Create empty .txt file (using text editor or command line)\\n2. Safe as .gitignore (. must use the dot symbol)\\n3. Add rules\\n *.parquet - to ignore all parquet files\\ndata/ - to ignore all files in folder data\\n\\nFor more pattern read GIT documentation\\nhttps://git-scm.com/docs/gitignore\\nAdded by Olga Rudakova (olgakurgan@gmail.com)',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': '.gitignore how-to'},\n",
       "   {'text': \"Make sure when you stop an EC2 instance that it actually stops (there's a meme about it somewhere). There are green circles (running), orange (stopping), and red (stopped). Always refresh the page to make sure you see the red circle and status of stopped.\\nEven when an EC2 instance is stopped, there WILL be other charges that are incurred (e.g. if you uploaded data to the EC2 instance, this data has to be stored somewhere, usually an EBS volume and this storage incurs a cost).\\nYou can set up billing alerts. (I've never done this, so no advice on how to do this).\\n(Question by: Akshit Miglani (akshit.miglani09@gmail.com) and Answer by Anna Vasylytsya)\",\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'AWS suggestions'},\n",
       "   {'text': 'You can get invitation code by coursera and use it in account to verify it it has different characteristics.\\nI really love it\\nhttps://www.youtube.com/watch?v=h_GdX6KtXjo',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'IBM Cloud an alternative for AWS'},\n",
       "   {'text': \"I am worried about the cost of keeping an AWS instance running during the course.\\nWith the instance specified during working environment setup, if you remember to Stop Instance once you finished your work for the day.  Using that strategy, in a day with about 5 hours of work you will pay around $0.40 USD which will account for $12 USD per month, which seems to be an affordable amount.\\nYou must remember that you would have a different IP public address every time you Restart your instance, and you would need to edit your ssh Config file.  It's worth the time though.\\nAdditionally, AWS enables you to set up an automatic email alert if a predefined budget is exceeded.\\nHere is a tutorial to set this up.\\nAlso, you can estimate the cost yourself, using AWS pricing calculator (to use it you don’t even need to be logged in).\\nAt the time of writing (20.05.2023) t3a.xlarge instance with 2 hr/day usage (which translates to 10 hr/week that should be enough to complete the course) and 30GB EBS monthly cost is 10.14 USD\\nHere’s a link to the estimate\\nAdded by Alex Litvinov (aaalex.lit@gmail.com)\",\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'AWS costs'},\n",
       "   {'text': 'For many parts - yes. Some things like kinesis are not in AWS free tier, but you can do it locally with localstack.',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'Is the AWS free tier enough for doing this course?'},\n",
       "   {'text': 'When I click an open IP-address in an AWS EC2 instance I get an error: “This site can’t be reached”. What should I do?\\nThis ip-address is not required to be open in a browser. It is needed to connect to the running EC2 instance via terminal from your local machine or via terminal from a remote server with such command, for example if:\\nip-address is 11.111.11.111\\ndownloaded key name is razer.pem (the key should be moved to a hidden folder .ssh)\\nyour user name is user_name\\nssh -i /Users/user_name/.ssh/razer.pem ubuntu@11.111.11.111',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'AWS EC2: this site can’t be reached'},\n",
       "   {'text': 'After this command `ssh -i ~/.ssh/razer.pem ubuntu@XX.XX.XX.XX` I got this error: \"unprotected private key file\". This page (https://99robots.com/how-to-fix-permission-error-ssh-amazon-ec2-instance/) explains how to fix this error. Basically you need to change the file permissions of the key file with this command: chmod 400 ~/.ssh/razer.pem',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'Unprotected private key file!'},\n",
       "   {'text': 'My SSH connection to AWS cannot last more than a few minutes, whether via terminal or VS code.\\nMy config:\\n# Copy Configuration in local nano editor, then Save it!\\nHost mlops-zoomcamp                                         # ssh connection calling name\\nUser ubuntu                                             # username AWS EC2\\nHostName <instance-public-IPv4-addr>                    # Public IP, it changes when Source EC2 is turned off.\\nIdentityFile ~/.ssh/name-of-your-private-key-file.pem   # Private SSH key file path\\nLocalForward 8888 localhost:8888                        # Connecting to a service on an internal network from the outside, static forward or set port user forward via on vscode\\nStrictHostKeyChecking no\\nAdded by Muhammed Çelik\\nThe disconnection will occur whether I SSH via WSL2 or via VS Code, and usually occurs after I run some code, i.e. “import mlflow”, so not particularly intense computation.\\nI cannot reconnect to the instance without stopping and restarting with a new IPv4 address.\\nI’ve gone through steps listed on this page: https://aws.amazon.com/premiumsupport/knowledge-center/ec2-linux-resolve-ssh-connection-errors/\\nInbound rule should allow all incoming IPs for SSH.\\nWhat I expect to happen:\\nSSH connection should remain while I’m actively using the instance, and if it does disconnect, I should be able to reconnect back.\\nSolution: sometimes the hang ups are caused by the instance running out of memory. In one instance, using EC2 feature to view screenshot of the instance as a means to troubleshoot, it was the OS out-of-memory feature which killed off some critical processes. In this case, if we can’t use a higher compute VM with more RAM, try adding a swap file, which uses the disk as RAM substitute and prevents the OOM error. Follow Ubuntu’s documentation here: https://help.ubuntu.com/community/SwapFaq.\\nAlternatively follow AWS’s own doc, which mirrors Ubuntu’s: https://aws.amazon.com/premiumsupport/knowledge-center/ec2-memory-swap-file/',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'AWS EC2 instance constantly drops SSH connection'},\n",
       "   {'text': 'Everytime I restart my EC2 instance I keep getting different IP and need to update the config file manually.\\n\\nSolution: You can create a script like this to automatically update the IP address of your EC2 instance.https://github.com/dimzachar/mlops-zoomcamp/blob/master/notes/Week_1/update_ssh_config.md',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'AWS EC2 IP Update'},\n",
       "   {'text': 'Make sure to use an instance with enough compute capabilities such as a t2.xlarge. You can check the monitoring tab in the EC2 dashboard to monitor your instance.',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'VS Code crashes when connecting to Jupyter'},\n",
       "   {'text': 'Error “ValueError: X has 526 features, but LinearRegression is expecting 525 features as input.” when running your Linear Regression Model on the validation data set:\\nSolution: The DictVectorizer creates an initial mapping for the features (columns). When calling the DictVecorizer again for the validation dataset transform should be used as it will ignore features that it did not see when fit_transform was last called. E.g.\\nX_train = dv.fit_transform(train_dict)\\nX_test = dv.transform(test_dict)',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'X has 526 features, but expecting 525 features'},\n",
       "   {'text': 'If some dependencies are missing\\nInstall following packages\\npandas\\nmatplotlib\\nscikit-learn\\nfastparquet\\npyarrow\\nseaborn\\npip install -r requirements.txt\\nI have seen this error when using pandas.read_parquet(), the solution is to install pyarrow or fastparquet by doing !pip install pyarrow in the notebook\\nNOTE: if you’re using Conda instead of pip, install fastparquet rather than pyarrow, as it is much easier to install and it’s functionally identical to pyarrow for our needs.',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'Missing dependencies'},\n",
       "   {'text': 'The evaluation RMSE I get doesn’t figure within the options!\\nIf you’re evaluating the model on the entire February data, try to filter outliers using the same technique you used on the train data (0≤duration≤60) and you’ll get a RMSE which is (approximately) in the options. Also don’t forget to convert the columns data types to str before using the DictVectorizer.\\nAnother option: Along with filtering outliers, additionally filter on null values by replacing them with -1.  You will get a RMSE which is (almost same as) in the options. Use ‘.round(2)’ method to round it to 2 decimal points.\\nWarning deprecation\\nThe python interpreter warning of modules that have been deprecated  and will be removed in future releases as well as making suggestion how to go about your code.\\nFor example\\nC:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\seaborn\\\\distributions.py:2619:\\nFutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\\nwarnings.warn(msg, FutureWarning)\\nTo suppress the warnings, you can include this code at the beginning of your notebook\\nimport warnings\\nwarnings.filterwarnings(\"ignore\")',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'No RMSE value in the options'},\n",
       "   {'text': 'sns.distplot(df_train[\"duration\"])\\nCan be replaced with\\nsns.histplot(\\ndf_train[\"duration\"] , kde=True,\\nstat=\"density\", kde_kws=dict(cut=3), bins=50,\\nalpha=.4, edgecolor=(1, 1, 1, 0.4),\\n)\\nTo get almost identical result',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'How to replace distplot with histplot'},\n",
       "   {'text': 'You need to replace the capital letter “L” with a small one “l”',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': \"KeyError: 'PULocationID'  or  'DOLocationID'\"},\n",
       "   {'text': 'I have faced a problem while reading the large parquet file. I tried some workarounds but they were NOT successful with Jupyter.\\nThe error message is:\\nIndexError: index 311297 is out of bounds for axis 0 with size 131743\\nI solved it by performing the homework directly as a python script.\\nAdded by Ibraheem Taha (ibraheemtaha91@gmail.com)\\nYou can try using the Pyspark library\\nAnswered by kamaldeen (kamaldeen32@gmail.com)',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'Reading large parquet files'},\n",
       "   {'text': 'First remove the outliers (trips with unusual duration) before plotting\\nAdded by Ibraheem Taha (ibraheemtaha91@gmail.com)',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'Distplot takes too long'},\n",
       "   {'text': 'Problem: RMSE on test set was too high when hot encoding the validation set with a previously fitted OneHotEncoder(handle_unknown=’ignore’) on the training set, while DictVectorizer would yield the correct RMSE.\\nIn principle both transformers should behave identically when treating categorical features (at least in this week’s homework where we don’t have sequences of strings in each row):\\nFeatures are put into binary columns encoding their presence (1) or absence (0)\\nUnknown categories are imputed as zeroes in the hot-encoded matrix',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'RMSE on test set too high'},\n",
       "   {'text': 'A: Alexey’s answer https://www.youtube.com/watch?v=8uJ36ZZr_Is&t=13s\\nIn summary,\\npd.get_dummies or OHE can come up with result in different orders and handle missing data differently, so train and val set would have different columns during train and validation\\nDictVectorizer would ignore missing (in train) and new (in val) datasets\\nOther sources:\\nhttps://datascience.stackexchange.com/questions/9443/when-to-use-one-hot-encoding-vs-labelencoder-vs-dictvectorizor\\nhttps://scikit-learn.org/stable/modules/feature_extraction.html\\nhttps://innovation.alteryx.com/encode-smarter/\\n~ ellacharmed',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'Q: Using of OneHotEncoder instead of DictVectorizer'},\n",
       "   {'text': \"Why didn't get_dummies in pandas library or OneHotEncoder in scikit-learn library be used for one-hot encoding? I know OneHotEncoder is the most common and useful. One-hot coding can also be done using the eye or identity components of the NumPy library.\\nM.Sari\\nOneHotEncoder has the option to output a row column tuple matrix. DictVectorizer is a one step method to encode and support row column tuple matrix output.\\nHarinder(sudwalh@gmail.com)\",\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'Q: Why did we not use OneHotEncoder(sklearn) instead of DictVectorizer ?'},\n",
       "   {'text': 'How to check that we removed the outliers?\\nUse the pandas function describe() which can provide a report of the data distribution along with the statistics to describe the data. For example, after clipping the outliers using boolean expression, the min and max can be verified using\\ndf[‘duration’].describe()',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'Clipping outliers'},\n",
       "   {'text': 'pd.get_dummies and DictVectorizer both create a one-hot encoding on string values. Therefore you need to convert the values in PUlocationID and DOlocationID to string.\\nIf you convert the values in PUlocationID and DOlocationID from numeric to string, the NaN values get converted to the string \"nan\".  With DictVectorizer the RMSE is the same whether you use \"nan\" or \"-1\" as string representation for the NaN values. Therefore the representation doesn\\'t have to be \"-1\" specifically, it could also be some other string.',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'Replacing NaNs for pickup location and drop off location with -1 for One-Hot Encoding'},\n",
       "   {'text': 'Problem: My LinearRegression RSME is very close to the answer but not exactly the same. Is this normal?\\nAnswer: No, LinearRegression is an deterministic model, it should always output the same results when given the same inputs.\\nAnswer:\\nCheck if you have treated the outlier properly for both train and validation sets\\nCheck if the one hot encoding has been done properly by looking at the shape of one hot encoded feature matrix. If it shows 2 features, there is wrong with one hot encoding. Hint: the drop off and pick up codes need to be converted to proper data format and then DictVectorizer is fitted.\\nHarshit Lamba (hlamba19@gmail.com)',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'Slightly different RSME'},\n",
       "   {'text': 'Problem: I’m facing an extremely low RMSE score (eg: 4.3451e-6) - what shall I do?\\nAnswer: Recheck your code to see if your model is learning the target prior to making the prediction. If the target variable is passed in as a parameter while fitting the model, chances are the model would score extremely low. However, that’s not what you would want and would much like to have your model predict that. A good way to check that is to make sure your X_train doesn’t contain any part of your y_train. The same stands for validation too.\\nSnehangsu De (desnehangsu@gmail.com)',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'Extremely low RSME'},\n",
       "   {'text': 'Problem: how to enable auto completion in jupyter notebook? Tab doesn’t work for me\\nSolution: !pip install --upgrade jedi==0.17.2\\nChristopher R.J.(romanjaimesc@gmail.com)',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'Enabling Auto-completion in jupyter notebook'},\n",
       "   {'text': \"Problem: While following the steps in the videos you may have problems trying to download with wget the files. Usually it is a 403 error type (Forbidden access).\\nSolution: The links point to files on cloudfront.net, something like this:\\nhttps://d37ci6vzurychx.cloudfront.net/tOSError: Could not open parquet input source '<Buffer>': Invalid: Parquet OSError: Could not open parquet input source '<Buffer>': Invalid: Parquet rip+data/green_tripdata_2021-01.parquet\\nI’m not download the dataset directly, i use dataset URL and run this in the file.\\nUpdate(27-May-2023): Vikram\\nI am able to download the data from the below link. This is from the official  NYC trip record page (https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page). Copy link from page directly as the below url might get changed if the NYC decides to move away from this. Go to the page , right click and use copy link.\\nwget https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2021-01.parquet\\n(Asif)\\nCopy the link address and replace the cloudfront.net part with s3.amazonaws.com/nyc-tlc/, so it looks like this:\\nhttps://s3.amazonaws.com/nyc-tlc/trip+data/green_tripdata_2021-01.parquet\\nMario Tormo (mario@tormo-romero.eu)\\nOSError: Could not open parquet input source '<Buffer>': Invalid: Parquet\",\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'Downloading the data from the NY Taxis datasets gives error : 403 Forbidden'},\n",
       "   {'text': 'Problem: PyCharm (remote) doesn’t see conda execution path. So, I cannot use conda env (which is located on a remote server).\\nSolution: In remote server in command line write “conda activate envname”, after write “which python” - it gives you python execution path. After you can use this path when you will add new interpreter in PyCharm: add local interpreter -> system interpreter -> and put the path with python.\\nSalimov Ilnaz (salimovilnaz777@gmail.com)',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'Using PyCharm & Conda env in remote development'},\n",
       "   {'text': 'Problem: The output of DictVectorizer was taking up too much memory. So much so, that I couldn’t even fit the linear regression model before running out of memory on my 16 GB machine.\\nSolution: In the example for DictVectorizer in the scikit-learn website, they set the parameter “sparse” as False. Although this helps with viewing the results, this results in a lot of memory usage. The solution is to either use “sparse=True” instead, or leave it at the default which is also True.\\nAhmed Fahim (afahim03@yahoo.com)',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'Running out of memory'},\n",
       "   {'text': 'Problem: For me, Installing anaconda didn’t modify the .bashrc profile. That means Anaconda env was not activated even after exiting and relaunching the unix shell.\\nSolution:\\nFor bash : Initiate conda again, which will add entries for anaconda in .bashrc file.\\n$ cd YOUR_PATH_ANACONDA/bin $ ./conda init bash\\nThat will automatically edit your .bashrc.\\nReload:\\n$ source ~/.bashrc\\nAhamed Irshad (daisyfuentesahamed@gmail.com)',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'Activating Anaconda env in .bashrc'},\n",
       "   {'text': 'While working through the HW1, you will realize that the training and the validation data set feature sizes are different. I was trying to figure out why and went down the entire rabbit hole only to see that I wasn’t doing ```transform``` on the premade dictionary vectorizer instead of ```fit_transform```. You already have the dictionary vectorizer made so no need to execute the fit pipeline on the model.\\nSam Lim(changhyeonlim@gmail.com)',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'The feature size is different for training set and validation set'},\n",
       "   {'text': 'I found a good guide how to get acces to your machine again when you removed your public key.\\nUsing the following link you can go to Session Manager and log in to your instance and create public key again. https://repost.aws/knowledge-center/ec2-linux-fix-permission-denied-errors\\nThe main problem for me here was to get my old public key, so for doing this you should run the following command: ssh-keygen -y -f /path_to_key_pair/my-key-pair.pem\\nFor more information: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/describe-keys.html#retrieving-the-public-key\\nHanna Zhukavets (a.zhukovec1901@gmail.com)',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'Permission denied (publickey) Error (when you remove your public key on the AWS machine)'},\n",
       "   {'text': 'Problem: The February dataset has been used as a validation/test dataset and been stripped of the outliers in a similar manner to the train dataset (taking only the rows for the duration between 1 and 60, inclusive). The RMSE obtained afterward is in the thousands.\\nAnswer: The sparsematrix result from DictVectorizer shouldn’t be turned into an ndarray. After removing that part of the code, I ended up receiving a correct result .\\nTahina Mahatoky (tahinadanny@gmail.com)',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'Overfitting: Absurdly high RMSE on the validation dataset'},\n",
       "   {'text': 'more specific error line:\\nfrom sklearn.feature_extraction import DictVectorizer\\nI had this issue and to solve it I did\\n!pip install scikit-learn\\nJoel Auccapuclla (auccapuclla 2013@gmail.com)',\n",
       "    'section': 'Module 2: Experiment tracking',\n",
       "    'question': 'Can’t import sklearn'},\n",
       "   {'text': 'Problem: Localhost:5000 Unavailable // Access to Localhost Denied // You don’t have authorization to view this page (127.0.0.1:5000)\\n\\nSolution: If you are on an chrome browser you need to head to `chrome://net-internals/#sockets` and press “Flush Socket Pools”',\n",
       "    'section': 'Module 2: Experiment tracking',\n",
       "    'question': 'Access Denied at Localhost:5000 - Authorization Issue'},\n",
       "   {'text': \"You have something running on the 5000 port. You need to stop it.\\nAnswer: On terminal in mac .\\nRun ps -A | grep gunicorn\\nLook for the number process id which is the 1st number after running the command\\nkill 13580\\nwhere 13580  represents the process number.\\nSource\\nwarrie.warrieus@gmail.com\\nOr by executing the following command it will kill all the processes using port 5000:\\n>> sudo fuser -k 5000/tcp\\nAnswered by Vaibhav Khandelwal\\nJust execute in the command below in he command line to kill the running port\\n->> kill -9 $(ps -A | grep python | awk '{print $1}')\\nAnswered by kamaldeen (kamaldeen32@gmail.com)\\nChange to different port (5001 in this case)\\n>> mlflow ui --backend-store-uri sqlite:///mlflow.db --port 5001\\nAnswered by krishna (nellaikrishna@gmail.com)\",\n",
       "    'section': 'Module 2: Experiment tracking',\n",
       "    'question': \"Connection in use: ('127.0.0.1', 5000)\"},\n",
       "   {'text': 'Running python register_model.py results in the following error:\\nValueError: could not convert string to float: \\'0 int\\\\n1   float\\\\n2     hyperopt_param\\\\n3       Literal{n_estimators}\\\\n4       quniform\\\\n5         Literal{10}\\\\n6         Literal{50}\\\\n7         Literal{1}\\'\\nFull Traceback:\\nTraceback (most recent call last):\\nFile \"/Users/name/Desktop/Programming/DataTalksClub/MLOps-Zoomcamp/2. Experiment tracking and model management/homework/scripts/register_model.py\", line 101, in <module>\\nrun(args.data_path, args.top_n)\\nFile \"/Users/name/Desktop/Programming/DataTalksClub/MLOps-Zoomcamp/2. Experiment tracking and model management/homework/scripts/register_model.py\", line 67, in run\\ntrain_and_log_model(data_path=data_path, params=run.data.params)\\nFile \"/Users/name/Desktop/Programming/DataTalksClub/MLOps-Zoomcamp/2. Experiment tracking and model management/homework/scripts/register_model.py\", line 41, in train_and_log_model\\nparams = space_eval(SPACE, params)\\nFile \"/Users/name/miniconda3/envs/mlops-zoomcamp/lib/python3.9/site-packages/hyperopt/fmin.py\", line 618, in space_eval\\nrval = pyll.rec_eval(space, memo=memo)\\nFile \"/Users/name/miniconda3/envs/mlops-zoomcamp/lib/python3.9/site-packages/hyperopt/pyll/base.py\", line 902, in rec_eval\\nrval = scope._impls[node.name](*args, **kwargs)\\nValueError: could not convert string to float: \\'0 int\\\\n1   float\\\\n2     hyperopt_param\\\\n3       Literal{n_estimators}\\\\n4       quniform\\\\n5         Literal{10}\\\\n6         Literal{50}\\\\n7         Literal{1}\\'\\nSolution: There are two plausible errors to this. Both are in the hpo.py file where the hyper-parameter tuning is run. The objective function should look like this.\\n\\n   def objective(params):\\n# It\\'s important to set the \"with\" statement and the \"log_params\" function here\\n# in order to properly log all the runs and parameters.\\nwith mlflow.start_run():\\n# Log the parameters\\nmlflow.log_params(params)\\nrf = RandomForestRegressor(**params)\\nrf.fit(X_train, y_train)\\ny_pred = rf.predict(X_valid)\\n# Calculate and log rmse\\nrmse = mean_squared_error(y_valid, y_pred, squared=False)\\nmlflow.log_metric(\\'rmse\\', rmse)\\nIf you add the with statement before this function, and just after the following line\\nX_valid, y_valid = load_pickle(os.path.join(data_path, \"valid.pkl\"))\\nand you log the parameters just after the search_space dictionary is defined, like this\\nsearch_space = {....}\\n# Log the parameters\\nmlflow.log_params(search_space)\\nThen there is a risk that the parameters will be logged in group. As a result, the\\nparams = space_eval(SPACE, params)\\nregister_model.py file will receive the parameters in group, while in fact it expects to receive them one by one. Thus, make sure that the objective function looks as above.\\nAdded by Jakob Salomonsson',\n",
       "    'section': 'Module 2: Experiment tracking',\n",
       "    'question': 'Could not convert string to float - ValueError'},\n",
       "   {'text': 'Make sure you launch the mlflow UI from the same directory as thec that is running the experiments (same directory that has the mlflow directory and the database that stores the experiments).\\nOr navigate to the correct directory when specifying the tracking_uri.\\nFor example:\\nIf the mlflow.db is in a subdirectory called database, the tracking uri would be ‘sqllite:///database/mlflow.db’\\nIf the mlflow.db is a directory above your current directory: the tracking uri would be ‘sqlite:///../mlflow.db’\\nAnswered by Anna Vasylytsya\\nAnother alternative is to use an absolute path to mlflow.db rather than relative path\\nAnd yet another alternative is to launch the UI from the same notebook by executing the following code cell\\nimport subprocess\\nMLFLOW_TRACKING_URI = \"sqlite:///data/mlflow.db\"\\nsubprocess.Popen([\"mlflow\", \"ui\", \"--backend-store-uri\", MLFLOW_TRACKING_URI])\\nAnd then using the same MLFLOW_TRACKING_URI when initializing mlflow or the client\\nclient = MlflowClient(tracking_uri=MLFLOW_TRACKING_URI)\\nmlflow.set_tracking_uri(MLFLOW_TRACKING_URI)',\n",
       "    'section': 'Module 2: Experiment tracking',\n",
       "    'question': 'Experiment not visible in MLflow UI'},\n",
       "   {'text': \"Problem:\\nGetting\\nERROR: THESE PACKAGES DO NOT MATCH THE HASHES FROM THE REQUIREMENTS FILE\\nduring MLFlow's installation process, particularly while installing the Numpy package using pip\\nWhen I installed mlflow using ‘pip install mlflow’ on 27th May 2022, I got the following error while numpy was getting installed through mlflow:\\n\\nCollecting numpy\\nDownloading numpy-1.22.4-cp310-cp310-win_amd64.whl (14.7 MB)\\n|██████████████              \\t| 6.3 MB 107 kB/s eta 0:01:19\\nERROR: THESE PACKAGES DO NOT MATCH THE HASHES FROM THE REQUIREMENTS FILE.\\nIf you have updated the package versions, please update the hashes. Otherwise, examine the package contents carefully; someone may have tampered with them.\\nnumpy from https://files.pythonhosted.org/packages/b5/50/d7978137464251c393df28fe0592fbb968110f752d66f60c7a53f7158076/numpy-1.22.4-cp310-cp310-win_amd64.whl#sha256=3e1ffa4748168e1cc8d3cde93f006fe92b5421396221a02f2274aab6ac83b077 (from mlflow):\\nExpected sha256 3e1ffa4748168e1cc8d3cde93f006fe92b5421396221a02f2274aab6ac83b077\\nGot    \\t15e691797dba353af05cf51233aefc4c654ea7ff194b3e7435e6eec321807e90\\nSolution:\\nThen when I install numpy separately (and not as part of mlflow), numpy gets installed (same version), and then when I do 'pip install mlflow', it also goes through.\\nPlease note that the above may not be consistently simulatable, but please be aware of this issue that could occur during pip install of mlflow.\\nAdded by Venkat Ramakrishnan\",\n",
       "    'section': 'Module 2: Experiment tracking',\n",
       "    'question': 'Hash Mismatch Error with Package Installation'},\n",
       "   {'text': 'After deleting an experiment from UI, the deleted experiment still persists in the database.\\nSolution: To delete this experiment permanently, follow these steps.\\nAssuming you are using sqlite database;\\nInstall ipython sql using the following command: pip install ipython-sql\\nIn your jupyter notebook, load the SQL magic scripts with this: %load_ext sql\\nLoad the database with this: %sql sqlite:///nameofdatabase.db\\nRun the following SQL script to delete the experiment permanently: check link',\n",
       "    'section': 'Module 2: Experiment tracking',\n",
       "    'question': 'How to Delete an Experiment Permanently from MLFlow UI'},\n",
       "   {'text': 'Problem: I cloned the public repo, made edits, committed and pushed them to my own repo. Now I want to get the recent commits from the public repo without overwriting my own changes to my own repo. Which command(s) should I use?\\nThis is what my config looks like (in case this might be useful):\\n[core]\\nrepositoryformatversion = 0\\nfilemode = true\\nbare = false\\nlogallrefupdates = true\\nignorecase = true\\nprecomposeunicode = true\\n[remote \"origin\"]\\nurl = git@github.com:my_username/mlops-zoomcamp.git\\nfetch = +refs/heads/*:refs/remotes/origin/*\\n[branch \"main\"]\\nremote = origin\\nmerge = refs/heads/main\\nSolution: You should fork DataClubsTak’s repo instead of cloning it. On GitHub, click “Fetch and Merge” under the menu “Fetch upstream” at the main page of your own',\n",
       "    'section': 'Module 2: Experiment tracking',\n",
       "    'question': 'How to Update Git Public Repo Without Overwriting Changes'},\n",
       "   {'text': 'This is caused by ```mlflow.xgboost.autolog()``` when version 1.6.1 of xgboost\\nDowngrade to 1.6.0\\n```pip install xgboost==1.6.0``` or update requirements file with xgboost==1.6.0 instead of xgboost\\nAdded by Nakul Bajaj',\n",
       "    'section': 'Module 2: Experiment tracking',\n",
       "    'question': 'Image size of 460x93139 pixels is too large. It must be less than 2^16 in each direction.'},\n",
       "   {'text': 'Since the version 1.29 the list_experiments method was deprecated and then removed in the later version\\nYou should use search_experiments instead\\nAdded by Alex Litvinov',\n",
       "    'section': 'Module 2: Experiment tracking',\n",
       "    'question': \"MlflowClient object has no attribute 'list_experiments'\"},\n",
       "   {'text': 'Make sure `mlflow.autolog()` ( or framework-specific autolog ) written BEFORE `with mlflow.start_run()` not after.\\nAlso make sure that all dependencies for the autologger are installed, including matplotlib. A warning about uninstalled dependencies will be raised.\\nMohammed Ayoub Chettouh',\n",
       "    'section': 'Module 2: Experiment tracking',\n",
       "    'question': 'MLflow Autolog not working'},\n",
       "   {'text': 'If you’re running MLflow on a remote VM, you need to forward the port too like we did in Module 1 for Jupyter notebook port 8888. Simply connect your server to VS Code, as we did, and add 5000 to the PORT like in the screenshot:\\nAdded by Sharon Ibejih\\nIf you are running MLflow locally and 127.0.0.1:5000 shows a blank page navigate to localhost:5000 instead.',\n",
       "    'section': 'Module 2: Experiment tracking',\n",
       "    'question': 'MLflow URL (http://127.0.0.1:5000), doesn’t open.'},\n",
       "   {'text': 'Got the same warning message as Warrie Warrie when using “mlflow.xgboost.autolog()”\\nIt turned out that this was just a warning message and upon checking MLflow UI (making sure that no “tag” filters were included), the model was actually automatically tracked in the MLflow.\\nAdded by Bengsoon Chuah, Asked by Warrie Warrie, Answered by Anna Vasylytsya & Ivan Starovit',\n",
       "    'section': 'Module 2: Experiment tracking',\n",
       "    'question': 'MLflow.xgboost Autolog Model Signature Failure'},\n",
       "   {'text': \"mlflow.exceptions.MlflowException: Cannot set a deleted experiment 'cross-sell' as the active experiment. You can restore the experiment, or permanently delete the  experiment to create a new one.\\nThere are many options to solve in this link: https://stackoverflow.com/questions/60088889/how-do-you-permanently-delete-an-experiment-in-mlflow\",\n",
       "    'section': 'Module 2: Experiment tracking',\n",
       "    'question': 'MlflowException: Unable to Set a Deleted Experiment'},\n",
       "   {'text': 'You do not have enough disk space to install the requirements. You can either increase the base EBS volume by following this link or add an external disk to your instance and configure conda installation to happen on the external disk.\\nAbinaya Mahendiran\\nOn GCP: I added another disk to my vm and followed this guide to mount the disk. Confirm the mount by running df -H (disk free) command in bash shell. I also deleted Anaconda and instead used miniconda. I downloaded miniconda in the additional disk that I mounted and when installing miniconda, enter the path to the extra disk instead of the default disk, this way conda is installed on the extra disk.\\nYang Cao',\n",
       "    'section': 'Module 2: Experiment tracking',\n",
       "    'question': 'No Space Left on Device - OSError[Errno 28]'},\n",
       "   {'text': 'I was using an old version of sklearn due to which I got the wrong number of parameters because in the latest version min_impurity_split for randomforrestRegressor was deprecated. Had to upgrade to the latest version to get the correct number of params.',\n",
       "    'section': 'Module 2: Experiment tracking',\n",
       "    'question': 'Parameters Mismatch in Homework Q3'},\n",
       "   {'text': \"Error: I installed all the libraries from the requirements.txt document in a new environment as follows:\\npip install -r requirementes.txt\\nThen when I run mlflow from my terminal like this:\\nmlflow\\nI get this error:\\nSOLUTION: You need to downgrade the version of 'protobuf' module to 3.20.x or lower. Initially, it was version=4.21, I installed protobuf==3.20\\npip install protobuf==3.20\\nAfter which I was able to run mlflow from my terminal.\\n-Submitted by Aashnna Soni\",\n",
       "    'section': 'Module 2: Experiment tracking',\n",
       "    'question': 'Protobuf error when installing MLflow'},\n",
       "   {'text': 'Please check your current directory while running the mlflow ui command. You need to run mlflow ui or mlflow server command in the right directory.',\n",
       "    'section': 'Module 2: Experiment tracking',\n",
       "    'question': 'Setting up Artifacts folders'},\n",
       "   {'text': 'If you have problem with setting up MLflow for experiment tracking on GCP, you can check these two links:\\nhttps://kargarisaac.github.io/blog/mlops/data%20engineering/2022/06/15/MLFlow-on-GCP.html\\nhttps://kargarisaac.github.io/blog/mlops/2022/08/26/machine-learning-workflow-orchestration-zenml.html',\n",
       "    'section': 'Module 2: Experiment tracking',\n",
       "    'question': 'Setting up MLflow experiment tracker on GCP'},\n",
       "   {'text': 'Solution: Downgrade setuptools (I downgraded 62.3.2 -> 49.1.0)',\n",
       "    'section': 'Module 2: Experiment tracking',\n",
       "    'question': 'Setuptools Replacing Distutils - MLflow Autolog Warning'},\n",
       "   {'text': 'I can’t sort runs in MLFlow\\nMake sure you are in table view (not list view) in the MLflow UI.\\nAdded and Answered by Anna Vasylytsya',\n",
       "    'section': 'Module 2: Experiment tracking',\n",
       "    'question': 'Sorting runs in MLflow UI'},\n",
       "   {'text': 'Problem: When I ran `$ mlflow ui` on a remote server and try to open it in my local browser I got an exception  and the page with mlflow ui wasn’t loaded.\\nSolution: You should `pip uninstall flask` on your remote server on conda env and after it install Flask `pip install Flask`. It is because the base conda env has ~flask<1.2, and when you clone it to your new work env, you are stuck with this old version.\\nAdded by Salimov Ilnaz',\n",
       "    'section': 'Module 2: Experiment tracking',\n",
       "    'question': \"TypeError: send_file() unexpected keyword 'max_age' during MLflow UI Launch\"},\n",
       "   {'text': 'Problem: After successfully installing mlflow using pip install mlflow on my Windows system, I am trying to run the mlflow ui command but it throws the following error:\\nFileNotFoundError: [WinError 2] The system cannot find the file specified\\nSolution: Add C:\\\\Users\\\\{User_Name}\\\\AppData\\\\Roaming\\\\Python\\\\Python39\\\\Scripts to the PATH\\nAdded by Alex Litvinov',\n",
       "    'section': 'Module 2: Experiment tracking',\n",
       "    'question': 'mlflow ui on Windows FileNotFoundError: [WinError 2] The system cannot find the file specified'},\n",
       "   {'text': 'Running “python hpo.py --data_path=./your-path --max_evals=50” for the homework leads to the following error: TypeError: unsupported operand type(s) for -: \\'str\\' and \\'int\\'\\nFull Traceback:\\nFile \"~/repos/mlops/02-experiment-tracking/homework/hpo.py\", line 73, in <module>\\nrun(args.data_path, args.max_evals)\\nFile \"~/repos/mlops/02-experiment-tracking/homework/hpo.py\", line 47, in run\\nfmin(\\nFile \"~/Library/Caches/pypoetry/virtualenvs/mlflow-intro-SyTqwt0D-py3.9/lib/python3.9/site-packages/hyperopt/fmin.py\", line 540, in fmin\\nreturn trials.fmin(\\nFile \"~/Library/Caches/pypoetry/virtualenvs/mlflow-intro-SyTqwt0D-py3.9/lib/python3.9/site-packages/hyperopt/base.py\", line 671, in fmin\\nreturn fmin(\\nFile \"~/Library/Caches/pypoetry/virtualenvs/mlflow-intro-SyTqwt0D-py3.9/lib/python3.9/site-packages/hyperopt/fmin.py\", line 586, in fmin\\nrval.exhaust()\\nFile \"~/Library/Caches/pypoetry/virtualenvs/mlflow-intro-SyTqwt0D-py3.9/lib/python3.9/site-packages/hyperopt/fmin.py\", line 364, in exhaust\\nself.run(self.max_evals - n_done, block_until_done=self.asynchronous)\\nTypeError: unsupported operand type(s) for -: \\'str\\' and \\'int\\'\\nSolution:\\nThe --max_evals argument in hpo.py has no defined datatype and will therefore implicitly be treated as string. It should be an integer, so that the script can work correctly. Add type=int to the argument definition:\\nparser.add_argument(\\n\"--max_evals\",\\ntype=int,\\ndefault=50,\\nhelp=\"the number of parameter evaluations for the optimizer to explore.\"\\n)',\n",
       "    'section': 'Module 2: Experiment tracking',\n",
       "    'question': 'Unsupported Operand Type Error in hpo.py'},\n",
       "   {'text': 'Getting the following warning when running mlflow.sklearn:\\n\\n2022/05/28 04:36:36 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of sklearn. If you encounter errors during autologging, try upgrading / downgrading sklearn to a supported version, or try upgrading MLflow. […]\\nSolution: use 0.22.1 <= scikit-learn <= 1.1.0\\nReference: https://www.mlflow.org/docs/latest/python_api/mlflow.sklearn.html',\n",
       "    'section': 'Module 2: Experiment tracking',\n",
       "    'question': 'Unsupported Scikit-Learn version'},\n",
       "   {'text': 'Problem: CLI commands (mlflow experiments list) do not return experiments\\nSolution description: need to set environment variable for the Tracking URI:\\n$ export MLFLOW_TRACKING_URI=http://127.0.0.1:5000\\nAdded and Answered by Dino Vitale',\n",
       "    'section': 'Module 2: Experiment tracking',\n",
       "    'question': 'Mlflow CLI does not return experiments'},\n",
       "   {'text': 'Problem: After starting the tracking server, when we try to use the mlflow cli commands as listed here, most of them can’t seem to find the experiments that have been run with the tracking server\\nSolution: We need to set the environment variable MLFLOW_TRACKING_URI to the URI of the sqlite database. This is something like “export MLFLOW_TRACKING_URI=sqlite:///{path to sqlite database}” . After this, we can view the experiments from the command line using commands like “mlflow experiments search”\\nEven after this commands like “mlflow gc” doesn’t seem to get the tracking uri, and they have to be passed explicitly as an argument every time the command is run.\\nAhmed Fahim (afahim03@yahoo.com)',\n",
       "    'section': 'Module 2: Experiment tracking',\n",
       "    'question': 'Viewing MLflow Experiments using MLflow CLI'},\n",
       "   {'text': 'All the experiment and other tracking information in mlflow are stored in sqllite database provided while initiating the mlflow ui command. This database can be inspected using Pycharm’s Database tab by using the SQLLite database type. Once the connection is created as below, the tables can be queried and inspected using regular SQL. The same applies for any SQL backed database such as postgres as well.\\nThis is very useful to understand the entity structure of the data being stored within mlflow and useful for any kind of systematic archiving of model tracking for longer periods.\\nAdded by Senthilkumar Gopal',\n",
       "    'section': 'Module 2: Experiment tracking',\n",
       "    'question': 'Viewing SQLlite Data Raw & Deleting Experiments Manually'},\n",
       "   {'text': 'Solution : It is another way to start it for remote hosting a mlflow server. For example, if you are multiple colleagues working together on something you most likely would not run mlflow on one laptop but rather everyone would connect to the same server running mlflow\\nAnswer by Christoffer Added by Akshit Miglani (akshit.miglani09@gmail.com)',\n",
       "    'section': 'Module 2: Experiment tracking',\n",
       "    'question': 'What does launching the tracking server locally mean?'},\n",
       "   {'text': 'Problem: parameter was not recognized during the model registry\\nSolution: parameters should be added in previous to the model registry. The parameters can be added by mlflow.log_params(params) so that the dictionary can be directly appended to the data.run.params.\\nAdded and Answered by Sam Lim',\n",
       "    'section': 'Module 2: Experiment tracking',\n",
       "    'question': 'Parameter adding in case of max_depth not recognized'},\n",
       "   {'text': 'Problem: Max_depth is not recognize even when I add the mlflow.log_params\\nSolution: the mlflow.log_params(params) should be added to the hpo.py script, but if you run it it will append the new model to the previous run that doesn’t contain the parameters, you should either remove the previous experiment or change it\\nPastor Soto',\n",
       "    'section': 'Module 2: Experiment tracking',\n",
       "    'question': 'Max_depth is not recognize even when I add the mlflow.log_params'},\n",
       "   {'text': \"Problem: About week_2 homework: The register_model.py  script, when I copy it into a jupyter notebook fails and spits out the following error. AttributeError: 'tuple' object has no attribute 'tb_frame'\\nSolution: remove click decorators\",\n",
       "    'section': 'Module 2: Experiment tracking',\n",
       "    'question': \"AttributeError: 'tuple' object has no attribute 'tb_frame'\"},\n",
       "   {'text': 'Problem: when running the preprocess_data.py file you get the following error:\\n\\nwandb: ERROR api_key not configured (no-tty). call wandb.login(key=[your_api_key])\\nSolution: Go to your WandB profile (top RHS) → user settings → scroll down to “Danger Zone” and copy your API key. \\n\\nThen before running preprocess_data.py, add and run the following cell in your notebook:\\n\\n%%bash\\n\\nWandb login <YOUR_API_KEY_HERE>.\\nAdded and Answered by James Gammerman (jgammerman@gmail.com)',\n",
       "    'section': 'Module 2: Experiment tracking',\n",
       "    'question': 'WandB API error'},\n",
       "   {'text': 'Please make sure you following the order below nd enabling the autologging before constructing the dataset. If you still have this issue check that your data is in format compatible with XGBoost.\\n# Enable MLflow autologging for XGBoost\\nmlflow.xgboost.autolog()\\n# Construct your dataset\\nX_train, y_train = ...\\n# Train your XGBoost model\\nmodel = xgb.XGBRegressor(...)\\nmodel.fit(X_train, y_train)\\nAdded by Olga Rudakova',\n",
       "    'section': 'Module 2: Experiment tracking',\n",
       "    'question': 'WARNING mlflow.xgboost: Failed to infer model signature: could not sample data to infer model signature: please ensure that autologging is enabled before constructing the dataset.'},\n",
       "   {'text': 'Problem\\nUsing wget command to download either data or python scripts on Windows, I am using the notebook provided by Visual Studio and despite having a python virtual env, it did not recognize the pip command.\\nSolution: Use python -m pip, this same for any other command. Ie. python -m wget\\nAdded by Erick Calderin',\n",
       "    'section': 'Module 2: Experiment tracking',\n",
       "    'question': 'wget not working'},\n",
       "   {'text': \"Problem: Open/run github notebook(.ipynb) directly in Google Colab\\nSolution: Change the domain from 'github.com' to 'githubtocolab.com'. The notebook will open in Google Colab.\\nOnly works with Public repo.\\nAdded by Ming Jun\\nNavigating in Wandb UI became difficult to me, I had to intuit some options until I found the correct one.\\nSolution: Go to the official doc.\\nAdded by Erick Calderin\",\n",
       "    'section': 'Module 2: Experiment tracking',\n",
       "    'question': 'Open/run github notebook(.ipynb) directly in Google Colab'},\n",
       "   {'text': 'Problem: Someone asked why we are using this type of split approach instead of just a random split.\\nSolution: For example, I have some models at work that train on Jan 1 2020 — Aug 1 2021 time period, and then test on Aug 1 - Dec 31 2021, and finally validate on Jan - March or something\\nWe do these “out of time”  validations to do a few things:\\nCheck for seasonality of our data\\nWe know if the RMSE for Test is 5 say, and then RMSE for validation is 20, then there’s serious seasonality to the data we are looking at, and now we might change to Time Series approaches\\nIf I’m predicting on Mar 30 2023 the outcomes for the next 3 months, the “random sample” in our train/test would have caused data leakage, overfitting, and poor model performance in production. We mustn’t take information about the future and apply it to the present when we are predicting in a model context.\\nThese are two of, I think, the biggest points for why we are doing jan/feb/march. I wouldn’t do it any other way.\\nTrain: Jan\\nTest: Feb\\nValidate: March\\nThe point of validation is to report out model metrics to leadership, regulators, auditors, and record the models performance to then later analyze target drift\\nAdded by Sam LaFell\\nProblem: If you get an error while trying to run the mlflow server on AWS CLI with S3 bucket and POSTGRES database:\\nReproducible Command:\\nmlflow server -h 0.0.0.0 -p 5000 --backend-store-uri postgresql://<DB_USERNAME>:<DB_PASSWORD>@<DB_ENDPOINT>:<DB_PORT>/<DB_NAME> --default-artifact-root s3://<BUCKET_NAME>\\nError:\\n\"urllib3 v2.0 only supports OpenSSL 1.1.1+, currently \"\\nImportError: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the \\'ssl\\' module is compiled with \\'OpenSSL 1.0.2k-fips  26 Jan 2017\\'. See: https://github.com/urllib3/urllib3/issues/2168\\nSolution: Upgrade mlflow using\\nCode: pip3 install --upgrade mlflow\\nResolution: It downgrades urllib3 2.0.3 to 1.26.16 which is compatible with mlflow and ssl 1.0.2\\nInstalling collected packages: urllib3\\nAttempting uninstall: urllib3\\nFound existing installation: urllib3 2.0.3\\nUninstalling urllib3-2.0.3:\\nSuccessfully uninstalled urllib3-2.0.3\\nSuccessfully installed urllib3-1.26.16\\nAdded by Sarvesh Thakur',\n",
       "    'section': 'Module 3: Orchestration',\n",
       "    'question': 'Why do we use Jan/Feb/March for Train/Test/Validation Purposes?'},\n",
       "   {'text': 'Problem description\\nSolution description\\n(optional) Added by Name',\n",
       "    'section': 'Module 3: Orchestration',\n",
       "    'question': 'Problem title'},\n",
       "   {'text': 'Here',\n",
       "    'section': 'Module 4: Deployment',\n",
       "    'question': 'Where is the FAQ for Prefect questions?'},\n",
       "   {'text': 'Windows with AWS CLI already installed\\nAWS CLI version:\\naws-cli/2.4.24 Python/3.8.8 Windows/10 exe/AMD64 prompt/off\\nExecuting\\n$(aws ecr get-login --no-include-email)\\nshows error\\naws.exe: error: argument operation: Invalid choice, valid choices are…\\nUse this command instead. More info here:\\nhttps://docs.aws.amazon.com/cli/latest/reference/ecr/get-login-password.html\\naws ecr get-login-password \\\\\\n--region <region> \\\\\\n| docker login \\\\\\n--username AWS \\\\\\n--password-stdin <aws_account_id>.dkr.ecr.<region>.amazonaws.com\\nAdded by MarcosMJD',\n",
       "    'section': 'Module 4: Deployment',\n",
       "    'question': 'aws.exe: error: argument operation: Invalid choice — Docker can not login to ECR.'},\n",
       "   {'text': 'Use ` at the end of each line except the last. Note that multiline string does not need `.\\nEscape “ to “\\\\ .\\nUse $env: to create env vars (non-persistent). E.g.:\\n$env:KINESIS_STREAM_INPUT=\"ride_events\"\\naws kinesis put-record --cli-binary-format raw-in-base64-out `\\n--stream-name $env:KINESIS_STREAM_INPUT `\\n--partition-key 1 `\\n--data \\'{\\n\\\\\"ride\\\\\": {\\n\\\\\"PULocationID\\\\\": 130,\\n\\\\\"DOLocationID\\\\\": 205,\\n\\\\\"trip_distance\\\\\": 3.66\\n},\\n\\\\\"ride_id\\\\\": 156\\n}\\'\\nAdded by MarcosMJD',\n",
       "    'section': 'Module 4: Deployment',\n",
       "    'question': 'Multiline commands in Windows Powershell'},\n",
       "   {'text': \"If one gets pipenv failures for pipenv install command -\\nAttributeError: module 'collections' has no attribute 'MutableMapping'\\nIt happens because you use the system Python (3.10) for pipenv.\\nIf you previously installed pipenv with apt-get, remove it - sudo-apt remove pipenv\\nMake sure you have a non-system Python installed in your environment. The easiest way to do it is to install anaconda or miniconda\\nNext, install pipenv to your non-system Python. If you use the setup from the lectures, it’s just this: pip install pipenv\\nNow re-run pipenv install XXXX (relevant dependencies) - should work\\nTested and worked on AWS instance, similar to the config Alexey presented in class.\\nAdded by Daniel HenSSL\",\n",
       "    'section': 'Module 4: Deployment',\n",
       "    'question': \"Pipenv installation not working (AttributeError: module 'collections' has no attribute 'MutableMapping')\"},\n",
       "   {'text': 'First check if SSL module configured with following command:\\nPython -m ssl\\n\\nIf the output of this is empty there is no problem with SSL configuration.\\n\\nThen you should upgrade your pipenv package in your current environment to resolve the problem.\\nAdded by Kenan Arslanbay',\n",
       "    'section': 'Module 4: Deployment',\n",
       "    'question': \"module is not available (Can't connect to HTTPS URL)\"},\n",
       "   {'text': \"During scikit-learn installation via the command:\\npipenv install scikit-learn==1.0.2\\nThe following error is raised:\\nModuleNotFoundError: No module named 'pip._vendor.six'\\nThen, one should:\\nsudo apt install python-six\\npipenv --rm\\npipenv install scikit-learn==1.0.2\\nAdded by Giovanni Pecoraro\",\n",
       "    'section': 'Module 4: Deployment',\n",
       "    'question': \"No module named 'pip._vendor.six'\"},\n",
       "   {'text': 'Problem description. How can we use Jupyter notebooks with the Pipenv environment?\\nSolution: Refer to this stackoverflow question. Basically install jupyter and ipykernel using pipenv. And then register the kernel with `python -m ipykernel install --user --name=my-virtualenv-name` inside the Pipenv shell. If you are using Jupyter notebooks in VS Code, doing this will also add the virtual environment in the list of kernels.\\nAdded by Ron Medina',\n",
       "    'section': 'Module 4: Deployment',\n",
       "    'question': 'Pipenv with Jupyter'},\n",
       "   {'text': \"Problem: I tried to run starter notebook on pipenv environment but had issues with no output on prints. \\nI used scikit-learn==1.2.2 and python==3.10\\nTornado version was 6.3.2\\n\\nSolution: The error you're encountering seems to be a bug related to Tornado, which is a Python web server and networking library. It's used by Jupyter under the hood to handle networking tasks.\\nDowngrading to tornado==6.1 fixed the issue\\nhttps://stackoverflow.com/questions/54971836/no-output-jupyter-notebook\",\n",
       "    'section': 'Module 4: Deployment',\n",
       "    'question': 'Pipenv with Jupyter no output'},\n",
       "   {'text': 'Problem description:  You might get an error ‘Invalid base64’ after running the ‘aws kinesis put-record’ command on your local machine. This might be the case if you are using the AWS CLI version 2 (note that in the video 4.4, around 57:42, you can see a warning since the instructor is using v1 of the CLI.\\nSolution description: To get around this, pass the argument ‘--cli-binary-format raw-in-base64-out’. This will encode your data string into base64 before passing it to kinesis\\nAdded by M',\n",
       "    'section': 'Module 4: Deployment',\n",
       "    'question': '‘Invalid base64’ error after running `aws kinesis put-record`'},\n",
       "   {'text': 'Problem description:   Running starter.ipynb in homework’s Q1 will show up this error.\\nSolution description: Update pandas (actually pandas version was the latest, but several dependencies are updated).\\nAdded by Marcos Jimenez',\n",
       "    'section': 'Module 4: Deployment',\n",
       "    'question': 'Error index 311297 is out of bounds for axis 0 with size 131483 when loading parquet file.'},\n",
       "   {'text': 'Use command $pipenv lock to force the creation of Pipfile.lock\\nAdded by Bijay P.',\n",
       "    'section': 'Module 4: Deployment',\n",
       "    'question': 'Pipfile.lock was not created along with Pipfile'},\n",
       "   {'text': 'This issue is usually due to the pythonfinder module in pipenv.\\nThe solution to this involves manually changing the scripts as describe here python_finder_fix\\nAdded by Ridwan Amure',\n",
       "    'section': 'Module 4: Deployment',\n",
       "    'question': 'Permission Denied using Pipenv'},\n",
       "   {'text': 'When passing arguments to a script via command line and converting it to a 4 digit number using f’{year:04d}’, this error showed up.\\nThis happens because all inputs from the command line are read as string by the script. They need to be converted to numeric/integer before transformation in fstring.\\nyear = int(sys.argv[1])\\nf’{year:04d}’\\nIf you use click library just edit a decorator\\n@click.command()\\n@click.option( \"--year\",  help=\"Year for evaluation\",   type=int)\\ndef  your_function(year):\\n<<Your code>>\\nAdded by Taras Sh',\n",
       "    'section': 'Module 4: Deployment',\n",
       "    'question': \"Error while parsing arguments via CLI  [ValueError: Unknown format code 'd' for object of type 'str']\"},\n",
       "   {'text': 'Ensure the correct image is being used to derive from.\\nCopy the data from local to the docker image using the COPY command to a relative path. Using absolute paths within the image might be troublesome.\\nUse paths starting from /app and don’t forget to do WORKDIR /app before actually performing the code execution.\\nMost common commands\\nBuild container using docker build -t mlops-learn .\\nExecute the script using docker run -it --rm mlops-learn\\n<mlops-learn> is just a name used for the image and does not have any significance.',\n",
       "    'section': 'Module 4: Deployment',\n",
       "    'question': 'Dockerizing tips'},\n",
       "   {'text': 'If you are trying to run Flask gunicorn & MLFlow server from the same container, defining both in Dockerfile with CMD will only run MLFlow & not Flask.\\nSolution: Create separate shell script with server run commands, for eg:\\n> \\tscript1.sh\\n#!/bin/bash\\ngunicorn --bind=0.0.0.0:9696 predict:app\\nAnother script with e.g. MLFlow server:\\n>\\tscript2.sh\\n#!/bin/bash\\nmlflow server -h 0.0.0.0 -p 5000 --backend-store-uri=sqlite:///mlflow.db --default-artifact-root=g3://zc-bucket/mlruns/\\nCreate a wrapper script to run above 2 scripts:\\n>\\twrapper_script.sh\\n#!/bin/bash\\n# Start the first process\\n./script1.sh &\\n# Start the second process\\n./script2.sh &\\n# Wait for any process to exit\\nwait -n\\n# Exit with status of process that exited first\\nexit $?\\nGive executable permissions to all scripts:\\nchmod +x *.sh\\nNow we can define last line of Dockerfile as:\\n> \\tDockerfile\\nCMD ./wrapper_script.sh\\nDont forget to expose all ports defined by services!',\n",
       "    'section': 'Module 4: Deployment',\n",
       "    'question': 'Running multiple services in a Docker container'},\n",
       "   {'text': 'Problem description cannot generate pipfile.lock raise InstallationError( pip9.exceptions.InstallationError: Command \"python setup.py egg_info\" failed with error code 1\\nSolution: you need to force and upgrade wheel and pipenv\\nJust run the command line :\\npip install --user --upgrade --upgrade-strategy eager pipenv wheel',\n",
       "    'section': 'Module 4: Deployment',\n",
       "    'question': 'Cannot generate pipfile.lock raise InstallationError( pip9.exceptions.InstallationError)'},\n",
       "   {'text': \"Problem description. How can we connect s3 bucket to MLFLOW?\\nSolution: Use boto3 and AWS CLI to store access keys. The access keys are what will be used by boto3 (AWS' Python API tool) to connect with the AWS servers. If there are no Access Keys how can they make sure that they have the right to access this Bucket? Maybe you're a malicious actor (Hacker for ex). The keys must be present for boto3 to talk to the AWS servers and they will provide access to the Bucket if you possess the right permissions. You can always set the Bucket as public so anyone can access it, now you don't need access keys because AWS won't care.\\nRead more here: https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html\\nAdded by Akshit Miglani\",\n",
       "    'section': 'Module 4: Deployment',\n",
       "    'question': 'Connecting s3 bucket to MLFLOW'},\n",
       "   {'text': 'Even though the upload works using aws cli and boto3 in Jupyter notebook.\\nSolution set the AWS_PROFILE environment variable (the default profile is called default)',\n",
       "    'section': 'Module 4: Deployment',\n",
       "    'question': 'Uploading to s3 fails with An error occurred (InvalidAccessKeyId) when calling the PutObject operation: The AWS Access Key Id you provided does not exist in our records.\"'},\n",
       "   {'text': 'Problem description: lib_lightgbm.so Reason: image not found\\nSolution description: Add “RUN apt-get install libgomp1” to your docker. (change installer command based on OS)\\nAdded by Kazeem Hakeem',\n",
       "    'section': 'Module 4: Deployment',\n",
       "    'question': 'Dockerizing lightgbm'},\n",
       "   {'text': 'When the request is processed in lambda function, mlflow library raises:\\n2022/09/19 21:18:47 WARNING mlflow.pyfunc: Encountered an unexpected error (AttributeError(\"module \\'dataclasses\\' has no attribute \\'__version__\\'\")) while detecting model dependency mismatches. Set logging level to DEBUG to see the full traceback.\\nSolution: Increase the memory of the lambda function.\\nAdded by MarcosMJD',\n",
       "    'section': 'Module 4: Deployment',\n",
       "    'question': 'Error raised when executing mlflow’s pyfunc.load_model in lambda function.'},\n",
       "   {'text': 'Just a note if you are following the video but also using the repo’s notebook The notebook is the end state of the video which eventually uses mlflow pipelines.\\nJust watch the video and be patient. Everything will work :)\\nAdded by Quinn Avila',\n",
       "    'section': 'Module 4: Deployment',\n",
       "    'question': '4.3 FYI Notebook is end state of Video -'},\n",
       "   {'text': 'Problem description: I was having issues because my python script was not reading AWS credentials from env vars, after building the image I was running it like this:\\ndocker run -it homework-04 -e AWS_ACCESS_KEY_ID=xxxxxxxx -e AWS_SECRET_ACCESS_KEY=xxxxxx\\nSolution 1:\\n\\nEnvironment Variables: \\nYou can set the AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY and AWS_SESSION_TOKEN (if you are using AWS STS) environment variables. You can set these in your shell, or you can include them in your Docker run command like this:\\nI found out by myself that those variables must be passed before specifying the name of the image, as follow:\\ndocker run -e AWS_ACCESS_KEY_ID=xxxxxxxx -e AWS_SECRET_ACCESS_KEY=xxxxxx -it homework-04\\nAdded by Erick Cal\\nSolution 2 (if AWS credentials were not found):\\nAWS Configuration Files: \\nThe AWS SDKs and CLI will check the ~/.aws/credentials and ~/.aws/config files for credentials if they exist. You can map these files into your Docker container using volumes:\\n\\ndocker run -it --rm -v ~/.aws:/root/.aws homework:v1',\n",
       "    'section': 'Module 4: Deployment',\n",
       "    'question': 'Passing envs to my docker image'},\n",
       "   {'text': 'If anyone is troubleshooting or just interested in seeing the model listed on the image svizor/zoomcamp-model:mlops-3.10.0-slim.\\nCreate a dockerfile. (yep thats all) and build “docker build -t zoomcamp_test .”\\nFROM svizor/zoomcamp-model:mlops-3.10.0-slim\\nRun “docker run -it zoomcamp_test ls /app” output -> model.bin\\nThis will list the contents of the app directory and “model.bin” should output. With this you could just copy your files, for example “copy myfile .” maybe a requirements file and this can be run for example “docker run -it myimage myscript arg1 arg2 ”. Of course keep in mind a build is needed everytime you change the Dockerfile.\\nAnother variation is to have it run when you run the docker file.\\n“””\\nFROM svizor/zoomcamp-model:mlops-3.10.0-slim\\nWORKDIR /app\\nCMD ls\\n“””\\nJust keep in mind CMD is needed because the RUN commands are used for building the image and the CMD is used at container runtime. And in your example you probably want to run a script or should we say CMD a script.\\nQuinn Avila',\n",
       "    'section': 'Module 4: Deployment',\n",
       "    'question': 'How to see the model in the docker container in app/?'},\n",
       "   {'text': 'To resolve this make sure to build the docker image with the platform tag, like this:\\n“docker build -t homework:v1 --platform=linux/arm64 .”',\n",
       "    'section': 'Module 4: Deployment',\n",
       "    'question': \"WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested\"},\n",
       "   {'text': \"Solution: instead of input_file = f'https://s3.amazonaws.com/nyc-tlc/trip+data/{taxi_type}_tripdata_{year:04d}-{month:02d}.parquet'  use input_file = f'https://d37ci6vzurychx.cloudfront.net/trip-data/{taxi_type}_tripdata_{year:04d}-{month:02d}.parquet'\\nIlnaz Salimov\\nsalimovilnaz777@gmail.com\",\n",
       "    'section': 'Module 4: Deployment',\n",
       "    'question': 'HTTPError: HTTP Error 403: Forbidden when call apply_model() in score.ipynb'},\n",
       "   {'text': 'i\\'m getting this error ModuleNotFoundError: No module named \\'pipenv.patched.pip._vendor.urllib3.response\\'\\nand Resolved from this command pip install pipenv --force-reinstall\\ngetting this errror site-packages\\\\pipenv\\\\patched\\\\pip\\\\_vendor\\\\urllib3\\\\connectionpool.py\"\\nResolved from this command pip install -U pip and pip install requests\\nAsif',\n",
       "    'section': 'Module 5: Monitoring',\n",
       "    'question': \"ModuleNotFoundError: No module named 'pipenv.patched.pip._vendor.urllib3.response'\"},\n",
       "   {'text': 'Problem description: When running docker-compose up as shown in the video 5.2 if you go to http://localhost:3000/ you get asked for a username and a password.\\nSolution: for both of them the default is “admin”. Then you can enter your new password. \\nSee also here\\nAdded by JaimeRV',\n",
       "    'section': 'Module 5: Monitoring',\n",
       "    'question': 'Login window in Grafana'},\n",
       "   {'text': 'Problem Description : In Linux, when starting services using docker compose up --build  as shown in video 5.2, the services won’t start and instead we get message unknown flag: --build in command prompt.\\nSolution : Since we install docker-compose separately in Linux, we have to run docker-compose up --build instead of docker compose up --build\\nAdded by Ashish Lalchandani',\n",
       "    'section': 'Module 5: Monitoring',\n",
       "    'question': 'Error in starting monitoring services in Linux'},\n",
       "   {'text': 'Problem: When running prepare.py getting KeyError: ‘content-length’\\nSolution: From Emeli Dral:\\nIt seems to me that the link we used in prepare.py to download taxi data does not work anymore. I substituted the instruction:\\nurl = f\"https://nyc-tlc.s3.amazonaws.com/trip+data/{file}\\nby the\\nurl = f\"https://d37ci6vzurychx.cloudfront.net/trip-data/{file}\"\\nin the prepare.py and it worked for me. Hopefully, if you do the same you will be able to get those data.',\n",
       "    'section': 'Module 5: Monitoring',\n",
       "    'question': 'KeyError ‘content-length’ when running prepare.py'},\n",
       "   {'text': 'Problem description\\nWhen I run the command “docker-compose up –build” and send the data to the real-time prediction service. The service will return “Max retries exceeded with url: /api”.\\nIn my case it because of my evidently service exit with code 2 due to the “app.py” in evidently service cannot import “from pyarrow import parquet as pq”.\\nSolution description\\nThe first solution is just install the pyarrow module “pip install pyarrow”\\nThe second solution is restart your machine.\\nThe third solution is if the first and second one didn’t work with your machine. I found that “app.py” of evidently service didn’t use that module. So comment the pyarrow module out and the problem was solved for me.\\nAdded by Surawut Jirasaktavee',\n",
       "    'section': 'Module 5: Monitoring',\n",
       "    'question': 'Evidently service exit with code 2'},\n",
       "   {'text': 'When using evidently if you get this error.\\nYou probably forgot to and parentheses () just and opening and closing and you are good to go.\\nQuinn Avila',\n",
       "    'section': 'Module 5: Monitoring',\n",
       "    'question': 'ValueError: Incorrect item instead of a metric or metric preset was passed to Report'},\n",
       "   {'text': 'You will get an error if you didn’t add a target=’duration_min’\\nIf you want to use RegressionQualityMetric() you need a target=’duration_min and you need this added to you current_data[‘duration_min’]\\nQuinn Avila',\n",
       "    'section': 'Module 5: Monitoring',\n",
       "    'question': 'For the report RegressionQualityMetric()'},\n",
       "   {'text': 'Problem description\\nValueError: Found array with 0 sample(s) (shape=(0, 6)) while a minimum of 1 is required by LinearRegression.\\nSolution description\\nThis happens because the generated data is based on an early date therefore the training dataset would be empty.\\nAdjust the following\\nbegin = datetime.datetime(202X, X, X, 0, 0)\\nAdded by Luke',\n",
       "    'section': 'Module 5: Monitoring',\n",
       "    'question': 'Found array with 0 sample(s)'},\n",
       "   {'text': 'Problem description\\nGetting “target columns” “prediction columns” not present errors after adding a metric\\nSolution description\\nMake sure to read through the documentation on what is required or optional when adding the metric. I added DatasetCorrelationsMetric which doesn’t require any parameters because the metric evaluates for correlations among the features.\\nSam Lim',\n",
       "    'section': 'Module 5: Monitoring',\n",
       "    'question': 'Adding additional metric'},\n",
       "   {'text': 'When you try to login in Grafana with standard requisites (admin/admin) it throw up an error.\\nAfter run grafana-cli admin reset-admin-password admin in Grafana container the problem will be fixed\\nAdded by Artem Glazkov',\n",
       "    'section': 'Module 5: Monitoring',\n",
       "    'question': 'Standard login in Grafana does not work'},\n",
       "   {'text': 'Problem description. While my metric generation script was still running, I noticed that the charts in Grafana don’t get updated.\\nSolution description. There are two things to pay attention to:\\nRefresh interval: set it to a small value: 5-10-30 seconds\\nUse your local timezone in a call to `pytz.timezone` – I couldn’t get updates before changing this from the original value “Europe/London” to my own zone',\n",
       "    'section': 'Module 5: Monitoring',\n",
       "    'question': 'The chart in Grafana doesn’t get updates'},\n",
       "   {'text': 'Problem description. Prefect server was not running locally, I ran `prefect server start` command but it stopped immediately..\\nSolution description. I used Prefect cloud to run the script, however I created an issue on the Prefect github.\\nBy Erick Calderin',\n",
       "    'section': 'Module 5: Monitoring',\n",
       "    'question': 'Prefect server was not running locally'},\n",
       "   {'text': 'Solution. Using docker CLI run docker system prune to remove unused things (build cache, containers, images etc)\\nAlso, to see what’s taking space before pruning you can run docker system df\\nBy Alex Litvinov',\n",
       "    'section': 'Module 5: Monitoring',\n",
       "    'question': 'no disk space left error when doing docker compose up'},\n",
       "   {'text': 'Problem: when run docker-compose up –build, you may see this error. To solve, add `command: php -S 0.0.0.0:8080 -t /var/www/html` in adminer block in yml file like:\\nadminer:\\ncommand: php -S 0.0.0.0:8080 -t /var/www/html\\nimage: adminer\\n…\\nIlnaz Salimov\\nsalimovilnaz777@gmail.com',\n",
       "    'section': 'Module 5: Monitoring',\n",
       "    'question': 'Failed to listen on :::8080 (reason: php_network_getaddresses: getaddrinfo failed: Address family for hostname not supported)'},\n",
       "   {'text': 'Problem: Can we generate charts like Evidently inside Grafana?\\nSolution: In Grafana that would be a stat panel (just a number) and scatter plot panel (I believe it requires a plug-in). However, there is no native way to quickly recreate this exact Evidently dashboard. You\\'d need to make sure you have all the relevant information logged to your Grafana data source, and then design your own plots in Grafana.\\nIf you want to recreate the Evidently visualizations externally, you can export the Evidently output in JSON with include_render=True\\n(more details here https://docs.evidentlyai.com/user-guide/customization/json-dict-output) and then parse information from it for your external visualization layer. To include everything you need for non-aggregated visuals, you should also add \"raw_data\": True  option (more details here https://docs.evidentlyai.com/user-guide/customization/report-data-aggregation).\\nOverall, this specific plot with under- and over-performance segments is more useful during debugging, so might be easier to access it ad hoc using Evidently.\\nAdded by Ming Jun, Asked by Luke, Answered by Elena Samuylova',\n",
       "    'section': 'Module 6: Best practices',\n",
       "    'question': 'Generate Evidently Chart in Grafana'},\n",
       "   {'text': \"You may get an error ‘{'errorMessage': 'Unable to locate credentials', …’ from the print statement in test_docker.py after running localstack with kinesis.\\nTo fix this, in the docker-compose.yaml file, in addition to the environment variables like AWS_DEFAULT_REGION, add two other variables AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY. Their value is not important; anything like abc will suffice\\nAdded by M\\nOther possibility is just to run\\naws --endpoint-url http://localhost:4566 configure\\nAnd providing random values for AWS Access Key ID , AWS Secret Access Key, Default region name, and Default output format.\\nAdded by M.A. Monjas\",\n",
       "    'section': 'Module 6: Best practices',\n",
       "    'question': 'Get an error ‘Unable to locate credentials’ after running localstack with kinesis'},\n",
       "   {'text': \"You may get an error while creating a bucket with localstack and the boto3 client:\\nbotocore.exceptions.ClientError: An error occurred (IllegalLocationConstraintException) when calling the CreateBucket operation: The unspecified location constraint is incompatible for the region specific endpoint this request was sent to.\\nTo fix this, instead of creating a bucket via\\ns3_client.create_bucket(Bucket='nyc-duration')\\nCreate it with\\ns3_client.create_bucket(Bucket='nyc-duration', CreateBucketConfiguration={\\n'LocationConstraint': AWS_DEFAULT_REGION})\\nyam\\nAdded by M\",\n",
       "    'section': 'Module 6: Best practices',\n",
       "    'question': 'Get an error ‘ unspecified location constraint is incompatible ’'},\n",
       "   {'text': 'When executing an AWS CLI command (e.g., aws s3 ls), you can get the error <botocore.awsrequest.AWSRequest object at 0x7fbaf2666280>.\\nTo fix it, simply set the AWS CLI environment variables:\\nexport AWS_DEFAULT_REGION=eu-west-1\\nexport AWS_ACCESS_KEY_ID=foobar\\nexport AWS_SECRET_ACCESS_KEY=foobar\\nTheir value is not important; anything would be ok.\\nAdded by Giovanni Pecoraro',\n",
       "    'section': 'Module 6: Best practices',\n",
       "    'question': 'Get an error “<botocore.awsrequest.AWSRequest object at 0x7fbaf2666280>” after running an AWS CLI command'},\n",
       "   {'text': 'At every commit the above error is thrown and no pre-commit hooks are ran.\\nMake sure the indentation in .pre-commit-config.yaml is correct. Especially the 4 spaces ahead of every `repo` statement\\nAdded by M. Ayoub C.',\n",
       "    'section': 'Module 6: Best practices',\n",
       "    'question': 'Pre-commit triggers an error at every commit: “mapping values are not allowed in this context”'},\n",
       "   {'text': 'No option to remove pytest test\\nRemove .vscode folder located on the folder you previously used for testing, e.g. folder code (from week6-best-practices) was chosen to test, so you may remove .vscode inside the folder.\\nAdded by Rizdi Aprilian',\n",
       "    'section': 'Module 6: Best practices',\n",
       "    'question': 'Could not reconfigure pytest from zero after getting done with previous folder'},\n",
       "   {'text': 'Problem description\\nFollowing video 6.3, at minute 11:23, get records command returns empty Records.\\nSolution description\\nAdd --no-sign-request to Kinesis get records call:\\n aws --endpoint-url=http://localhost:4566 kinesis get-records --shard-iterator […] --no-sign-request',\n",
       "    'section': 'Module 6: Best practices',\n",
       "    'question': 'Empty Records in Kinesis Get Records with LocalStack'},\n",
       "   {'text': \"Problem description\\ngit commit -m 'Updated xxxxxx'\\nAn error has occurred: InvalidConfigError:\\n==> File .pre-commit-config.yaml\\n=====> 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\\nSolution description\\nSet uft-8 encoding when creating the pre-commit yaml file:\\npre-commit sample-config | out-file .pre-commit-config.yaml -encoding utf8\\nAdded by MarcosMJD\",\n",
       "    'section': 'Module 6: Best practices',\n",
       "    'question': 'In Powershell, Git commit raises utf-8 encoding error after creating pre-commit yaml file'},\n",
       "   {'text': \"Problem description\\ngit commit -m 'Updated xxxxxx'\\n[INFO] Initializing environment for https://github.com/pre-commit/pre-commit-hooks.\\n[INFO] Installing environment for https://github.com/pre-commit/pre-commit-hooks.\\n[INFO] Once installed this environment will be reused.\\nAn unexpected error has occurred: CalledProcessError: command:\\n…\\nreturn code: 1\\nexpected return code: 0\\nstdout:\\nAttributeError: 'PythonInfo' object has no attribute 'version_nodot'\\nSolution description\\nClear app-data of the virtualenv\\npython -m virtualenv api -vvv --reset-app-data\\nAdded by MarcosMJD\",\n",
       "    'section': 'Module 6: Best practices',\n",
       "    'question': \"Git commit with pre-commit hook raises error ‘'PythonInfo' object has no attribute 'version_nodot'\"},\n",
       "   {'text': 'Problem description\\nProject structure:\\n/sources/production/model_service.py\\n/sources/tests/unit_tests/test_model_service.py (“from production.model_service import ModelService)\\nWhen running python test_model_service.py from the sources directory, it works.\\nWhen running pytest ./test/unit_tests fails. ‘No module named ‘production’’\\nSolution description\\nUse python -m pytest ./test/unit_tests\\nExplanation: pytest does not add to the sys.path the path where pytest is run.\\nYou can run python -m pytest, or alternatively export PYTHONPATH=. Before executing pytest\\nAdded by MarcosMJD',\n",
       "    'section': 'Module 6: Best practices',\n",
       "    'question': 'Pytest error ‘module not found’ when if using custom packages in the source code'},\n",
       "   {'text': 'Problem description\\nProject structure:\\n/sources/production/model_service.py\\n/sources/tests/unit_tests/test_model_service.py (“from production.model_service import ModelService)\\ngit commit -t ‘test’ raises ‘No module named ‘production’’ when calling pytest hook\\n- repo: local\\nhooks:\\n- id: pytest-check\\nname: pytest-check\\nentry: pytest\\nlanguage: system\\npass_filenames: false\\nalways_run: true\\nargs: [\\n\"tests/\"\\n]\\nSolution description\\nUse this hook instead:\\n- repo: local\\nhooks:\\n- id: pytest-check\\nname: pytest-check\\nentry: \"./sources/tests/unit_tests/run.sh\"\\nlanguage: system\\ntypes: [python]\\npass_filenames: false\\nalways_run: true\\nAnd make sure that run.sh sets the right directory and run pytest:\\ncd \"$(dirname \"$0\")\"\\ncd ../..\\nexport PYTHONPATH=.\\npipenv run pytest ./tests/unit_tests\\nAdded by MarcosMJD',\n",
       "    'section': 'Module 6: Best practices',\n",
       "    'question': 'Pytest error ‘module not found’ when using pre-commit hooks if using custom packages in the source code'},\n",
       "   {'text': 'Problem description\\nThis is the step in the ci yml file definition:\\n- name: Run Unit Tests\\nworking-directory: \"sources\"\\nrun: ./tests/unit_tests/run.sh\\nWhen executing github ci action, error raises:\\n…/tests/unit_test/run.sh Permission error\\nError: Process completed with error code 126\\nSolution description\\nAdd execution  permission to the script and commit+push:\\ngit update-index --chmod=+x .\\\\sources\\\\tests\\\\unit_tests\\\\run.sh\\nAdded by MarcosMJD',\n",
       "    'section': 'Module 6: Best practices',\n",
       "    'question': 'Github actions: Permission denied error when executing script file'},\n",
       "   {'text': 'Problem description\\nWhen a docker-compose file contains a lot of containers, running the containers may take too much resource. There is a need to easily select only a group of containers while ignoring irrelevant containers during testing.\\nSolution description\\nAdd profiles: [“profile_name”] in the service definition.\\nWhen starting up the service, add `--profile profile_name` in the command.\\nAdded by Ammar Chalifah',\n",
       "    'section': 'Module 6: Best practices',\n",
       "    'question': 'Managing Multiple Docker Containers with docker-compose profile'},\n",
       "   {'text': 'Problem description\\nIf you are having problems with the integration tests and kinesis double check that your aws regions match on the docker-compose and local config. Otherwise you will be creating a stream in the wrong region\\nSolution description\\nFor example set ~/.aws/config region = us-east-1 and the docker-compose.yaml - AWS_DEFAULT_REGION=us-east-1\\nAdded by Quinn Avila',\n",
       "    'section': 'Module 6: Best practices',\n",
       "    'question': 'AWS regions need to match docker-compose'},\n",
       "   {'text': 'Problem description\\nPre-commit command was failing with isort repo.\\nSolution description\\nSet version to 5.12.0\\nAdded by Erick Calderin',\n",
       "    'section': 'Module 6: Best practices',\n",
       "    'question': 'Isort Pre-commit'},\n",
       "   {'text': 'Problem description\\nInfrastructure created in AWS with CD-Deploy Action needs to be destroyed\\nSolution description\\nFrom local:\\nterraform init -backend-config=\"key=mlops-zoomcamp-prod.tfstate\" --reconfigure\\nterraform destroy --var-file vars/prod.tfvars\\nAdded by Erick Calderin',\n",
       "    'section': 'Module 6: Best practices',\n",
       "    'question': 'How to destroy infrastructure created via GitHub Actions'}]}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('documents.json', 'rt') as f:\n",
    "    docs_raw = json.load(f)\n",
    "\n",
    "docs_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e9f86bf-93e2-4af9-8b48-42670e2bfd02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"The purpose of this document is to capture frequently asked technical questions\\nThe exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  “Office Hours'' live.1\\nSubscribe to course public Google Calendar (it works from Desktop only).\\nRegister before the course starts using this link.\\nJoin the course Telegram channel with announcements.\\nDon’t forget to register in DataTalks.Club's Slack and join the channel.\",\n",
       " 'section': 'General course-related questions',\n",
       " 'question': 'Course - When will the course start?',\n",
       " 'course': 'data-engineering-zoomcamp'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# documents = []\n",
    "# for items in docs_raw:\n",
    "#     for idx, item in enumerate(items['documents']):\n",
    "#         item['id'] = idx\n",
    "#         item['course'] = items['course']\n",
    "#         documents.append(item)\n",
    "\n",
    "# documents[0]\n",
    "\n",
    "documents = []\n",
    "for items in docs_raw:\n",
    "    for item in items['documents']:\n",
    "        item['course'] = items['course']\n",
    "        documents.append(item)\n",
    "\n",
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "08728644-2ebf-44a5-ae75-afc1a4b35a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "def generate_document_id(doc):\n",
    "    combined = f\"{doc['course']}-{doc['question']}-{doc['text'][:10]}\"\n",
    "    hash_object = hashlib.md5(combined.encode())\n",
    "    hash_hex = hash_object.hexdigest()\n",
    "    document_id = hash_hex[:8]\n",
    "    return document_id\n",
    "\n",
    "for doc in documents:\n",
    "    doc['id'] = generate_document_id(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eb779074-eee9-4ea4-97bf-b7dc21592500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'GitHub - DataTalksClub data-engineering-zoomcamp#prerequisites',\n",
       " 'section': 'General course-related questions',\n",
       " 'question': 'Course - What are the prerequisites for this course?',\n",
       " 'course': 'data-engineering-zoomcamp',\n",
       " 'id': '1f6520ca'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ca64ce36-d234-47c4-ad90-c23286c1b95c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(947, 948)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "hashes = defaultdict(list)\n",
    "for doc in documents:\n",
    "    doc_id = doc['id']\n",
    "    hashes[doc_id].append(doc)\n",
    "\n",
    "len(hashes), len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4a21f981-a8b4-4ef1-ad07-b0b5b7843570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "593f7569 2\n"
     ]
    }
   ],
   "source": [
    "for k, values in hashes.items():\n",
    "    if len(values) > 1:\n",
    "        print(k, len(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d27685e9-1793-4c73-8d81-57edf621c8a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': \"They both do the same, it's just less typing from the script.\\nAsked by Andrew Katoch, Added by Edidiong Esu\",\n",
       "  'section': '6. Decision Trees and Ensemble Learning',\n",
       "  'question': 'Does it matter if we let the Python file create the server or if we run gunicorn directly?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '593f7569'},\n",
       " {'text': \"They both do the same, it's just less typing from the script.\",\n",
       "  'section': '6. Decision Trees and Ensemble Learning',\n",
       "  'question': 'Does it matter if we let the Python file create the server or if we run gunicorn directly?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '593f7569'}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashes['593f7569']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a40fde32-28f7-4c0b-b088-9a10653ce2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"text\": \"The purpose of this document is to capture frequently asked technical questions\\nThe exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  \\u201cOffice Hours'' live.1\\nSubscribe to course public Google Calendar (it works from Desktop only).\\nRegister before the course starts using this link.\\nJoin the course Telegram channel with announcements.\\nDon\\u2019t forget to register in DataTalks.Club's Slack and join the channel.\",\n",
      "    \"section\": \"General course-related questions\",\n",
      "    \"question\": \"Course - When will the course start?\",\n",
      "    \"course\": \"data-engineering-zoomcamp\",\n",
      "    \"id\": \"c02e79ef\"\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"GitHub - DataTalksClub data-engineering-zoomcamp#prerequisites\",\n"
     ]
    }
   ],
   "source": [
    "# save to above document to json\n",
    "\n",
    "with open('documents_with_id.json', 'wt') as f_out:\n",
    "    json.dump(documents, f_out, indent=2)\n",
    "\n",
    "!head documents_with_id.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "3ef06e43-386d-46b7-a56d-c18662b9cd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You emulate a student who's taking our course. \\\n",
    "Formulate 5 questions this student might ask based on a FAQ record. The record \\\n",
    "should contain the answer to the questions , and the questions should be complete and not too short. \\\n",
    "If possible, use as fewer words as possible from the record. The final output contains ONLY \\\n",
    "a LIST of questions DO NOT include any introductory sentences.\n",
    "\n",
    "The record:\n",
    "\n",
    "section: {section}\n",
    "question: {question}\n",
    "answer: {text}\n",
    "\n",
    "Provide the output in parsable JSON without using code blocks: \n",
    "```\n",
    "[\"question1\", \"question2\", ..., \"question5\"]\n",
    "```\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "838130fb-bf7c-4ba4-8dfb-5baf24e88434",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "client = Groq(api_key=groq_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "65d2915c-2191-4a4b-9a56-7f4c9780be13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\"Can I still join the course despite missing the start date and what are the implications on the final projects deadline if I do decide to join?\", \"What is the deadline for turning in the final projects once I have joined the course?\", \"Can I still submit homeworks even if I registered late?\", \"What are the tasks that I should prioritize if I\\'m joining the course late?\", \"Are there any time constraints that I should be aware of as a latecomer to the course?\"]'"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = documents[2]\n",
    "prompt = prompt_template.format(**doc)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        model=\"llama3-8b-8192\",\n",
    "    )\n",
    "json_response = response.choices[0].message.content\n",
    "json_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "18919aef-51c2-4d3b-8327-b414d1036326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Can I still join the course despite missing the start date and what are the implications on the final projects deadline if I do decide to join?',\n",
       " 'What is the deadline for turning in the final projects once I have joined the course?',\n",
       " 'Can I still submit homeworks even if I registered late?',\n",
       " \"What are the tasks that I should prioritize if I'm joining the course late?\",\n",
       " 'Are there any time constraints that I should be aware of as a latecomer to the course?']"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(json_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "5d176dcb-9be3-413a-b624-966a145d7e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_question(doc):\n",
    "    prompt = prompt_template.format(**doc)\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        model=\"llama3-8b-8192\",\n",
    "    )\n",
    "    json_response = response.choices[0].message.content\n",
    "    return json_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "f72cdc5c-5e79-4138-889b-e111b3c888d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "453c0b95-7836-4f1c-bb65-4fbd404b2cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████| 948/948 [39:39<00:00,  2.51s/it]\n"
     ]
    }
   ],
   "source": [
    "question_list = {}\n",
    "for doc in tqdm(documents):\n",
    "    doc_id = doc['id']\n",
    "    if doc_id in question_list:\n",
    "        continue\n",
    "    questions = generate_question(doc)\n",
    "    question_list[doc_id] = questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "24643540-4d25-4f32-bf2b-a534c7b82f2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'c02e79ef': '[\"When will the course, which is focused on technical questions, officially begin?\", \"Can I still register for the course after it has started?\", \"What platform should I use to stay updated on the course schedule?\", \"Will the course be available on multiple devices or just on desktop?\", \"How can I communicate with other students and the instructor during the course?\"]',\n",
       " '1f6520ca': '[\\n\"What are the prerequisites for this course?\",\\n\"Where can I find information about the course schedule?\",\\n\"What kind of support does the course offer for students who struggle with certain topics?\",\\n\"How do I access the course materials and resources?\",\\n\"Are there any specific software or tools required for the course?\"',\n",
       " '7842b56a': '[\"What can I do if I miss the course start date and still want to participate?\", \"If I don\\'t register for the course, am I still allowed to complete the homeworks?\", \"What deadlines do I need to be aware of for the final projects?\", \"Can I still be part of the course even after the registration deadline has passed?\", \"Is there a specific time limit to submit the homeworks and projects in case I join late?\"]',\n",
       " '0bbf41ec': '[\"What date can I expect to receive a confirmation email for registering for the Data Engineering Bootcamp?\", \"Can I just start learning and submitting homework without receiving a confirmation email?\", \"What is the purpose of registering for the Data Engineering Bootcamp if it\\'s not checked against any registered list?\", \"Do I need to receive a confirmation email after registering for the Data Engineering Bootcamp?\", \"Are the registration and start dates the same for the Data Engineering Bootcamp?\"]',\n",
       " '63394d91': '[\"What are the dependencies and requirements I should install before the course starts?\", \"Can I do something to prepare for the course before it begins?\", \"Do I need to have prior knowledge of the subjects listed in the prerequisites?\", \"What if I\\'m not sure if I\\'m comfortable with the subjects mentioned in the syllabus?\", \"Are there specific ways to go over the prerequisites and syllabus for better understanding?\"]',\n",
       " '2ed9b986': 'Here is the output in parsable JSON format:\\n\\n[\"How many Zoom Camps are in a year?\", \"What are the start and end dates for the Data-Engineering Zoomcamp?\", \"Are the Zoom Camps limited to a specific course?\", \"Can I take any Zoom camp at any time, or do I have to follow the live cohort schedule?\", \"How often are the Data-Engineering Zoomcamps held?\"]',\n",
       " '93e2c8ed': '[\\n  \"What changes will be made to the current cohort compared to the previous one in the 2024 edition of the course?\",\\n  \"Will the 2024 edition still use the same tools and software as the previous cohort?\",\\n  \"Is it true that the course is using new AI technology instead of Prefect in the 2024 edition?\",\\n  \"What led to the replacement of Prefect with Mage AI in the 2024 edition of the course?\",\\n  \"How will the re-recorded terraform videos impact the learning experience in the 2024 edition?\"\\n]',\n",
       " 'a482086d': '[\"Can I follow the course even after it finishes?\", \"After the course finishes, will all the materials still be available?\", \"Are there any benefits to finishing the course quickly, or can I work on it at my own pace?\", \"What should I focus on during the course if I want to prepare for the next cohort?\", \"Can I start working on my final capstone project at any time during the course?\"]',\n",
       " 'eb56ae98': '[\"Can I get support if I take the course in a different mode?\", \"What kind of information can I find in this document?\", \"How can I ensure I\\'m not missing information already answered?\", \"How can I get the bot to help me with my search?\", \"Can I rely solely on the bot\\'s answers?\"]',\n",
       " '4292531b': '[\"What is the correct YouTube playlist to refer to for the main course videos?\", \"How do I navigate the GitHub repository for the videos?\", \"What is the purpose of the year-specific playlists?\", \"Why are additional videos uploaded to the year-specific playlists?\", \"How do I find the correct playlist on the Slack channel?\"]',\n",
       " 'ea739c65': '[\"What are the expected weekly hours of engagement for students in this course, considering my background and previous experience with modules similar to its modules?\", \"Can I calculate the expected weekly hours myself based on the data provided, or is this information only available from an external source?\", \"What should I do if I realize I need to dedicate more time to the course than initially anticipated?\", \"Will the course material be so demanding that I\\'ll need to require more hours than the initial expectation of 5-15 hours per week?\", \"Can the expected weekly hours for the course increase or decrease over time, or will they remain constant throughout the course?\"]',\n",
       " 'cb257ee5': '[\"How can I follow the course if I\\'m unable to commit to a live cohort?\", \"Can I still earn a certificate if I\\'m taking the course at my own pace?\", \"Will I be able to review my peers\\' projects in a self-paced mode?\", \"Can I consider the self-paced mode as completing the course requirements?\", \"Will the certificate be available once I have completed the self-paced mode?\"]',\n",
       " '04aa4897': '[\"What is the video/zoom link to the stream for the “Office Hour” or workshop sessions?\", \"How do students participate in the live sessions?\", \"How do I submit my questions during the live sessions?\", \"Will I see the live sessions on the DataTalksClub YouTube Channel?\", \"Should I post my questions in the chat during the live sessions?\"]',\n",
       " '9681be3b': '[\"What if I can\\'t attend the Office hours workshop session, will the recordings be available?\", \"Will the Office hours sessions be recorded and available online?\", \"What will happen if I miss an Office hours lecture?\", \"Will the recording of Office hours be available to us?\", \"Will the live Office hours be recorded and available afterwards?\"]',\n",
       " 'a1daf537': '[\"What are the responsibilities that come with being an online student?\", \"How do I know when homework or project deadlines are approaching?\", \"Where can I find the updated homework and project deadlines?\", \"Can I trust the deadlines provided on the website?\", \"Are there any extensions granted for late submissions?\"]',\n",
       " 'be5bfee4': '[\"Are late submissions of homework allowed in the course?\", \"Is it possible to still submit finished homework after the due date if the submission form is still open?\", \"What is the policy on accepting late homework submissions?\", \"Can I still submit my homework if the submission form is still open past the deadline?\", \"How can I confirm that my late homework submission was received?\"]',\n",
       " '0e424a44': '[\"What is the purpose of my repository on GitHub, GitLab, or BitBucket for homework?\", \"Can I use any other location for my code besides the link provided?\", \"What makes a \\'reasonable person\\' think I have completed the week\\'s exercises?\", \"Is my GitHub, GitLab, or BitBucket repository the only place where I can upload my homework?\", \"How do I know if my homework URL is correct?\"]',\n",
       " '29865466': '[\"What is the system for points in the course management platform that I can use to submit my homework?\", \"How do I determine how many points I have for each specific homework submission?\", \"What does the leaderboard show regarding points earned in the course management platform?\", \"How many points do I earn for submitting something to the FAQ section?\", \"For learning in public links, how many points do I earn for each?\"]',\n",
       " '016d46a1': '[\"How do I get on the leaderboard?\", \"Why am I not on the leaderboard?\", \"What\\'s my display name?\", \"How can I see my leaderboard?\", \"Can I change my display name?\"]',\n",
       " '47972cb1': '[\"Is Python 3.9 still the recommended version to use in 2024 considering the simplicity and stability it provides?\", \"What if I use a different Python version such as 3.10 and 3.11, would that be okay?\", \"Why do I need to use a specific version of Python?\", \"Are there any changes to the recommended Python version that I should know about?\", \"Would using a newer or older version of Python cause issues with the course material?\"]',\n",
       " 'ddf6c1b3': '[ \"What environment setup should I use if I don\\'t want to face challenges, especially for Windows users?\", \"Can I set up the environment on my laptop or PC?\", \"Should I use Docker for the week 1 Introduction?\", \"Can I work on the environment virtually from different locations, like home and office?\", \"Is it viable to use a cloud-based virtual machine instead of setting it up locally?\" ]',\n",
       " 'ac25d3af': 'Here are the 5 questions:\\n\\n[\"Is GitHub codespaces an alternative to using cli/git bash to ingest the data and create a docker file?\", \"Can we use GitHub repository in Linux resources with pre-installed tools?\", \"What kind of tools does GitHub Codespaces provide?\", \"Can I open any GitHub repository with GitHub Codespaces?\", \"Do GitHub Codespaces have Docker and Docker Compose pre-installed?\"]',\n",
       " '251218fc': '[\"What are the exact requirements for using a Linux environment for the course?\", \"Are PostgreSQL & Docker the only allowed database and containerization tools?\", \"Do we need to use a specific version of PostgreSQL?\", \"Is GCP VM the only cloud platform we can use for the course?\", \"Can we use a personal laptop or computer for the course?\"]',\n",
       " '3c0114ce': '[\\n\"Can I use both GitHub Codespaces and Google Cloud Platform (GCP) for the course, or do I need to choose between them?\",\\n\"What is the preferred option for the end project, using GitHub Codespaces or GCP?\",\\n\"Is it necessary to have both GitHub Codespaces and GCP for this course, or can I use one of them?\",\\n\"Which option is generally more beneficial for learning, GitHub Codespaces or GCP?\",\\n\"Can I set up a local environment instead of using GitHub Codespaces or GCP for this course?\"',\n",
       " 'f43f5fe7': 'Here are the 5 questions based on the FAQ record:\\n\\n[\"Why do we need to open Run command window when attempting to connect to a GCP VM using VSCode on a Windows machine?\", \"What are the ways to open Registry Editor?\", \"How do we change the registry values in \\'HKEY_CURRENT_USER\\\\Software\\\\Microsoft\\\\Command Processor\\'\", \"What are the alternative methods to solve the GCP VM connection issue in VSCode?\", \"Where is the known_hosts file located in Windows?\"]',\n",
       " 'd061525d': '[\\n\"What are the reasons behind choosing Google Cloud Provider (GCP) over other cloud providers, and what are the benefits?\",\\n\"Are we restricted to using only GCP, or can we use other cloud platforms like AWS?\",\\n\"Why does everyone having a Google account make a difference in the course?\",\\n\"What are the available free trial periods and credits for new users, and how can we sign up for a free GCP account?\",\\n\"What services will be provided by other cloud platforms like Azure and AWS if we are comfortable using them?\"',\n",
       " '1cd01b2c': '[\\n\"What if I encounter trouble with GCP\\'s free trial?\", \\n\"Can I trust the accuracy of the course materials?\", \\n\"Is it necessary to use specific software for the course?\", \\n\"How can I be sure the course materials are up-to-date?\", \\n\"Are there any recommended resources outside of the course?\"',\n",
       " 'e4a7c3b0': '{\"questions\": \\n[\"What is the possibility of getting a guide to setting up a home lab, considering GCP and other cloud providers are not accessible in some countries?\", \\n\"What are some viable alternatives to using GCP for the coursework?\", \\n\"Would it be possible to complete the majority of the course without a cloud provider?\", \\n\"Is there a local alternative for every topic covered in the course?\", \\n\"Can the entire course be completed without relying on GCP or any other cloud provider?\"]}',\n",
       " '7cd1912e': '[\\n\"Can I use Amazon Web Services (AWS) for the environment setup in the course, or is there a recommended platform?\",\\n\"I\\'ve decided to use AWS for the environment setup, but will I still be able to get help from instructors and fellow students, since it\\'s not the recommended platform?\",\\n\"Since the course emphasizes Google Cloud Platform (GCP), will I be disadvantaged in the final capstone project if I choose to use AWS?\",\\n\"Can I adapt the tutorials and information on the videos to use AWS instead of GCP?\",\\n\"While I\\'m using AWS, can I still get help from instructors and fellow students, or will I be mostly relying on those who also use AWS?\"',\n",
       " '52393fb3': '[\\n\"When are the live Zoom calls besides the \\'Office Hour\\'?\",\\n\"Are there any other live Zoom calls besides the \\'Office Hour\\' and how often will they occur?\",\\n\"Will there be additional live Zoom calls during the course and if so, how will we be notified?\",\\n\"Are there any more live Zoom calls scheduled besides the \\'Office Hour\\'?\",\\n\"Can you elaborate on the live Zoom calls that will happen during the Capstone period?\"\\n]',\n",
       " '10515af5': '[\"Will we be using updated data for the NYC Trip project, considering 2022 has already started?\", \"Are we continuing with the same data from last year\\'s project?\", \"Will we still be analyzing the January 2021 data for the NYC Trip project?\", \"Can we confirm if we\\'ll be using the same dataset as last year?\", \"Will we be looking at the same data sets for both years\\' projects?\"]',\n",
       " 'cdb86a97': '[\"Is the 2022 course material still available?\", \"Do I need to do any specific programming languages for this course?\", \"What kind of tools will we be using in this course?\", \"Is the grading system the same as last year\\'s course?\", \"Will I have access to the 2022 project template?\"]',\n",
       " '3e0114ad': '[\"What are some alternative tools I can use for the final project, similar to Airflow?\", \"Is it necessary to use the recommended tools for the project, or are there other options available?\", \"Can I use a different workflow management tool for the project?\", \"Are there specific features that a tool for the project should have?\", \"Are there any tools that would be particularly difficult to work with for the project?\"]',\n",
       " 'b2799574': '[\"What are the alternatives to using Mage in the course?\", \"Can I use Airflow or Prefect instead of the recommended tool?\", \"Are there any alternatives to using GCP products?\", \"Can I use Tableau instead of Metabase or Google data studio?\", \"Can I use a different data stack than the ones recommended in the course?\"]',\n",
       " '2f19301f': '[\"How can I contribute to the course materials if I think they\\'re useful?\", \"Can I share the course with my friends or classmates if I find it helpful?\", \"What if I spot errors or propose improvements to the course content?\", \"Can I create a pull request if I see ways to enhance the repository?\", \"How can I make my peers know about the course if I think it\\'s valuable?\"]',\n",
       " '7c700adb': '[\\n\"How does the course fare with different operating systems, such as Windows or Mac, considering it mentions Linux as ideal?\",\\n\"Will I have any difficulties while using Linux as the course seems to favour it?\",\\n\"Is it possible to use my Windows/Mac for this course without any tech issues?\",\\n\"Can I use either Windows, Mac, or Linux for this course without facing any compatibility issues?\",\\n\"What technical specifications are required to follow this course?\"',\n",
       " '44b14808': '[\"What happens when Windows users encounter issues with shell scripts in modules with *.sh files as they move forward in the course?\", \"Why do later modules rely so heavily on shell scripts that might not work for most Windows users?\", \"Is there a recommended setup for Windows users to run shell scripts in modules with *.sh files if they\\'re not using WSL?\", \"Can we get some guidance on how to navigate the potential roadblocks for Windows users in these modules?\", \"Are there any tips or workarounds for Windows users who want to progress in the course despite the issues with shell scripts?\"]',\n",
       " '76e4baf6': '[\"What are some necessary prerequisites to take this course?\", \"Are there any specific programming languages or software I should be familiar with before starting the course?\", \"Are there any online tutorials or courses that can help me prepare for the course?\", \"What is the best way to approach the course material, and how can I make sure I\\'m on track with the lessons?\", \"What additional resources should I take a look at, aside from the course materials?\"]',\n",
       " '48b533a8': '[\\n\"What is the purpose of having a first and second attempt for the project submissions?\",\\n\"How do the deadlines for the first and second attempts work?\",\\n\"Can I submit a project on the first attempt but still have a chance to reassess after the grade is given?\",\\n\"What happens if I submit a project on the first attempt and it\\'s deemed unsatisfactory? Do I still have a chance to resubmit?\",\\n\"Are there any specific requirements I need to meet for my project to be accepted on the second attempt?\"',\n",
       " '954044d1': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\"What are the first steps to troubleshoot issues I\\'m encountering in the course?\", \"Can I search for solutions to my problem using specific keywords?\", \"What should I do if I\\'m unable to resolve an issue after trying the troubleshooting steps?\", \"How do I properly ask a question on Stackoverflow and other platforms?\", \"What should I do if I\\'m stuck on a problem and can\\'t seem to find a solution?\"]',\n",
       " 'a820b9b3': '[\"What steps can I take if the video tutorial I\\'m following doesn\\'t help me solve my problem?\", \"How do I know if I\\'m providing enough information when asking for help?\", \"Can I ask for help on a specific programming language I\\'m using with this course?\", \"What if I don\\'t understand something in this course, how can I clarify it?\", \"I\\'m getting an error message, how can I make sure I provide the most relevant information to the instructors?\"]',\n",
       " 'f2945cd2': '[\\n\"How do I set up a local repository on my computer for this course?\",\\n\"How do I ignore large database and other unwanted files when using Git?\",\\n\"What is the best way to store my notes and versions of files for this course?\",\\n\"Should I store my passwords and keys in a Git repository?\",\\n\"Are there any other resources you would recommend for learning Git and GitHub?\"\\n]',\n",
       " 'eb9d376f': '[\"How do I fix the error \"Error: Makefile:2: *** missing separator.  Stop.\" when trying to convert tabs in my document to spaces in VS Code?\", \"Why do I have to convert tabs to spaces in VS Code?\", \"Is it possible to use both tabs and spaces in VS Code?\", \"How can I solve the issue of tabs not being recognized as tabs in my document?\", \"Is there a specific stack or resource I should refer to for information on converting tabs to spaces in VS Code?\"]',\n",
       " '72f25f6d': '[\\n\"How do I open an HTML file from my Linux running on WSL with a Windows browser?\",\\n\"Why can\\'t I open HTML files from my Linux directory directly in my Windows browser?\",\\n\"Is there a specific way to configure my browser from within the Linux subsystem?\",\\n\"How do I choose which browser to use when opening an HTML file from WSL?\",\\n\"Can I customize the browser settings for WSLview?\"',\n",
       " 'a1e59afc': 'Here are the 5 questions based on the FAQ record:\\n\\n[\"What is the purpose of setting up Chrome Remote Desktop on a Debian Linux virtual machine instance on Compute Engine?\", \"Why do I get an error downloading 2021 yellow taxi trip records from the TLC website?\", \"What alternative link can I use to download the 2021 yellow taxi trip records?\", \"How can I uncompress the downloaded \\'gz\\' file?\", \"What is the correct command to uncompress the \\'gz\\' file?\"]',\n",
       " '71c10610': 'Here is the list of questions:\\n\\n[\"How to handle taxi data files now that the files are available as *.csv.gz?\", \"Why can\\'t the data file store correctly if the file extension is csv.gz instead of csv?\", \"How do we parse the file name from the URL and use it as csv_name?\", \"Can the pandas read_csv function read csv.gz files directly?\", \"What is the alternative solution when storing the data file as “output.csv”\"]',\n",
       " '17a5aea1': '[\\n\"How do I access the Data Dictionary for NY Taxi data?\", \\n\"Where can I find the Yellow Trips Data Dictionary?\", \\n\"Can I access the Green Trips Data Dictionary through the same resource?\", \\n\"What is provided in the Data Dictionary for NY Taxi data?\", \\n\"Are there separate Data Dictionaries for yellow and green trips?\" \\n]',\n",
       " '5a275db7': '[\\n\"Can I just unzip a parquet file in the command line and import it with pandas?\",\\n\"How do I convert a downloaded parquet file directly to a csv file?\",\\n\"In the video, it shows pd.read_csv() function, can I use it to directly read the parquet file?\",\\n\"Can I modify the ingest_data.py code without running it directly in the Python script?\",\\n\"How do I add a specific file name to the downloaded parquet file in the ingest_data.py code?\"\\n]',\n",
       " '7ec0f9b0': 'Here is the list of 5 questions in parsable JSON format:\\n\\n[\"What is the problem when I run lwget is not recognized as an internal or external command?\", \"How do I install wget on Ubuntu?\", \"How do I install wget on MacOS?\", \"How do I install wget on Windows using Chocolatey or a binary?\", \"Is there an alternative to installing wget?\"]',\n",
       " 'bb1ba786': 'Here are 5 questions based on the FAQ record:\\n\\n[\"Why do I get an ERROR when using wget with a website URL on MacOS?\", \"Can I still download files with wget on MacOS even if the website\\'s certificate is invalid?\", \"Is there a way to use wget to download files without verifying the website\\'s certificate?\", \"I\\'m running my command in a Jupyter Notebook, how can I fix the error?\", \"Why can\\'t I use wget to download a file directly from the command line on MacOS?\"]',\n",
       " '2f83dbe7': 'Here are the 5 questions:\\n\\n[\\n\"How can I use the backslash as an escape character in Git Bash for Windows?\",\\n\"What is the recommended way to set up Terraform with Docker on Windows?\",\\n\"Can I use Docker containers to deploy my web application?\",\\n\"How do I run multiple commands in a Docker container?\",\\n\"Can I use Git Bash to interact with my Terraform state?\"',\n",
       " '543ff080': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\"Are there specific instructions for storing secrets for GitHub Codespaces?\", \\n\"What resources does GitHub provide for managing account-specific secrets in Codespaces?\", \\n\"Is there a particular way I should store my secrets so they can be accessed in GitHub Codespaces?\", \\n\"How do I properly manage my account-specific secrets for GitHub Codespaces?\", \\n\"Can you provide more information on storing secrets that will be available in GitHub Codespaces?\"]',\n",
       " 'd407d65b': '[\"Why does my Docker connection keep failing?\", \"How do I check the Docker daemon is running?\", \"What should I do if I encounter issues when trying to connect to the Docker daemon?\", \"How can I fix the connection issue in WSL?\", \"What is the command to update WSL in PowerShell?\"]',\n",
       " 'c9375c56': 'Here is the list of questions based on the FAQ record:\\n\\n[\"Why do I need to run the Docker client with elevated privileges on Windows?\", \"How do I enable Hyper-V as the backend for Docker on Windows 10 Pro/11 Pro?\", \"What option do Windows 10 Home/11 Home users have for running Docker if Hyper-V is not available?\", \"How can I update the WSL2 Linux Kernel if I run into an issue during installation?\", \"Can you provide a tutorial on how to install WSL2 on Windows 11?\"]',\n",
       " 'e866156b': 'Here are the 5 questions:\\n\\n[\"Why does docker pull attempt to fetch the given image name from a repository, and what\\'s the difference between a public and a private repository? \",\\n\"What happens if a typo is present in the image name when attempting to perform a docker pull? \",\\n\"Why do I need to execute docker login if I\\'m trying to fetch a Docker image from a private repository that I have access to? \",\\n\"Can you explain why I\\'m encountering a \\'permission denied\\' error when creating a PostgreSQL Docker container with a mounted volume on macOS M1? \",\\n\"What are the steps to resolve the \\'permission denied\\' error when creating a PostgreSQL Docker container with a mounted volume on macOS M1, particularly when using Rancher Desktop? \"]',\n",
       " '16370470': '[\"What can I do if I can\\'t delete a local folder that was mounted to a Docker volume?\", \"How do I delete a folder inside a Docker volume that\\'s protected by write and read permissions?\", \"Why can\\'t I delete a folder if it\\'s owned by a user like 999?\", \"How do I start my obisidian once the Docker volume is deleted?\", \"What command can I use to force delete a folder in my local machine?\"]',\n",
       " '316df755': '[\"Why does Docker won\\'t start or get stuck in settings on my Windows 10 or 11 computer?\", \"How can I ensure I\\'m running the latest version of Docker for Windows?\", \"What can I do if Docker is stuck on starting?\", \"Can I use Docker on the Pro Edition of Windows 10 or 11?\", \"How do I enable Hyper-V or WSL2 as the backend for Docker on my Windows 10 or 11 Pro computer?\"]',\n",
       " 'f3aa9252': 'Here are the 5 questions based on the FAQ record:\\n\\n[\\n\"Should I run docker commands from the windows file system or a file system of a Linux distribution in WSL\",\\n\"What are the tutorials I need to follow to make WSL2 work with Docker?\",\\n\"What happens if Docker remains stuck even after setting up WSL2 correctly?\",\\n\"Is there an option to reset Docker to its default settings if it\\'s not working?\",\\n\"What\\'s an alternative to WSL2 for running Docker?\"',\n",
       " 'a4abe7a5': '[\"Can we store all our code in the default Linux distro when running Docker on WSL2 backend for Windows 10 Home and Windows 11 Home users for better file system performance?\", \"Where can we find more information on the best practices for using Docker?\", \"Why is it recommended to use Docker on WSL2 backend for Windows 10 Home and Windows 11 Home users?\", \"What kind of performance benefits can we expect by using Docker on WSL2 backend on Windows 10 Home and Windows 11 Home machines?\", \"Should we use the default Linux distro as a file system for Docker containers on Windows 10 Home and Windows 11 Home systems, and what are the implications of doing so?\"]',\n",
       " 'fb930700': '[\"What is causing the \\'The input device is not a TTY\\' error when running Docker commands on Windows?\", \"How can I make the Docker command work with a TTY on Windows?\", \"Is there a way to make the Docker command \\'tty\\' device work on Windows without prefixing each command with \\'winpty\\'?\", \"What\\'s the purpose of using \\'winpty\\' before a Docker command on Windows?\", \"Can I make a permanent fix to the Docker command on Windows by adding a command line option or configuration file?\"]',\n",
       " 'aa187680': '[\\n\"How do I resolve the error when trying to pip install on a Docker container under Windows?\",\\n\"Why can\\'t I pip install on a Docker container running under Windows?\",\\n\"What can I do when pip install fails in a Docker container due to a connection error?\",\\n\"How do I troubleshoot the issue of pip not installing dependencies in a Docker container on Windows?\",\\n\"Why does pip fail when trying to install packages in a containerized environment under Windows?\"',\n",
       " 'b000e899': '{\\n\"[\"Docker - ny_taxi_postgres_data does not have the expected data after running the script. What could be the issue?\\\\\"\", \\n\"Docker - Why is the ny_taxi_postgres_data folder still empty after running the script on Windows?\\\\\"\", \\n\"When running the docker script, the ny_taxi_postgres_data folder in Vs-code is empty. How do I fix this issue?\\\\\"\", \\n\"How do I troubleshoot if the Docker container is not accessing the ny_taxi_postgres_data folder correctly?\\\\\"\", \\n\"What is the solution to fix the empty ny_taxi_postgres_data folder issue when using Docker on Windows?\"]\\n}',\n",
       " '9c66759f': '[\\n\"What is the process for setting up Docker on a Mac, and is the method mentioned in the FAQ still reliable?\",\\n\"How do I know if Docker has changed its licensing model and if it affects the setup process?\",\\n\"Is it safe to download Docker from a third-party website instead of the official Docker website?\",\\n\"Can I still use Docker on a Mac if I have an outdated operating system?\",\\n\"Are there any specific requirements or dependencies needed to run Docker on a Mac?\"\\n]',\n",
       " 'e3106e07': 'Here are the 5 questions a student might ask based on the FAQ record:\\n\\n[\"Could Docker really not change permissions of the directory \"/var/lib/postgresql/data\"?\", \"Why did the PostgreSQL initialization fail with an \\'operation not permitted\\' error?\", \"How do I create a local Docker volume and map it to the postgres data directory?\", \"Why is the ny_taxi_postgres_data folder empty after creating the volume?\", \"What if I get an \\'initdb: error: directory exists but is not empty\\' error while creating a new database system?\"]',\n",
       " '72229da5': 'Here is the list of questions based on the FAQ record:\\n\\n[\"Why do I need to move my data to a folder without spaces on Windows?\", \"What are the different options to replace the \\'-v\\' part when mounting volumes on Windows?\", \"How do I add quotes to the \\'-v\\' part when mounting volumes on Windows?\", \"Why does the volume mapping not work automatically and what are the possible solutions?\", \"Can I use a volume name instead of a path when mounting volumes?\"]',\n",
       " '58c9f99f': '[\"What are the possible solutions to resolve the Docker error \\'invalid mode\\' when mounting a PostgreSQL volume?\", \"How can I change the mounting path in Docker to fix the error \\'invalid mode\\'?\", \"Can I use a windows path as a mounting path in Docker, and if so, what is the correct syntax?\", \"How do I resolve the \\'invalid mode\\' error in Docker when mounting a PostgreSQL volume on a Windows system?\", \"Is there a specific way to specify the mounting path in Docker to avoid the \\'invalid mode\\' error?\"]',\n",
       " 'bc42139a': '[\"What happens when I run the Docker command for a second time and get an error message saying that it can\\'t create a build with the previous mounted state?\", \"How do I resolve the issue of Docker complaining about a mounted source path not being able to be created?\", \"Can I still use the PostgreSQL user, password, and database with the simplified command, or do I need to specify these separately?\", \"What happens if I try to mount the same path twice when running the Docker command?\", \"Can I omit the -v option completely when trying to start the Docker container a second time?\"]',\n",
       " 'a146e3ee': '[\\\\\"What could be the reason for the Docker build error \\'error checking context: \\'can\\'t stat \\'/home/user/repos/data-engineering/week_1_basics_n_setup/2_docker_sql/ny_taxi_postgres_data\\'\\' when running docker build -t taxi_ingest:v001?\\\\\",\\n\"Could I use a different directory for running the docker build command when fixing the error?\",\\n\"How to change the permission of the directory on Ubuntu to fix the problem?\",\\n\"Can you provide more information on how to take ownership of a file/folder in Windows to fix the problem?\",\\n\"What is the underlying permission issue that caused this Docker build error?\"]',\n",
       " '593a85ba': 'Here are the 5 questions:\\n\\n[\"What happens when I get an ERRO[0000] error while waiting for a Docker container?\",\\n\"How do I check if Docker is installed correctly on my system?\",\\n\"What should I do if I encounter an error when trying to uninstall Docker via snap?\",\\n\"What is causing the Bind for 0.0.0.0:5432 to fail with an error saying the port is already in use?\",\\n\"Why do I get an error when trying to run \\'snap status docker\\'?\"]',\n",
       " '50bd1a71': '[\"What is the common mistake that can cause an error when trying to build a Docker image?\",\"How can I resolve the \\'can\\'t stat\\' error when building a Docker image?\", \"What is causing the error \\'can\\'t stat\\' in Docker build context?\", \"How do I give authorization rights to a host folder in Docker?\", \"What is the solution to add permission to a folder in Linux?\"]',\n",
       " 'f409f751': 'Here are the 5 questions:\\n\\n[\"How to resolve the \\'failed to solve with frontend dockerfile.v0: failed to read dockerfile: error from sender: open ny_taxi_postgres_data: permission denied\\' error when running the Docker command on Ubuntu/Linux systems?\",\\n\"What happens when trying to run the command to build the Docker container again on an Ubuntu/Linux system?\",\\n\"Can you explain why I get a permissions error when I run the build command on my Docker container?\",\\n\"What command can I use to grant permissions to the folder hosting the Docker files?\",\\n\"How can I check if I have sufficient permissions to rebuild the pipeline or create a new Docker container?\"]',\n",
       " '7d217da3': '[\"What is the command to get a list of Docker networks along with their IDs and status?\", \"How do I get the IP address of a container in a specific Docker network?\", \"Can you explain the concept of Docker volumes and their use case?\", \"Is there a way to map multiple ports from a container to a host machine?\", \"How can I get the Docker network name and use it as an input for Terraform configuration?\"]',\n",
       " '09081824': 'Here is the list of questions:\\n\\n[\"What if I encounter an error that says a container name is already in use by a container?\", \"Why does the error message \\'Conflict. The container name \"pg-database\" is already in use by container “xxx”\\' appear when restarting a Docker image?\", \"How can I stop a Docker container that\\'s already running without removing it?\", \"What\\'s the difference between using \\'docker run\\' and \\'docker start\\' to restart a Docker image?\", \"Is it possible to rename a Docker container if it\\'s already in use by another container?\"]',\n",
       " '4df80c55': '[\\n\"What happens when trying to translate a host name using docker-compose?\",\\n\"Why do I get an error when trying to translate a host name \\'pgdatabase\\' in a Docker environment?\",\\n\"What is the typical error I would encounter when trying to translate a host name when using Docker compose?\",\\n\"What can I do to resolve the \\'could not translate host name\\' error when using Docker compose?\",\\n\"How can I determine the correct network and database name when running Docker compose?\"',\n",
       " '3aee7261': '[\\n\"How do I install Docker on MacOS/Windows 11 VM running on top of Linux, since I\\'m getting an error due to nested virtualization?\",\\n\"What is the command I should run before starting my VM to enable nested virtualization on Intel CPU?\",\\n\"Can you provide the same command for AMD CPU to enable nested virtualization before starting the VM?\",\\n\"I\\'m running a Linux VM on top of Windows and getting a Docker installation error. Do you have any specific solution for this case?\",\\n\"Is there any workaround to bypass the nested virtualization issue when trying to install Docker on a MacOS/Windows 11 VM hosted on Linux?\"',\n",
       " '6497b659': '[\"How can I manage my Docker container, images, network and compose projects from VS Code?\", \"Is it possible to stop a Docker container?\", \"How do I install the official Docker extension in VS Code?\", \"Can VS Code connect with Docker that runs on WSL2?\", \"Can you provide the command to stop a Docker container?\"]',\n",
       " 'a02f2039': '[\"What are the potential causes of a PostgreSQL database directory appearing to contain a database, but the database system being shut down?\", \"How do I troubleshoot a PostgreSQL connection failure where the server closed the connection unexpectedly?\", \"What happens when a PostgreSQL database system is terminated abnormally before or while processing a request?\", \"Why do I need to delete the directory with data and restart the container when troubleshooting a PostgreSQL issue?\", \"How do I map the directory to the container using the -v flag in Docker?\"]',\n",
       " 'c6db65aa': '[ \"What are the procedures to install Docker on Ubuntu if the standard methods fail?\", \"Why does Docker installation on Ubuntu depend on the Ubuntu version?\", \"Can snap commands be used for installing Docker on all Ubuntu versions?\", \"Do older versions of Ubuntu support the snap method for installing Docker?\", \"Can Terraform be installed using the same procedure as Docker on Ubuntu?\" ]',\n",
       " 'f476a606': '[\\n\"Docker-Compose mounting error? Why do I get Operation not permitted error when trying to change permissions of a directory when using Docker-Compose?\",\\n\"How can I specify a named volume in the Docker-Compose file?\",\\n\"What\\'s the problem with my Docker volume not being used when I\\'ve already defined it in the Compose file?\",\\n\"I\\'ve created a local Docker volume, but it\\'s not being used. How do I troubleshoot this issue?\",\\n\"After renaming a Docker volume, how do I force Docker Compose to use the new name instead of the old one?\"',\n",
       " 'e41b100c': '[\"How do I fix the error when trying to start Docker containers in detached mode?\", \"What does the error \\'Couldn\\'t translate host name to address\\' mean in the context of Docker-Compose?\", \"Why does the Docker-Compose command \\'docker-compose up -d\\' not work as expected?\", \"How do I view the logs of a specific container that\\'s running in Docker?\", \"How do I list all containers, including stopped ones, using the Docker command \\'docker ps\\'?\"]',\n",
       " 'cd0f9300': 'Here are 5 questions based on the FAQ record:\\n\\n[\"What happens to database data when executing `docker-compose up`?\", \"Why do I get a hostname translation error when running my Ingestion script?\", \"How do I find the default network name created by Docker Compose?\", \"What are some alternative tools to use if I\\'m experiencing issues with pgcli?\", \"How can I resolve the issue if my Ingestion script still doesn\\'t work after finding the default network name?\"]',\n",
       " '7f845a1c': 'Here are the 5 questions that the student might ask based on the FAQ record:\\n\\n[\"What is the error message when trying to run docker-compose with a hostname that contains a dash (-)?\",\\n\"How do I see all the stopped and running containers if docker-compose up -d fails?\",\\n\"Why do I get the error \\'network not found\\' when trying to connect to a server on localhost:8080?\",\\n\"What is the best approach to specify the docker network in the docker-compose.yml file?\",\\n\"Can I use a hostname with a dash (-) in the environment variables in docker-compose.yml?\"]',\n",
       " '36e54439': 'Here is the list of questions based on the FAQ record:\\n\\n[\\n\"Docker-Compose - What happens to the data if I run it on Google Cloud Platform (GCP)?\",\\n\"What advantage do Docker Volumes bring when using Docker-Compose on GCP?\",\\n\"What kind of Volumes are used when persisting PGAdmin contents on GCP?\",\\n\"Why are persistent backups important when working with PGAdmin and Docker-Compose on GCP?\",\\n\"How do Docker Volumes overcome the issue of data persistence on GCP when using Docker-Compose?\"\\n]',\n",
       " '32e8450c': '[\\n\"Since the Docker engine keeps crashing and failing to fetch extensions, what could be the root cause of this issue?\",\\n\"When I restart the Docker engine, the problem persists, and the failure to fetch extensions still occurs. What can I do?\",\\n\"Docker engine stopped Failed to fetch extensions continues to appear on the screen. What is the step-by-step solution to resolve this?\",\\n\"I\\'ve tried restarting the Docker engine, but the issue remains. Are there any other troubleshooting steps I can take?\",\\n\"After reinstalling Docker, do I need to fetch images again, or can I restore the old configuration?\"]',\n",
       " '96606db2': 'Here are the 5 questions based on the FAQ record:\\n\\n[\\n\"How do I persist pgAdmin configuration, specifically the server name?\",\\n\"What command do I use to give the pgAdmin container access to write to the \\'pgAdmin_data\\' folder?\",\\n\"What is the recommended way to set up the \\'volumes\\' section for pgAdmin in Docker Compose?\",\\n\"Can you provide an example of how to define the correct \\'volumes\\' and \\'ports\\' settings for pgAdmin in Docker Compose?\",\\n\"What is the significance of the \\'PGADMIN_DEFAULT_EMAIL\\' and \\'PGADMIN_DEFAULT_PASSWORD\\' environment variables when setting up pgAdmin in Docker Compose?\"',\n",
       " '0882bfac': 'Here are 5 questions a student might ask based on the FAQ record:\\n\\n[\"What happens if I don\\'t create the docker group and add my user?\", \"How can I avoid setting up my database connection every time I fire up the containers?\", \"How do I overcome the permission denied error while using Docker-Compose?\", \"How do I create a volume for pgAdmin in my docker-compose.yaml file?\", \"How do I add a volume to my docker-compose.yaml file?\"]',\n",
       " '7d067f5c': '[\"What do I do if Docker-Compose is still not available after changing my .bashrc?\", \"How do I resolve issues with Docker-Compose not being available after completing the 1.4.1 video?\", \"Why am I not able to use the docker-compose command after installing Docker-Compose?\", \"Can I use the docker-compose file directly from GitHub instead of the command?\", \"How can I easily use the Docker-Compose command instead of the downloaded docker-compose file?\"]',\n",
       " 'ff352621': '[\"How do I resolve the error getting credentials after running docker-compose up -d?\", \"What are common Docker-Compose issues and how can we troubleshoot them?\", \"How do I install pass via the terminal?\", \"What is pass and why is it needed for Docker-Compose?\", \"Can you provide more information about the docker-compose error in the FAQ record?\"]',\n",
       " '2d653208': 'Here are the 5 questions based on the FAQ record:\\n\\n[\"What do I need to do if I\\'m experiencing problems with Docker compose and getting data into postgres?\", \"How do I create a new volume on Docker?\", \"What should I change in my docker-compose.yml file to resolve issues?\", \"How do I ensure that no other containers are running except the one I just executed in Docker?\", \"What\\'s the correct order of execution when setting up pgadmin and pgdatabase with Docker compose?\"]',\n",
       " 'f09ea61e': '[\"What happens when attempting to run \\'docker compose up -d\\' and receiving an error related to getting credentials?\", \"Why am I getting an \\'executable file not found in %PATH%\\' error with \\'docker-credential-desktop\\' when running Docker Compose?\", \"How can I fix the \\'docker-credential-desktop\\' file not found issue when using Docker Compose?\", \"What is the location of the config.json file that needs to be modified for Docker?\", \"What changes should I make to the \\'credsStore\\' value in the config.json file for Docker?\"]',\n",
       " 'fbd3d2bb': '[\\n\"Which Docker-Compose binary should I use when working with WSL, and how can I determine the correct one for my system?\",\\n\"What commands can I run to check my system and determine which Docker-Compose binary I need to download?\",\\n\"Can I use Docker-Compose with my Linux system? If so, how do I download the correct version?\",\\n\"How do I determine the correct \\'flavor\\' for my system using the \\'uname -m\\' command?\",\\n\"Can I use the command \\'sudo curl\\' to manually download and install Docker-Compose on my system?\"',\n",
       " '0b014d0c': '[\\n  \"What do I need to do if I am getting an \\'undefined volume\\' error with Docker-Compose on Windows/WSL, and my file is written exactly like the video?\",\\n  \"How can I fix the \\'service \"pgdatabase\" refers to undefined volume\\' error in Docker-Compose on Windows/WSL?\",\\n  \"What is the solution to the \\'invalid compose project\\' error I\\'m getting with Docker-Compose on Windows/WSL?\",\\n  \"How do I make Docker-Compose work on Windows/WSL if I\\'m getting an error about an undefined volume?\",\\n  \"What do I need to add to my docker-compose.yaml file to resolve the \\'undefined volume\\' error on Windows/WSL?\" ]',\n",
       " 'd21bff1d': 'Here is the list of questions:\\n\\n[\"Why do I get a WSL Docker directory permissions error when using the Windows file system?\", \"How do I resolve the issue with different permission management between WSL and Windows?\", \"What is the purpose of using Docker volumes in the context of our course?\", \"Why is using a local volume unnecessary in this case?\", \"How can I manage volumes better by using Docker volumes instead of a local drive?\"]',\n",
       " '6afb7b55': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\\n\"Is Docker the solution for Postgres Querying issues?\", \\n\"Why doesn\\'t pgadmin work easily for Running on git bash or vm in Windows?\", \\n\"Do the psycopg2 and libpq libraries need to be installed separately?\", \\n\"What is the exact solution for Postgres Querying issues when pgadmin doesn\\'t work?\", \\n\"Is there any specific command to install psycopg2 using Pip?\" \\n]',\n",
       " 'b51c3b82': ' [{\"Why does it happen when I try to use the requested service in WSL and what can I do to fix it?\", \"How can I go to the library of apps installed in the Microsoft Store?\", \"What should I search for in the Windows updates?\", \"How can I update the Windows Terminal and see the changes?\", \"How do I restart my system after installing Windows security updates?\" }]',\n",
       " '326af690': '[\\n  \"Why does WSL integration with distro Ubuntu sometimes stop unexpectedly with exit code 1 on Windows?\",\\n  \"How do I fix DNS issues in WSL on Windows which cause problems with Docker?\",\\n  \"What should I do if I encounter a \\'bash: conda: command not found\\' error in WSL on Windows?\",\\n  \"What message do I see if I try to start Docker without initializing the database and specifying the superuser password?\",\\n  \"How do I switch from running Docker as Windows containers to running it as Linux containers?\"\\n]',\n",
       " 'c2ec9047': 'Here is the list of questions based on the FAQ record:\\n\\n[\\n\"WSL2 is not detecting my .ssh keys in the correct folder\",\\n\"How can I fix the \\'permissions too open at Windows\\' issue when running the GPC VM through SSH through WSL2\",\\n\"What can I do if I get an error when trying to run a command in the terminal and get an error\",\\n\"Is using \\'sudo\\' before a command the only way to resolve the \\'permissions too open at Windows\\' issue\",\\n\"How can I set the correct permissions for my private key SSH file if changing it manually doesn\\'t work\"\\n]',\n",
       " '3b711e73': '[\\n\"How can I resolve WSL not referencing the correct .ssh/config path from Windows?\",\\n\"What is an alternative to creating a config file in the .ssh/ folder?\",\\n\"Can you provide an example of how to create a config file in the .ssh/ folder?\",\\n\"How do I specify the HostName and User fields in the config file?\",\\n\"What is the purpose of referencing the .ssh/ folder in the config file?\"',\n",
       " 'cfe07c9d': '[\\n\"How do I resolve the error message \\'PGCLI - connection failed: :1), port 5432 failed: could not receive data from server: Connection refused could not send SSL negotiation packet: Connection refused\\' when trying to connect to PostgreSQL using pgcli?\",\\n\"What is the correct format for the pgcli command line to connect to a PostgreSQL database?\",\\n\"How do I specify the hostname when connecting to a PostgreSQL database using pgcli?\",\\n\"What is the purpose of the -h option in the pgcli command\",\\n\"How do I connect to a specific PostgreSQL database using pgcli\"',\n",
       " 'acf42bb8': '[\"What happens when I run PGCLI with the --help option but get an error message\", \"Why do I keep getting a PGCLI --help error when trying to install Docker\", \"Is it possible to troubleshoot PGCLI installation errors\", \"Can I view the PGCLI help documentation when the --help option doesn\\'t work\", \"How can I resolve an issue where PGCLI --help provides no output\"]',\n",
       " '176ce516': '[\"What happens to the mapped port of the pgsql container and how does it affect running pgcli\", \"Can I access the contents of a running Docker container from my local machine\", \"Is it necessary to run pgcli inside a container to interact with the PostgreSQL database\", \"Can I create another Docker container that interacts with the same PostgreSQL database instance\", \"What are the implications of running multiple Docker containers on the same system with the same port\"]',\n",
       " '3e5d1e9b': 'Here are the 5 questions based on the FAQ record:\\n\\n[\"What can I do if I get the error \\'FATAL: password authentication failed for user \"root\" (You already have Postgres)\\' when using PGCLI?\", \"How do I know which application is using a specific port on my local machine?\", \"Why do I get this error when I try to connect to my Postgres docker container?\", \"What alternative port can I use when creating the docker container to avoid this error?\", \"How can I stop and start the running Postgres service on my local machine to free up the port?\"]',\n",
       " '78833f32': 'Here are the 5 questions based on the FAQ record:\\n\\n[\"How do I fix the PermissionError when installing pgcli?\", \\n\"What is the recommended approach to install pgcli without affecting my system Python?\", \\n\"Can you explain why I\\'m getting a PermissionError when using conda install ?\", \\n\"What are the alternatives if the conda install gets stuck at \\'Solving environment\\'?\", \\n\"Why is there a Traceback error when running pgcli and how can I resolve it?\"]',\n",
       " '63823f21': 'Here are the 5 questions based on the FAQ record:\\n\\n[\"Why do I get an ImportError: no pq wrapper available when trying to use PGCLI?\",\\n\"How can I check my current Python version and why is it important?\",\\n\"What can I do if I don\\'t have a Python version of 3.9 installed? Can I upgrade it or create a new environment\",\\n\"Can I install the lib for Postgres in the same way as I install other Python packages?\",\\n\"How do I install pgcli using conda and what are the necessary command-line options?\"]',\n",
       " 'b36ea564': 'Here is the list of questions:\\n\\n[\"If my Bash prompt is stuck on the password command for postgres, how can I use pgcli?\", \"Why do I keep getting the error PGCLI -connection failed: FATAL:  password authentication failed for user \\\\\"root\\\\\"\", \"How do I resolve the \\'password authentication failed\\' error when using pgcli?\", \"What\\'s the best way to connect to the Postgres:13 image in the previous step of the tutorial?\", \"What are some solutions to resolve the \\'password authentication failed\\' error when using pgcli?\"]',\n",
       " 'e2a46ce5': 'Here are 5 questions based on the FAQ record:\\n\\n[\"How do I resolve the pgcli: command not found issue on my Windows machine?\", \"Why does bash not recognize pgcli even after installation?\", \"What causes the error \\'pgcli: command not found\\' on Git bash?\", \"How do I find the correct path for pgcli on my system?\", \"What if I have Python installed in a different location, how do I add the path to PATH?\"]',\n",
       " '27bdbc3f': '[\\n  \"Can I run pgcli in a Docker container if running it locally causes issues?\",\\n  \"How do I start a Docker container with the correct network and variables for pgcli?\",\\n  \"What does the docker run command do in relation to the variables for pgcli?\",\\n  \"How do I access pgcli running in a Docker container?\",\\n  \"Can I use the same commands to connect to a different PostgreSQL database with pgcli in a Docker container?\"',\n",
       " 'f7c5d8da': '[\\n\"How should I handle columns with capital letters in my Postgres database when using the PGCLI command?\", \\n\"Why does Docker use a specific format for capitalizing database column names?\", \\n\"Are there any general guidelines for using quotes when creating tables in Terraform?\", \\n\"What is the correct syntax for querying a database table using Docker\\'s pg CLI command?\", \\n\"Can you provide a simple example of a query that demonstrates the difference between a quoted and unquoted column name in Postgres?\"',\n",
       " 'c91ad8f2': '[\"Can I get the fix for the error message \\'column c.relhasoids does not exist\\' when I run the command `\\\\d <database name>`?\", \"How do I reinstall pgcli if I\\'m getting an error when running a command?\", \"Is there a way to reinstall a non-existent database using pgcli?\", \"What should I do if I get an error when I run the command `\\\\d <database name>`?\", \"Is restarting PC the only solution to resolve the issue with pgcli?\"]',\n",
       " '88bf31a0': '[\"What happens when we get a Postgres OperationalError connection to server at localhost port 5432 and the password authentication for user root fails?\", \"Why do we get a port conflict issue when using port 5432 in our engine.create_engine function?\", \"How can we resolve the port conflict issue when uploading data from Jupyter notebook?\", \"Is there a service running in Windows that could be causing the Postgres error?\", \"Why do we map port 5431 or another port number instead of 5432 in our code?\"]',\n",
       " '23524e6d': '[\"When connecting via pgcli, why do I get an OperationalError saying the role root does not exist?\", \"Why do I get a role conflict when uploading data via a connection in jupyter notebook?\", \"How can I check if there is a root user with the ability to login when Postgres is already installed on my computer?\", \"What is another possible solution to resolve the port issue from 5432 to 5431?\", \"Why do I get a role conflict if I change POSTGRES_USER=juroot to PGUSER=postgres?\"]',\n",
       " '9211bbd6': 'Here are the 5 questions this student might ask:\\n\\n[\"What can I do when I see a \\'Postgres - OperationalError\\' while trying to connect to the database?\", \"Why am I getting an \\'OperationalError\\' when trying to connect to the \\'ny_taxi\\' database?\", \"How can I resolve a connection failure to a PostgreSQL server on port 5432?\", \"What could be the reason for a database named \\'ny_taxi\\' not existing?\", \"How can I prevent this error if I already have Postgres installed on my computer?\"]',\n",
       " '5db86809': 'Here are 5 questions based on the FAQ record:\\n\\n[\"Why do I get a Postgres - ModuleNotFoundError: No module named \\'psycopg2\\' when trying to install?\", \"How do I solve the \\'ModuleNotFoundError: No module named \\'psycopg2\\'” error?\", \"What should I do if updating psycopg2-binary doesn\\'t solve the \\'ModuleNotFoundError: No module named \\'psycopg2\\'” error?\", \"Why do I need to install PostgreSQL if psycopg2 doesn\\'t work even though psycopg2-binary is installed?\", \"How do I update conda or pip if I\\'m facing issues with psycopg installation?\"]',\n",
       " '20c604dd': '[\\n\"What can cause a \\'column does not exist\\' error in Pyscopg2 when the column actually exists in MacBook Pro M2?\",\\n\"Why do join queries throw an error if the column name is mentioned directly or enclosed in single quotes?\",\\n\"In what situation does Pyscopg2 throw a \\'column does not exist\\' error?\",\\n\"Which type of quotation marks should be used in column names in Pyscopg2 to avoid \\'column does not exist\\' errors?\",\\n\"Why does using single quotes in Pyscopg2\\'s column names lead to a \\'column does not exist\\' error?\"',\n",
       " 'b11b8c15': 'Here is the list of questions in parsable JSON format:\\n\\n [\"Why does the pgAdmin - Create server dialog not appear?\", \"How do I register a server in pgAdmin?\", \"What is wrong with the latest version of pgAdmin?\", \"Can I still use the old register method in pgAdmin?\", \"Why does the create server dialog not open?\" ]',\n",
       " 'a6475348': 'Here is the list of questions:\\n\\n[\"How to troubleshoot a blank white screen after logging into pgAdmin running in a Docker container?\", \"Why do I get a \\'The referrer does not match the host\\' error when trying to log into pgAdmin in a browser?\", \"Is there a way to prevent the CSRF error in pgAdmin using an environment variable?\", \"Can I use a different browser to access pgAdmin without getting a blank screen?\", \"Why doesn\\'t this issue occur when I open a Codespace in locally installed VSCode?\"]',\n",
       " '1ea7680e': 'Here are the 5 questions:\\n\\n[\"Why I am unable to access the PgAdmin address via my browser when using Docker on my Mac Pro device?\", \"How do I modify the docker run command to access the pgAdmin address?\", \"Can you provide the modified docker-compose.yaml configuration to access the pgAdmin address?\", \"I\\'m getting a ModuleNotFoundError for pysqlite2 when trying to import it in Python, what can I do to resolve this issue?\", \"I\\'m using Anaconda, how do I resolve the DLL load failure when importing _sqlite3?\"]',\n",
       " '10acd478': '[\"Why are there missing records when uploading data to a Postgres table from a Jupyter notebook?\", \"How do I fix the issue of missing records when re-running a script from top to bottom in a Jupyter notebook?\", \"What is the solution to ingest all data when using a Jupyter notebook for the first time?\", \"Why does the while loop start with the second chunk of data when re-running a script?\", \"Who does one contact if there are further issues with the Jupyter notebook script?\"]',\n",
       " '752e8452': 'Here are the 5 questions based on the FAQ record:\\n\\n[\\n\"How do I iterate over a CSV file in Python without getting an error?\", \\n\"What is the recommended tool to decompress a gzip file if I want to preview it in VSCode?\", \\n\"What is the difference between unzip and gunzip? Should I use one over the other?\", \\n\"Can you provide an example of how to read a CSV file in Python using pandas?\", \\n\"How do I measure the time taken by a Python script to execute?\"\\n]',\n",
       " 'aa6f52b8': 'Here are the 5 questions that the student might ask based on the FAQ record:\\n\\n[\"How do I import pandas as pd in iPython?\", \"What is parse_dates used for when reading a CSV file?\", \"Can I specify multiple column names to be parsed as dates when reading a CSV file?\", \"Is it possible to convert a string column to datetime format in pandas without using the parse_dates parameter?\", \"How can I view the information about the DataFrame after parsing the CSV file?\"]',\n",
       " '3dacbb98': '[\\n\"Can I use a scripting language other than Python to ingest data from the provided GitHub link using curl?\",\\n\"How can I use curl to download data from the GitHub link provided and save it in a CSV file?\",\\n\"My Python code is unable to ingest data from the GitHub link provided using curl, what is the correct syntax?\",\\n\"What is the way to integrate Python with curl to download data from the GitHub link provided and save it as a CSV file?\",\\n\"Are there any specific prerequisites or installation steps required for utilizing curl in Python to download data from the GitHub link provided?\" \\n]',\n",
       " '8b71a398': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"How can I read a CSV file compressed using Gzip with Pandas?\", \"Is it possible to read a Gzip compressed CSV file using the read_csv() function in Pandas?\", \"What is the file extension for a CSV file compressed using Gzip?\", \"How do I specify the compression type when using the read_csv() function to read a Gzip compressed CSV file?\", \"Can I set a parameter to improve memory efficiency when reading a Gzip compressed CSV file with Pandas?\"]',\n",
       " 'aa244fa0': 'Here is the list of questions based on the FAQ record:\\n\\n[\"How to iterate through and ingest parquet file in Python?\",\"How to clear a table in PostgreSQL before writing data from a parquet file?\",\"Can I use a batch size other than the default 65536 when writing data to a PostgreSQL table?\",\"How can I use PyArrow to resolve the lack of a chunksize option for parquet files?\",\"How do I calculate the total number of rows in a parquet file using PyArrow?\"]',\n",
       " 'eac816d7': '[\"What is the cause of the \\'ImportError: cannot import name \\'TypeAliasType\\' from \\'typing_extensions\\'. error raised during the jupyter notebook’s cell execution when trying to use SQLAlchemy?\", \"How can I update the version of the \\'typing_extensions\\' module to >= 4.6.0?\", \"Can I update \\'typing_extensions\\' using Conda?\", \"Can I update \\'typing_extensions\\' using pip?\", \"Is it necessary to update \\'typing_extensions\\' for using SQLAlchemy?\"]',\n",
       " 'd44d1c77': '[\\n  \"How can I fix the TypeError \\'module\\' object is not callable when I\\'m trying to use SQLAlchemy with Python?\",\\n  \"Do you have any solutions for the issue when trying to create a connection to PostgreSQL using SQLAlchemy?\",\\n  \"Can you show me the correct way to create a database engine using SQLAlchemy and PostgreSQL?\",\\n  \"I\\'m having problems with the PostgreSQL connection string, how do I set it up correctly?\",\\n  \"What\\'s the correct syntax for creating a PostgreSQL connection with SQLAlchemy and psycopg driver?\"',\n",
       " 'ed34766a': 'Here are the 5 questions based on the FAQ record:\\n\\n[\"Why did I get a ModuleNotFoundError: No module named \\'psycopg2\\' when running my Python script?\", \\n\"What is causing the error engine = create_engine(\\'postgresql://root:root@localhost:5432/ny_taxi\\') in my Jupyter notebook?\", \\n\"How do I resolve the \\'No module named \\'psycopg2\\'\\' error when using SQLAlchemy?\", \\n\"What Python module do I need to install to fix the \\'ModuleNotFoundError: No module named \\'psycopg2\\'\\' error?\", \\n\"Can I install the required Python module using Conda or pip?\"]',\n",
       " 'fd714677': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"How do I add the Google Cloud SDK PATH to Windows?\", \"What is the error I get when trying to update my system PATH?\", \"How do I use conda to add the PATH to Windows?\", \"What do I need to do to install Gitbash on Windows?\", \"How do I set Gitbash as the default terminal on Windows?\"]',\n",
       " '9de2c3e9': 'Here are the 5 questions based on the FAQ record:\\n\\n[\"Why does the GCP project creation fail with a 409 error?\", \"Why is the project ID already in use?\", \"Can I still use \\'testproject\\' as a project ID?\", \"How do I resolve the conflict with the existing project ID?\", \"Will the project ID be available again if it\\'s already taken?\"]',\n",
       " '827dd4af': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"How do I resolve the error \\'Error 403: The project to be billed is associated with an absent billing account\\' in GCP?\", \"Why do I need to enter my project ID in the GCP dashboard?\", \"What is the value I should enter in the GCP project ID field?\", \"How do I link my billing account to my current GCP project?\", \"Why is my billing account not associated with my GCP project?\"]',\n",
       " 'a42a7e8c': '[\\n  \"What if I\\'m having issues with my credit/debit card payment to Google Cloud?\",\\n  \"Why is it difficult to get support for problems with paying for the trial account?\",\\n  \"Can you recommend an alternative payment method for Google Cloud if Kaspi is not working?\",\\n  \"I\\'m from Kazakhstan, is there a specific issue with Kaspi payment method for GCP?\",\\n  \"Are there any other payment methods besides TBC that might work for a Google Cloud free trial account?\"\\n]',\n",
       " '4eefdd01': '[\"Where can I find the GCP \"ny-rides.json\" file in Instance with my project so I can use it with Docker and Terraform?\", \"How do I create a new JSON key in GCP Service Accounts Keys tab to access my ny-rides.json file?\", \"What type of key should I select when adding a key in GCP Service Accounts Keys tab to get a JSON key?\", \"Can I find the ny-rides.json file under the IAM & Admin tab in GCP after selecting my project?\", \"How do I access the KEYS tab in GCP Service Accounts Keys tab to create a new JSON key for my ny-rides.json file?\"]',\n",
       " '0282578d': '[\"What should I do with my instance in Google Cloud after completing the lecture?\", \"Do I need to delete my instance from the Google Cloud platform?\", \"Should I delete my instance as Alexey did in the lecture?\", \"Can I delete my instance in Google Cloud?\", \"Will I have to do this twice for the week 1 readings if I delete my instance?\"]',\n",
       " 'bd3e60fd': '[\"How can I get real-time information about system resource usage, including CPU, memory, and processes?\", \"How can I display information about system memory usage and availability?\", \"How can I show disk space usage of file systems?\", \"How can I list all running processes along with detailed information?\", \"How can I list active network connections and listening ports?\"]',\n",
       " 'c4e9bc60': 'Here are the 5 questions based on the FAQ record:\\n\\n[\"When I try to enable billing for my project, it says it hasn\\'t been enabled despite setting it up successfully?\", \"Why do I get a 403 error when trying to update my dataset if billing is enabled?\", \"Can you explain why disabling and re-enabling billing resolved the issue for you?\", \"How do I enable billing for a project with the error \\'Billing has not been enabled for this project\\'?\", \"Why is the default table expiration time limited to less than 60 days with billing enabled?\"]',\n",
       " 'f10b49be': 'Here is the list of questions:\\n\\n[\\n\"How do I install the GCP SDK on Windows if I\\'m having trouble?\",\\n\"What happens if I\\'m getting an error about quota exceeded or API not enabled when using the GCP SDK?\",\\n\"Why does my GCP Virtual Machine keep failing to start due to resource issues?\",\\n\"Can I automate the process of creating an image of a GCP Virtual Machine, or do I have to do it manually?\",\\n\"What are the main steps to follow when setting up a project in GCP?\"',\n",
       " '3184bd8b': '[\"Is it necessary to use a GCP VM in the course?\", \"When is it useful to use a GCP VM?\", \"Can we use our own environments instead of a GCP VM?\", \"Why is it not possible to directly commit changes in the VM?\", \"What are the advantages of using our own environment?\"]',\n",
       " '8bea4d53': 'Here are the 5 questions:\\n\\n[\"How do I create a directory if it says mkdir: cannot create directory ‘.ssh’: Permission denied?\", \"Why can\\'t I create a directory in the root folder (/)?\", \"How do I link to Video 1.4.1?\", \"What are the implications of trying to create a directory in the root folder (/) versus my home directory?\", \"How do I ensure I\\'m doing things correctly?\"\"]',\n",
       " '86d11cc0': '[\\n\"Why do I get a permission denied error when trying to save a file in my GCP VM via VS Code?\",\\n\"How can I change the owner of files I\\'m trying to edit in my GCP VM via VS Code?\",\\n\"What is the correct command to change the ownership of a directory in my GCP VM?\",\\n\"Can I run the command to change the owner of files in my GCP VM via VS Code or must I run it on a separate terminal?\",\\n\"Why do I need to change the owner of files I\\'m trying to edit in my GCP VM via VS Code?\"',\n",
       " '2cb48591': '[\"What can I do if my connection request to my GCP VM keeps timing out, even though I connected successfully last week?\", \"Why does my VM connection request keep timing out, and how can I resolve the issue?\", \"How do I troubleshoot issues with my GCP VM connection request timeout?\", \"Why is my GCP VM connection request timing out, and what are some possible solutions?\", \"What should I do if my GCP VM connection request keeps timing out, and how can I fix the issue?\"]',\n",
       " '9523c813': '[\\n\"How can I troubleshoot the issue where I\\'m unable to connect to the host port 22 on my GCP VM and get a \\'no route to host\\' error?\",\\n\"What can I do to resolve the timed-out error when trying to SSH into my GCP VM?\",\\n\"Why do I get a \\'no route to host\\' error when attempting to connect to the host port 22 on my GCP VM?\",\\n\"What are some common causes of SSH connection failures to GCP VMs, and how can I troubleshoot them?\",\\n\"How can I configure my GCP VM to allow SSH connections after I\\'ve encountered difficulties with the default configuration?\"',\n",
       " '4f8d9174': 'Here are the questions based on the FAQ record:\\n\\n[\"How do I forward ports from a GCP VM without using VS Code?\",\\n\"Can I use built-in tools in Ubuntu to forward ports?\",\\n\"How do I access pgAdmin from my local machine without additional clients?\",\\n\"Can I use a single SSH command to forward multiple ports?\",\\n\"What are the credentials I might need if I have trouble accessing Jupyter Notebook?\"]',\n",
       " '29f84a82': '[\\n\"GCP gcloud + MS VS Code - what happens when running gcloud auth application-default login?\",\\n\"Why does gcloud auth hang when logging in for the first time?\",\\n\"How do I configure Trusted Domains in gcloud?\",\\n\"What is the error message I see when I try to login to gcp via the gcloud cli?\",\\n\"What should I do when I see a prompt to open the login page in a browser after running gcloud auth?\"',\n",
       " '20a01fd0': '[\\n\"What are possible causes of the error \\'Terraform - Error: Failed to query available provider packages Could not retrieve the list of available versions for provider hashicorp/google\\'\"?\\n\"What happens when terraform is unable to access the online registry due to an internet connectivity error?\"\\n\"Why do I need to restart my network when trying to resolve the error \\'Could not query provider registry for registry.terrafogorm.io/hashicorp/google: the request failed after 2 attempts, please try again later\\'?\"\\n\"What should I check before running terraform init again after resolving an internet connectivity error?\"\\n\"Can clearing cookies resolve the issue with terraform unable to access the online registry?\"',\n",
       " '5a712a20': 'Here is the list of 5 questions in JSON format:\\n\\n[\"Why do I get an error with Terraform when trying to post to Google storage?\", \"What is causing the OAuth2 token to fail when trying to fetch from Google\\'s token endpoint?\", \"Why is the network the cause of the issue, and how does it affect Terraform?\", \"How can I configure Terraform to use a VPN when accessing Google services?\", \"What can I do if I\\'m having trouble with my VPN and need help from my provider?\"]',\n",
       " '06021091': '[\"What is the most straightforward way to install Terraform on WSL for the purposes of this module?\", \"Are there any specific configurations I need to make for Terraform to work effectively with WSL?\", \"Can you provide more details on how to configure Terraform on Windows 10 Linux Subsystem?\", \"How do I ensure that Terraform is properly set up to integrate with Docker for this module?\", \"Is there a recommended Terraform version for this module that I should be using?\"]',\n",
       " 'df8ea7e8': 'Here are the 5 questions:\\n\\n[\\n\"Terraform - why do I get an error when trying to run terraform apply and it says \\'Error acquiring the state lock\\'?\",\\n\"I applied changes using terraform apply, but the infrastructure didn\\'t change on AWS. Why is this happening?\",\\n\"Can I run terraform on a remote machine, or does it need to be on the same machine as my code?\",\\n\"Can I use terraform to deploy only certain resources, like just an EC2 instance, instead of deploying the whole infrastructure?\",\\n\"How do I check the terraform state to see what resources are being managed?\"',\n",
       " '1093daf5': 'Here are the 5 questions the student might ask:\\n\\n[\"When do I get the error \\'Terraform - Error 400 Bad Request. Invalid JWT Token\\' on WSL?\", \"Why is the error \\'oauth2: cannot fetch token: 400 Bad Request\\' occurring when running \\'terraform apply\\' on wsl2?\", \"How can I resolve the error \\'Invalid JWT: Token must be a short-lived token (60 minutes) and in a reasonable timeframe\\'?\", \"What is causing the \\'invalid_grant\\' error in Terraform when running on WSL?\", \"How do I fix the system time on WSL2 when it is desynchronized, which is causing JWT token issues?\"]',\n",
       " '947213b1': '[\"When we encounter a Terraform error \\'Error 403: Access denied\\', what is the usual cause and how do we fix it?\", \"If our Google credentials are not set up correctly, how can we troubleshoot and resolve the issue?\", \"What is the purpose of the $GOOGLE_APPLICATION_CREDENTIALS variable and how do we use it?\", \"How do we enable access to the Google cloud service account using the key file?\", \"What does the command \\'gcloud auth activate-service-account --key-file $GOOGLE_APPLICATION_CREDENTIALS\\' do and why is it necessary?\"]',\n",
       " '002d4943': '[\"How can I use the same service account for both Docker and Terraform, considering they require different credentials?\", \"Do I need to set my environment variables again after setting them for Docker?\", \"Can I have multiple service accounts if I need to work on different resources?\", \"What are the minimum requirements for a service account in this course?\", \"Will another service account allow me to work on multiple projects simultaneously?\"]',\n",
       " '8dc77677': '[\"Where can I find the Terraform files for downloads?\", \"How do I get started with Docker on this course?\", \"Is there any specific version of Docker recommended for this module?\", \"What exactly is Terraform, and how is it related to Docker?\", \"Can I use a Linux environment or do I need to use Windows for Docker?\"]',\n",
       " '29d3d343': '[\"What happens when I run terraform init outside the working directory that contains terraform configuration files? Why do I get an error message saying Terraform initialized in an empty directory?,\", \"What are the Terraform configuration files and how do I create them?,\", \"Why do I have to navigate to the working directory before running terraform init?,\", \"Can I run multiple terraform commands in a single directory, or do I have to navigate in and out of directories throughout the process?,\", \"How do I handle errors that occur when trying to initialize Terraform in an empty directory?\"]',\n",
       " 'e2095203': '[\\n\"What are possible reasons for getting an error creating Dataset: googleapi: Error 403: Request had insufficient authentication scopes in Terraform?\",\\n\"Why am I getting an error saying \\'Access denied\\' or \\'Forbidden\\' when running Terraform commands?\",\\n\"How can I troubleshoot the \\'googleapi: Error 403: Request had insufficient authentication scopes\\' error in Terraform?\",\\n\"What specific permissions do I need to set for the GOOGLE_APPLICATION_CREDENTIALS in Terraform?\",\\n\"How do I correctly set the GOOGLE_APPLICATION_CREDENTIALS environment variable in Terraform?\"',\n",
       " '22a2b9f2': '[\"What is the correct way to declare my project name in Terraform for creating a bucket?\", \"What is the difference between project ID and project name in GCP console?\", \"Why am I getting a \\'Permission denied\\' error?\", \"Can I create a bucket using Terraform?\", \"What is the cause of the \\'forbidden\\' error when trying to create a bucket in Terraform?\"]',\n",
       " '5d7588f0': 'Here is the list of questions in parsable JSON:\\n\\n[\"How do I ensure the sensitivity of the credentials file when using Google Cloud?\", \"I\\'m having trouble inputting the provider credentials, how can I simplify this process?\", \"What is the correct syntax for using a file as credentials in the \\'google\\' provider?\", \"How do I configure the \\'region\\' and \\'zone\\' settings in the \\'google\\' provider?\", \"Can I use the \\'gcpkey\\' variable as a file path?\" ]',\n",
       " '5276a695': '[\\n  \"What if I encounter an error saying a column doesn\\'t exist when trying to query a database?\",\\n  \"How can I properly format columns with names that start with uppercase letters when querying a database?\",\\n  \"How can I select data from a subset of existing data when querying a database?\",\\n  \"Is it possible to use parentheses or quotes to surround the value being searched for when querying a database?\",\\n  \"What should I do if the data in the database does not match the value being searched for when querying it?\"\\n]',\n",
       " '70c159df': '[\\n  \"How do I resolve the error \\'Column Zone doesn\\'t exist\\' when I run a SQL SELECT statement?\",\\n  \"What\\'s the recommended way to deal with quotation marks when working with databases?\",\\n  \"Why is it necessary to convert column names to lowercase when working with Pandas?\",\\n  \"What\\'s the purpose of using the \\'str.lower()\\' function when modifying data in Pandas?\",\\n  \"How can I avoid having to use quotation marks all the time when working with database tables?\"\\n]',\n",
       " 'f55efcf0': 'Here are the 5 questions in parsable JSON format:\\n\\n[\"What is the solution to resolve the host error with curl on a Mac?\", \"How do we use Terraform to manage infrastructure as code?\", \"What is the purpose of using os.system for curl commands?\", \"How can we ensure seamless integration of Docker and Terraform for infrastructure deployment?\", \"Can we use curl to download files from a specific URL and output it to a CSV file?\"]',\n",
       " '2b7a8512': '[\\n\"How do I resolve the \\'ssh: Could not resolve hostname\\' error in my Docker container?\", \\n\"What is the purpose of the .ssh/config file in my laptop?\", \\n\"I\\'m getting an error with my Terraform deployment, can I use the .ssh/config file to resolve it?\", \\n\"After changing my .ssh/config file, do I need to restart my Docker container?\", \\n\"I\\'m new to Docker, how do I know if my .ssh/config file is correctly applied?\"\\n]',\n",
       " '1cd746c4': 'Here are the 5 questions that the student might ask based on the FAQ record:\\n\\n[\"Why is \\'pip\\' not recognized as an internal or external command, operable program or batch file?\", \"How do I add Anaconda to my PATH on Linux?\", \"What\\'s the correct way to set the PATH on Windows?\", \"Why do I need to modify my bashrc file?\", \"Why do I need to restart my terminal for the changes to take effect?\"]',\n",
       " '6d367222': 'Here is the list of questions:\\n\\n[\\n\"How do I resolve the \\'error starting userland proxy: listen tcp4 0.0.0.0:8080: bind: address already in use\\' error?\", \\n\"What can I do to resolve the \\'error response from daemon: cannot stop container: permission denied\\' error?\", \\n\"How do I fix the \\'Error: cannot import module psycopg2\\' error?\", \\n\"What is the cause of and how do I resolve the \\'docker build Error checking context: \\'can\\'t stat \\'<path-to-file>\\' error?\", \\n\"What are the steps to resolve the \\'Error: error response from daemon: cannot stop container: ...\\' error when using Docker?\"',\n",
       " '84e601e1': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"How can I get a pip-friendly requirements.txt file from Anaconda?\", \"What are the steps to get a requirements.txt file in pip format from Anaconda?\", \"Is conda list -d > requirements.txt a valid command to get a requirements.txt file?\", \"Why might pip freeze > requirements.txt give odd pathing?\", \"How is pip used after installing it with conda install pip?\"]',\n",
       " '4cf83cc2': '[\"How do I find the FAQ questions from previous cohorts for the orchestration module? Can you provide links to them? What is the preferred way to install the software for the orchestration module? How can I ensure that my workflow is properly configured in the orchestration module? Are there any specific prerequisites or requirements I need to fulfill before starting this module?\"]',\n",
       " '5adc5188': '[\"What can be the cause of Docker containers exiting instantly with code 132?\", \"Is the issue of Docker containers exiting instantly with code 132 specific to older architecture?\", \"Do you need a specific computer hardware to solve the Docker containers\\' instant exit issue?\", \"Is Docker containers\\' instant exit issue related to the way a VirtualBox VM is set up?\", \"Can you recommend a specific virtual machine simulator to use instead of VirtualBox?\"]',\n",
       " '3ef0bb96': 'Here are the 5 questions:\\n\\n[\"What causes Mage - Unexpected Kernel Restarts; Kernel Running out of memory errors in WSL 2?\", \"Why is it necessary to dedicate at least two cpu cores to Docker in WSL 2?\", \"How do I check the .wsl config file to ensure it\\'s set correctly?\", \"How do I edit the .wslconfig file to allocate more processors to Docker?\", \"Why do I need to shut down WSL and restart Docker Desktop after making changes to the .wslconfig file?\"]',\n",
       " 'a41ce360': '[\"What are some common issues that occur when configuring PostgreSQL and how can I troubleshoot them?\", \"How do I resolve connection issues with my PostgreSQL database?\", \"Can you provide an example of how to use workflow orchestration with PostgreSQL?\", \"What are some best practices for optimizing PostgreSQL performance?\", \"How do I handle database failures or errors during workflow orchestration?\"]',\n",
       " 'b1cf59e5': '[\"Why would I get an OperationalError when trying to connect to my PostgreSQL server using MAGE, and how can I resolve this issue?\", \"What is the difference between the port number in the io_config.yml file and the port number on my local machine?\", \"How can I ensure a successful connection to my PostgreSQL server when using MAGE, and what might be causing a connection refused error?\", \"Can I set the POSTGRES_PORT variable to the same port number as my existing PostgreSQL installation on my local machine?\", \"How do I properly configure the io_config.yml file to connect to my PostgreSQL server using MAGE when I have multiple PostgreSQL installations with different port numbers?\"]',\n",
       " 'f9d6f8bd': 'ayne\\n[\"Why does executing SELECT 1; result in a KeyError when using MAGE?\",\\n\"Why does the SELECT 1 query fail when attaching to PostgreSQL using MAGE?\",\\n\"What is the solution for the KeyError that occurs when running SELECT 1 in MAGE with PostgreSQL?\",\\n\"What configuration is missing in MAGE when trying to execute a SELECT 1 query?\",\\n\"What is the correct profile to use in MAGE to resolve the KeyError when executing SELECT 1?\"]',\n",
       " 'f3adb937': '[\\n\"MAGE 2.2.4 ConnectionError, how can I resolve the \\'TimeoutError: The write operation timed out\\' error?\",\\n\"How do I troubleshoot the 404 error found in the BigQuery dataset when trying to connect using SQL in MAGE 2.2.4?\",\\n\"Why do I get a \\'NotFound: 404 Not found\\' error when trying to use BigQuery with MAGE 2.2.4?\",\\n\"What if I\\'m still getting a 404 error even after giving the correct roles and permissions to my service account in MAGE 2.2.4?\",\\n\"How can I update the mage io_config.yaml file to resolve the connection timeout error in MAGE 2.2.4?\"',\n",
       " 'eb3d6d36': 'Here is the list of questions based on the FAQ record:\\n\\n[\\n\"I am getting a RefreshError with an invalid JWT token. How can I resolve this issue?\",\\n\"Why am I receiving an error message saying my JWT token must be short-lived and within a reasonable timeframe?\",\\n\"What do I need to check in the JWT claim to fix this error?\",\\n\"I\\'ve got an invalid JWT token error, but how can I ensure my token is valid and not expired?\",\\n\"Can you provide a resource that explains how to fix the invalid JWT token issue in Google Client?\"',\n",
       " 'a76e1f4d': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"Why do I keep getting an IndexError: list index out of range in Mage 0.9.61?\", \"What caused this error in Mage version 0.9.61?\", \"Is there a solution available for the IndexError: list index out of range issue?\", \"How do I update my docker container to resolve the error?\", \"Is the error still present after the update in Mage version 0.9.62?\"]',\n",
       " '934facf8': '[\\n\"Can I use a non-existent directory path to save a file in Python, or will I get an OSError?\",\\n\"What does mkdir(parents=True) do in the given code snippet?\",\\n\"I\\'m still a bit unsure about how as_posix() converts a path. Can you provide more details?\",\\n\"How can I resolve the OSError when trying to save a file into a non-existent directory?\",\\n\"In Python, how do I check if a directory exists before trying to save a file in it?\"',\n",
       " 'a2c7b59f': '[\"What are the key points to consider while deploying Mage to GCP using Terraform?\", \"Why is the video on DE Zoomcamp 2.2.7 missing deployment steps?\", \"How do I set the project_id default value in variables.tf while deploying Mage to GCP?\", \"Why do I need to enable the Cloud Filestore API before deploying Mage to GCP?\", \"What prompts does Terraform ask during the terraform apply step and how do I proceed with the deployment?\"]',\n",
       " '997d4aaa': 'Here are 5 questions based on the FAQ record:\\n\\n[\"How do I run multiple Magento instances in Docker from different directories?\", \"How do I run multiple Docker containers from different directories with Docker Compose?\", \"Why do I get a \\'Request had insufficient authentication scopes\\' error when deploying Mage to Google Cloud?\", \"How do I add the \\'cloud platform scope\\' to my GCP virtual machine?\", \"How do I allow full access to all Cloud APIs in the GCP console?\"]',\n",
       " 'bc269b95': '[\"What happens when trying to deploy infrastructures with Terraform on a GCP free trial account?\", \"Why do I face a Load Balancer problem on my GCP free trial account?\", \"Can I use a Load Balancer with my Terraform code when I\\'m on a free trial account?\", \"How do I resolve the Load Balancer quota problem when I\\'m on a GCP free trial account?\", \"What codes do I need to comment or delete in the main.tf file when using Terraform on a GCP free trial account?\"]',\n",
       " '10ea342e': 'Here is the list of questions:\\n\\n[\"Why are my GCP free credits going so fast?\", \"How can I destroy all resources after running terraform destroy?\", \"Why is it taking so long to deploy the MAGE Terraform files?\", \"How do I get rid of the error when running terraform apply?\", \"How can I find the correct regions and zones for my GCP project?\"]',\n",
       " '4bd23594': 'Here are the 5 questions this student might ask:\\n\\n[\"What permission is denied on the resource \\'//vpcaccess.googleapis.com/projects/<ommit>/locations/us-west1\\'?\",\\n\"What is the reason for the \\'IAM_PERMISSION_DENIED\\' error?\",\\n\"Can you explain the \\'googleapi: Error 403\\' message?\",\\n\"What is wrong with the \\'vpcaccess.connectors.create\\' permission?\",\\n\"How can I resolve the error in creating a Connector?\"\\']',\n",
       " 'b0d48cd7': '[\"What happens when I try to save a file into a non-existent directory in the course?\", \"How can I make a folder exist in my code for a file path?\", \"Why can\\'t Git push an empty folder to GitHub?\", \"Why doesn\\'t the relative path for writing locally work when using GitHub storage?\", \"What should I do if I need to use two separate paths for local writing and GCS bucket upload?\"]',\n",
       " '70a37f2c': 'Here are the 5 questions based on the FAQ record:\\n\\n[\"What column names are in the green dataset versus the yellow dataset in Module 2 of Workflow Orchestration?\", \\n\"Why are there different column names in the green and yellow datasets, and how do I switch between them?\", \\n\"Can you provide more information about the lpep_pickup_datetime and tpep_pickup_datetime columns in Module 2?\", \\n\"How do I modify the script to work with the different column names in the green and yellow datasets?\", \\n\"What specific changes do I need to make to the script to accommodate the different column names in the green dataset versus the yellow dataset?\"]',\n",
       " '8ab78bee': '[\\n\"How can I prevent the process of downloading the VSC using Pandas from being killed right away?\",\\n\"What is the correct process for reading the dataset URL using Pandas?\",\\n\"How can I use Pandas to append data to a parquet file?\",\\n\"What compression settings should I use when writing to the parquet file?\",\\n\"Is the \\'append=True\\' parameter essential when using the fastparquet engine?\"\\n]',\n",
       " '54c6db2f': '[\"When I try to push my Docker image, I get a denied error saying that the requested access to the resource is denied, but I\\'m sure I\\'ve logged in correctly to Docker Desktop. What\\'s going on?\", \"I\\'m trying to push my Docker image, but it keeps getting denied due to a username issue. How do I ensure I\\'m using the right username for both building and pushing?\", \"When I build my Docker image, I set the username and tag correctly, but when I try to push it, the error is still saying that the requested access is denied. What am I doing wrong?\", \"I\\'ve checked my Docker Desktop login credentials multiple times, but the push to Docker image failure persists. Can you provide more troubleshooting steps?\", \"I\\'m trying to troubleshoot a push to Docker image failure, and the error message is \\'denied: requested access to the resource is denied\\'. What are the general causes of this issue and how can I resolve it?\"]',\n",
       " 'c5b998f3': '[\"What happens when my flow script fails with a \\'killed\\' message and shows an \\'INFO\\' log indicating that it\\'s executing a specific step immediately?\", \"Why do you think my flow script fails with \\'killed\\' message when I\\'m executing a step that seemingly starts immediately?\", \"One of my flow scripts fails with \\'killed\\' message although it appears to be starting execution successfully, what\\'s the likely cause of this error?\", \"When I see a \\'killed\\' message in my flow script, how do I troubleshoot this issue and fix it?\", \"Can memory issues on my VM be the root cause of a flow script failing with \\'killed\\' message, and if so, how can I resolve this?\"]',\n",
       " 'eec29536': '[\"How can I resolve the issue of disk space being full in my GCP VM, specifically when using Prefect?\", \"What does it mean when I get an SSL error during workflow orchestration with Prefect, and how can I fix it?\", \"Why does my GCP VM\\'s disk space fill up when I\\'m playing around with Prefect, and how can I free up space?\", \"What happens if I delete a flow from the Prefect UI without also deleting the corresponding flow from the filesystem?\", \"How do I resolve SSL certificate verification errors on my Mac when trying to run Prefect workflows?\"]',\n",
       " '727e5a69': '[\"What happens when a Docker container crashes with a status code of 137?\", \"Why is my container consuming all available RAM in Module 2\\'s homework?\", \"How can I prevent my container from running out of memory in the workflow orchestration?\", \"Can I continue working on the homework without restarting my computer if my container crashes?\", \"Are there any online resources I can use if my computer is unable to run the container efficiently?\"]',\n",
       " 'da899638': 'Here are the 5 questions based on the provided FAQ record:\\n\\n[\\n\"What happens when there is a slow upload internet while running an ETL script from web to GCS?\",\\n\"Why does running an ETL script from web to GCS cause network errors or WSL2 crashes?\",\\n\"How can I solve the issue of network errors or WSL2 crashes while uploading data from web to GCS?\",\\n\"What is the default timeout value for uploading data to GCS and how can I change it?\",\\n\"What is the size of the yellow taxi data for February 2019 in terms of compressed and uncompressed Parquet files?\"',\n",
       " 'dde58c8f': 'Here is the list of questions in JSON format:\\n\\n[\\n\"What happens when I try to re-run the export block for transformed green_taxi data to PostgreSQL?\",\\n\"Why am I getting an error when I try to export transformed green_taxi data to PostgreSQL?\",\\n\"How do I resolve the error \\'UndefinedColumn: column...\\' when exporting transformed green_taxi data to PostgreSQL?\",\\n\"What steps do I need to take to export transformed green_taxi data to PostgreSQL successfully?\",\\n\"What do I need to do if I get an error saying that the table does not exist when exporting transformed green_taxi data to PostgreSQL?\"',\n",
       " '207be93b': '[\\n\"What does SettingWithCopyWarning mean and why do I get this error when I\\'m trying to set a value in my DataFrame?\",\\n\"Why am I getting a warning in my code and how can I avoid it?\",\\n\"In my code, I\\'m trying to change the value of a column in a DataFrame but I get this warning. How do I fix it?\",\\n\"Can you give me an example of what is considered a \\'copy of a slice from a DataFrame\\'? I\\'m not sure\",\\n\"I\\'m having trouble understanding how to use data.loc[] = value. Can you explain the difference between data.loc[] = value and df[] = value?\"',\n",
       " 'f0617e65': '[\"What steps can I take if I\\'m using a slow laptop with large CSV files in nyc data?\", \"Can I use a specific kernel in Mage instead of Python for big CSV files?\", \"How can I avoid using Python kernel for large CSV files in nyc data?\", \"How to work with big CSV files in nyc data with a slow laptop?\", \"What alternatives can I use to handle large CSV files in Mage?\"]',\n",
       " '6290a1a6': '[\\n\"What happens if I try to delete a block in a pipeline if there\\'s a connection between the block and another block?\",\\n\"I\\'m having trouble deleting a block in my pipeline because it\\'s connected to another block, can you provide some guidance on how to handle this situation?\",\\n\"Can you provide an explanation for the error I received when I tried to delete a block in my pipeline?\",\\n\"What are the correct steps to follow when deleting a block in a pipeline if there\\'s a connection between blocks?\",\\n\"How do I safely delete a block in a pipeline without encountering any errors due to connections with other blocks?\"',\n",
       " '5a06248c': '[\"What happens when Mage UI won\\'t let me edit the Pipeline name?\", \"How can I work around the permission denied error while editing a pipeline?\", \"Is it possible to save a pipeline without editing the name initially?\", \"Can I edit the pipeline name later, or is it a permanent restriction?\", \"What should I do if I encounter a permission error while trying to edit the pipeline name in Mage UI?\"]',\n",
       " 'c46a2e9e': 'Here are 5 questions based on the FAQ record:\\n\\n[\"How do I make Mage load the partitioned files that we created on 2.2.4, to load them into BigQuery ?\", \"I get an error saying column \\'vendor_id\\' of relation \\'green_taxi\\' does not exist in Mage. What do I do ?\", \"How can I download only some specific dates from the partitioned files ?\", \"What is the best way to resolve the error \\'column X of relation Y does not exist\\' in Mage ?\", \"How can I create a Data Loader connector to run a SQL command to resolve this error ?\"]',\n",
       " '0513ab8a': 'Here is the list of questions in JSON format:\\n\\n[\\n\"What Files Should I Submit for Homework 2\",\\n\"How do I access my mage files locally\",\\n\"In the mage files, how do I download the .py/.sql files for the blocks I created\",\\n\"Where do I move the downloaded files to in my GitHub repo\",\\n\"How do I commit my changes after downloading the mage files\"\\n]',\n",
       " 'a9385356': '[\"How do I include the files from the week 2 folder of the Data Engineering Zoomcamp in a personal copy of the Data Engineering Zoomcamp repo?\", \"How do I merge the Mage repo files with my main Data Engineering Zoomcamp repo?\", \"What do I need to do to track the Mage repo files in my main Data Engineering Zoomcamp repo?\", \"How do I get around the issue of GitHub seeing the Mage repo and the Data Engineering Zoomcamp repo as separate repositories?\", \"What steps do I need to follow to add the contents of the Mage folder to my main Data Engineering Zoomcamp repo?\"]',\n",
       " 'c30468c0': 'Here are the 5 questions this student might ask based on the FAQ record:\\n\\n[\"What causes ValueError: The truth value of a Series is ambiguous in workflow orchestration?\", \"How can I resolve the truth value ambiguity error when adding multiple assertions in workflow orchestration?\", \"Can you explain why I\\'m getting a ValueError in my workflow orchestration code?\", \"How do I correctly write conditions in workflow orchestration when using multiple columns?\", \"What are the correct logical operators to use for column comparisons in workflow orchestration?\"]',\n",
       " '305aead7': '[\\n\"What happens when I just boot up my PC, continuing from the progress I was doing from yesterday?\",\\n\"Why do Mage AI Files disappear after running docker compose up?\",\\n\"What can I do if my files are gone when I try to access the web interface?\",\\n\"How do I properly shut down the mage docker compose after the files disappear?\",\\n\"What should I check before running docker compose up to avoid files disappearing?\"',\n",
       " '77410975': '[\"How do I avoid errors in the io.config.yaml file in Mage, especially when I have \\' at the trailing side?\", \"What is the correct way to modify the io.config.yaml file to prevent errors in Mage?\", \"Can you give an example of how to fix the error caused by \\' at the trailing side in the io.config.yaml file?\", \"How do I resolve issues when reading the io.config.yaml file in Mage?\", \"What should I do if I encounter errors while configuring the io.config.yaml file for Mage?\"]',\n",
       " '0952abde': 'Here are the questions based on the FAQ record:\\n\\n[\"What happens when Mage can\\'t open the credentials file to access a GCS bucket using pyarrow? Can you explain the permanent error google::cloud::Status(UNKNOWN when exporting data from Mage?',\n",
       " '7c4326eb': 'Here are the 5 questions the student might ask:\\n\\n[\"What is the Mage error and how can I fix it?\", \"Why is the OAuth2 access token not created?\", \"What is the cause of the \\'could not create a OAuth2 access token\\' error?\", \"How do I resolve the \\'retry policy exhausted\\' error in Mage?\", \"How can I successfully authenticate with Google Cloud?\"]',\n",
       " 'a1fc1a14': 'Here is the list of questions the student might ask based on the FAQ record:\\n\\n[\"Why do I get a PermissionError when trying to export data from Mage to a GCS bucket?\", \"What happens when my assigned service account doesn\\'t have the necessary permissions to access a Google Cloud Storage Bucket?\", \"Why do I receive an error indicating that the service account doesn\\'t have storage.buckets.get access to the Google Cloud Storage bucket?\", \"How do I resolve the Google Cloud Status error GetBucketMetadata when trying to access a GCS bucket?\", \"What is the solution to add the necessary permissions to a service account to use Google Cloud Storage?\"]',\n",
       " '6d67fba9': '[\\n\"How can I ensure my pyspark script is ready to be sent to the Dataproc cluster?\",\\n\"How do I create a Dataproc Cluster in GCP Console?\",\\n\"What roles do I need to add to the Dataproc service account?\",\\n\"How do I copy and place my python script in the GCS bucket path?\",\\n\"What do I need to do to let Mage access Google Dataproc and the script it needs to execute?\"',\n",
       " '06876291': '[\"What do I need to do to resolve the issue where Docker-compose takes infinitely long to install zip and unzip packages for Linux, which are required to unpack datasets? Can I add a flag to the apt-get command to speed up the process?\", \"Is there a specific version of Python that I need to use to solve the issue with Docker-compose installing zip and unzip packages? Can I just use the standard Python distribution?\", \"How can I specify that apt-get automatically agrees to install additional packages when using Docker-compose?\", \"Is there a more efficient way to unpack datasets in Linux other than using the zip and unzip packages provided by Docker-compose?\", \"Why do I need to use the -Y flag when installing zip and unzip packages with apt-get? What does this flag do exactly?\"]',\n",
       " '690ba010': 'Here are the 5 questions:\\n\\n[\"What should I do if I encounter an error when writing data from the web to a GCS bucket in Module 3?\", \"How can I avoid errors when writing data from the web to a GCS bucket in Module 3?\", \"What specific data types should I use to ensure successful data writing in a GCS bucket in Module 3?\", \"Are there any specific guidelines for nullable data types to consider in Module 3?\", \"What common mistakes can I make when working with GCS buckets in Module 3 that can cause errors?\"]',\n",
       " 'b6fdd91d': 'Here are the 5 questions this student might ask based on the FAQ record:\\n\\n[\"What causes the error when trying to create a table in BigQuery after ingesting data from Parquet files?\", \"Why do all files within a directory need to have the same schema when ingesting data into BigQuery?\", \"How can we ensure that the data types for columns in our DataFrame are correct before uploading them to BigQuery?\", \"What happens if we encounter mismatched data types when ingesting Parquet files from different folders into BigQuery?\", \"Can we use multiple data types for the same column in different files in the same directory when ingesting data into BigQuery?\"]',\n",
       " '155aa868': '[\"What triggers a gzip.BadGzipFile error when importing FHV data to GCS? Can you also explain how to resolve this issue? \", \"How do I know I\\'m using the wrong URL to the FHV dataset? What URL should I use instead? \", \"Can I use a different type of file for the data other than CSV? \", \"Is it possible to bypass this error and still use the data? \", \"Why does the URL \\'releases/download\\' part of the URL is crucial in this process? \"]',\n",
       " 'e78cf960': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"How do I load data from a URL into a GCS bucket?\", \"Can I use a GCS bucket to store data for a data warehousing project?\", \"What is the process for uploading data from a URL to a GCP bucket?\", \"Is GCS bucket suitable for storing large amounts of data in a data warehousing scenario?\", \"Can I use GCS bucket to store data in a cloud-based data warehouse?\"]',\n",
       " '9afa1f74': '[\\n\"How do I troubleshoot a Bad character (ASCII 0) error while querying my dataset in GCS Bucket?\",\\n\"Why do I sometimes get a wrong formatting issue when uploading my CSV files?\",\\n\"Can I upload unformatted CSV.GZ files directly in GCS Bucket and avoid formatting issues?\",\\n\"What are some tips for uploading large CSV files without formatting issues in GCS Bucket?\",\\n\"Can I use wget to upload my CSV files in GCS Bucket and avoid formatting issues?\"',\n",
       " 'fac138a7': '[\"How do I resolve the error bq: command not found when trying to use the BigQuery Command Line Tool?\", \"What is the correct command to check if the BigQuery Command Line Tool is installed?\", \"Why am I unable to execute the bq command in my terminal?\", \"Is there another way to use the BigQuery Command Line Tool if I\\'m not able to install it?\", \"How do I verify if I have the BigQuery Command Line Tool installed on my GCP account?\"]',\n",
       " '0174dde5': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"What precautions should I take when using BigQuery?\", \\n\"Why did someone get a $80 bill for creating a BigQuery dataset?\", \\n\"Is it safe to create BigQuery datasets in my account\\'s free credits?\", \\n\"Is it possible to spin up a VM and use BigQuery without getting charged?\", \\n\"Are there any specific times when I should be checking my billing in Google Cloud Platform?\"]',\n",
       " '1023ee65': '```\\n[\"Why do my GCS Bucket and BigQuery dataset need to be in the same region when loading data?\", \"Can I create a new dataset in BigQuery to fix the issue of my GCS Bucket and BigQuery being in different regions?\", \"What happens if I forget to specify the same region for my GCS Bucket and BigQuery when creating them?\", \"How do I load data from a GCS Bucket in one region to a BigQuery dataset in another region?\", \"Can I manually switch the region of my existing BigQuery dataset to match the region of my GCS Bucket?\"]',\n",
       " 'effd2bfa': '[\\n\"How can I resolve the issue where my BigQuery dataset can\\'t read and write in different locations if my GCS Bucket was created in a specific region?\", \\n\"How do I ensure I can read and write to BigQuery if my source data is stored in one region and my destination is in another?\", \\n\"What restrictions are there when it comes to reading and writing data between different regions in BigQuery if I have a GCS Bucket set up?\", \\n\"Can I use BigQuery datasets in a region different from where my GCS Bucket was created?\", \\n\"Are there any implications or limitations when it comes to creating a BigQuery dataset in a region different from where my GCS Bucket was stored?\"',\n",
       " '5b55273c': '[\\n\"How can I save my queries in the BigQuery SQL Editor?\",\\n\"What should I do if my Chrome Tab freezes and I lose my entire SQL script?\",\\n\"Can I save my SQL script to a file and edit it outside of the BigQuery SQL Editor?\",\\n\"Why is it important to save my progress in the BigQuery SQL Editor?\",\\n\"Is there a way to format my SQL queries with colors when saving them to a file?\"\\n]',\n",
       " '1835bfe0': '[\"What are the data streaming capabilities in BigQuery that allow for potential integration in future project iterations for real-time analytics?\", \"Can I use BigQuery for real-time analytics load data from a data source?\", \"How does BigQuery handle high-volume data loads for real-time analytics?\", \"Can BigQuery be used for real-time analytics of both structured and unstructured data?\", \"How does BigQuery compare to other data warehousing tools in terms of its real-time analytics capabilities?\"]',\n",
       " '04656af5': '[\"What are the common reasons for an invalid timestamp error when loading data from external tables into a materialized table in BigQuery?\", \\n\"What happens when the timestamp column contains invalid data during data appending to a file in Google Cloud Storage?\", \\n\"Can we use string datatype to define the schema from the external table in BigQuery?\", \\n\"How can we filter out invalid rows from the import to the materialized table in BigQuery?\", \\n\"What is the recommended solution to add valid timestamp data type fields in materialized table in BigQuery?\"]',\n",
       " '2d6536d3': 'Here are 5 questions that a student might ask based on the FAQ record:\\n\\n[\\n\"Can we use other functions instead of pd.read_parquet and pq.write_to_dataset to fix the Timestamp error in BigQuery?\",\\n\"What is the significance of TimestampType(MICROS) and TimestampType(MILLIS) in annotating a valid Timestamp in BigQuery?\",\\n\"Can we expect compatibility issues between parquet files created with pyarrow and pyspark, as mentioned in the references?\",\\n\"How do errors occur when editing parquet files with Python, specifically regarding datetime format?\", \\n\"Can we avoid the use_deprecated_int96_timestamps=True parameter in pq.write_to_dataset, or is it the best solution to write timestamps to INT96 Parquet format?\"',\n",
       " '0516ccbe': 'Here are the 5 questions:\\n\\n[\"Why do datetime columns in Parquet files created from Pandas show up as integer columns in BigQuery?\", \"How can I convert datetime columns in Epoch milliseconds to Timestamp columns in microseconds when writing to Google Cloud Storage?\", \"What is the preferred method to generate the Parquet file with the correct logical type for datetime columns when using Mage?\", \"How can I provide PyArrow with explicit schema to generate the Parquet file with the correct logical type for datetime columns?\", \"What are the datetime column logical types that can be correctly converted to timestamp when loaded by BigQuery?\"]',\n",
       " '6052513d': '[\"How do I create an external table in BigQuery using Python and Cloud Storage?\", \\n\"What are the different external source formats available in BigQuery?\", \\n\"How do I specify the source URI for my external table in BigQuery?\", \\n\"Is there an auto-detection feature for the external data format in BigQuery?\", \\n\"How do I confirm that the external table has been successfully created in BigQuery?\"]',\n",
       " '7a71fa2c': 'Here are 5 questions based on the FAQ record:\\n\\n[\"How do I check if a BigQuery table already exists?\", \"Can I use the BigQuery API to overwrite an existing table or view when creating a new one?\", \"How do I create an external table in BigQuery using Python?\", \"What happens if the table I\\'m trying to create already exists when I use the `client.create_table` function?\", \"What is the purpose of the `try-except` block in the `tableExists` function?\"]',\n",
       " 'f83d9435': '[\"Why does BigQuery show an error saying the missing close double quote (\") character when uploading data?\", \"How do I avoid the \\'Missing close double quote (\") character\\' error when uploading data to BigQuery?\", \"What is the correct method for uploading data from Google Cloud Storage to BigQuery?\", \"Can you elaborate on the command you provided for uploading data from Google Cloud Storage to BigQuery using BigQuery Cloud Shell?\", \"How do I determine the dataset name and table name when using the BigQuery Cloud Shell command?\"]',\n",
       " 'dbf65e11': 'Here is the list of questions in JSON format:\\n\\n[\\n\"Why can\\'t I read and write data in different locations in BigQuery? I have set different regions for my GCS and BigQuery storage.\",\\n\"How do I check the region of my GCS in the Google Cloud Bucket?\",\\n\"Why would my BigQuery dataset need to be in the same region as my GCS?\",\\n\"What is causing the issue when trying to read and write data across different regions in BigQuery?\",\\n\"How do I change the region of my BigQuery dataset to match the one in my GCS?\"',\n",
       " 'c489266b': 'Here is the output in parsable JSON:\\n\\n[\"How can I use Cloud Functions to automate tasks in Google Cloud?\", \"What is the purpose of using Cloud Functions in combination with BigQuery?\", \"What is the benefit of using autodetect=True in the LoadJobConfig?\", \"How do I define the schema for the data in the CSV.gz files?\", \"What is the difference between WRITE_TRUNCATE and WRITE_APPEND write_disposition properties?\"]',\n",
       " 'ebd63566': '[\"What should I do when querying two different tables, one external and one materialized, and I get the same result when using count(distinct(*))?\",\"How do I handle caching issues when combining data from different tables in a data warehouse?\",\"Can someone explain why unchecking cache preferences in query settings resolves the issue when querying external and materialized tables?\",\"I\\'m experiencing the same result when querying different tables in a data warehouse. How do I ensure accurate counts using count(distinct(*))?\",\"Is there a specific way to combine data from different tables in a data warehouse without encountering caching issues?\"]',\n",
       " 'f7252f17': 'Here is the list of questions based on the provided FAQ record:\\n\\n[\\n\"How to handle type error from big query and parquet data?\",\\n\"What happens when I inject data into GCS using Pandas?\",\\n\"Why does my data pipeline cause inconsistent data type between parquet in GCS and schema defined in big query?\",\\n\"How do I fix the data type issue in my data pipeline?\",\\n\"What is the best practice for defining the data type of all the columns in the Transformation section of the ETL pipeline?\"',\n",
       " '47a43bb0': '[\\n\"How do I resolve an error when reading a table in BigQuery and it says that a Parquet column has a type INT64 which does not match the target cpp_type DOUBLE?\", \\n\"What can I do when GCP BQ gives an error saying that my project ID is invalid and it must contain between 6-63 lowercase letters, digits, or dashes?\", \\n\"How do I resolve an error in BigQuery when I misplace content after a from clause in SQL?\", \\n\"What is the correct format for a BigQuery project ID so that I can avoid \\'Invalid project ID\\' errors?\", \\n\"Can you provide guidance on formatting a BigQuery project ID to avoid using extra spaces or symbols and ensuring it only contains lowercase letters, digits, and dashes?\"',\n",
       " 'f3f13def': '{\"error: Parquet column \\'DOlocationID\\' has type INT64 which does not match the target cpp_type DOUBLE.\",\"GCP BigQuery: Why INT64 isn\\'t matching DOUBLE?\",\"How can I partition multiple columns in BigQuery?\",\"Is partitioning more than one column possible in BigQuery?\",\"What type of partitioning does BigQuery support specifically?\"}',\n",
       " '4fd37712': '[\\n\"Can you explain why I\\'m getting an error when trying to use DATE() in BigQuery and what are the correct formats I can use?\",\\n\"Why do I need to convert INT64 columns to DOUBLE and how can I do it?\",\\n\"What is the problem with reading a Parquet file and what can I do to fix it?\",\\n\"What does PARTITION BY expression mean and how can I use it correctly in BigQuery?\",\\n\"How can I convert a column to datetime format in pandas DataFrame and why is this necessary?\"',\n",
       " '8abeca36': '[\\n\"How can I resolve an error that states \\'Parquet column \\'DOlocationID\\' has type INT64 which does not match the target cpp_type DOUBLE\\' when reading a table in BigQuery?\", \\n\"What is the difference between native tables and external tables in BigQuery, and when would I use each?\", \\n\"Can you explain the process of creating an external table in BigQuery, and how does it differ from creating a native table?\", \\n\"How do I troubleshoot an error that says \\'Error while reading table: [table_name]\\' in BigQuery?\", \\n\"What are the key benefits of using external tables in BigQuery compared to native tables?\"',\n",
       " '16c16ff9': 'Here are the 5 questions based on the FAQ record:\\n\\n[\"How can I run a command to export a machine learning model from BigQuery to Google Cloud Storage?\", \"Why do I get a \\'Not found: Dataset was not found in location US\\' error when running the command to export an ML model?\", \"What should I correct to resolve the issue with running the command to export an ML model?\", \"How do I enter the correct project_id and gcs_bucket folder address to resolve the error?\", \"Can I resolve the issue with running the command to export an ML model by verifying the region of my BigQuery data set and Google Cloud Storage bucket?\"]',\n",
       " 'c65d8fd9': '[\"What should I do if I encounter an error while reading a table in Parquet format? Specifically, if a Parquet column has type INT64 but my target cpp_type is DOUBLE?\", \"Why did the Dataset of Dim_zones.sql not get created in location US when I ran fact_trips.sql?\", \"How can I fix an error that says \\'Error while reading table: trips_data_all.external_fhv_tripdata\\'?\", \"In Dim_zones.sql, why do I need to mention the location as \\'US\\' when creating the table?\", \"How many times do I need to run the dim_zones table and the fact_trips.sql to solve the issue?\"]',\n",
       " 'c1a95536': '[\"How can I resolve the error \\'Parquet column \\'DOlocationID\\' has type INT64 which does not match the target cpp_type DOUBLE\\' when reading the table trips_data_all.external_ffh_tripdata?\", \"Why does the export ML model to make predictions not work on my MacBook with an Apple M1 chip? What are the specific steps I need to take to resolve this issue?\", \"What are the necessary steps to set up the serving directory on my computer to use with GCP BQ ML?\", \"Can I still use the standard TensorFlow serving image, or do I need to use an alternative?\", \"How do I run the Docker command to start the TensorFlow Serving container and make predictions using the curl command?\"]',\n",
       " 'bba0da04': '[\\n\"What do I do if my VM runs out of space? VMs\",\\n\"Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column \\'DOlocationID\\' has type INT64 which does not match the target cpp_type DOUBLE.\",\\n\"Why does Prefect caching affect my flow code?\",\\n\"Who can help me with Persistent Storage?\",\\n\"What are some steps I can take during ETLs to manage data?\"\\n]',\n",
       " 'a2120335': '[\\n  \"What does it mean when the Parquet column \\'DOlocationID\\' has a type INT64 that does not match the target cpp_type DOUBLE?\",\\n  \"Why does the error message say \\'Parquet column \\'DOlocationID\\' has type INT64 which does not match the target cpp_type DOUBLE\\'?\",\\n  \"What does \\'Stop with loading the files into a bucket\\' actually mean?\",\\n  \"Can you clarify what it means when you\\'re asked to \\'Stop with loading the files into a bucket\\' for your assignment?\",\\n  \"What should you do after loading the files into a bucket, according to your previous instructions?\"',\n",
       " 'a4ba2478': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"Why do I get an Out of bounds error when trying to read parquets from nyc.gov directly into pandas?\", \"What is the cause of the out of bounds error when trying to read a parquet file?\", \"How can I fix the error when reading parquet files from nyc.gov?\", \"Why do I need to use pyarrow to read parquet files?\", \"What is the purpose of the `errors=\\'coerce\\'` parameter in converting datetime columns in pandas?\"]',\n",
       " '74c361fe': '[\\n\"How do I combine all 12 parquet files for 2022 green taxi data for homework 3?\", \\n\"What is the format of the 2022 NYC taxi data parquet files?\", \\n\"I noticed that the \\'DOlocationID\\' column has a type mismatch between Parquet and BigQuery, what can I do about it?\", \\n\"How can I refer to multiple parquet files in BigQuery using the URIs option?\", \\n\"Can I use a single string to refer to all 12 2022 NYC taxi parquet files?\" \\n]',\n",
       " 'b9b3ef9f': '[\"Why does Parquet column \\'DOlocationID\\' have type INT64 which does not match the target cpp_type DOUBLE when reading trips_data_all.external_fhv_tripdata?\", \"How can I avoid schema issues in the homework?\", \"How do I upload many files at once using the \\'upload files\\' button in GCS?\", \"Can I choose to upload a folder using the \\'upload files\\' button in GCS?\", \"Can I download files locally before uploading them to GCS?\"]',\n",
       " '009ac612': 'Here is the list of questions in parsable JSON format:\\n\\n[\"What are the possible reasons for an error when reading a table in Parquet format?\", \"Why is my partitioned/clustered table not giving me the expected results?\", \"How do I debug an error while reading a Parquet column with a mismatched data type?\", \"What can cause an error when trying to read a table in parquet format?\", \"Why am I getting an unexpected prediction from my partitioned/clustered table?\"]',\n",
       " '68815ec2': '[\\n\"How to resolve Parquet column type mismatch error while reading a table?\",\\n\"What kind of error will I get if I don\\'t match column types while reading a file?\",\\n\"Did anyone get an exact match for one of the options given in Module 3 homework Q6?\",\\n\"What should I do if I\\'m not getting an exact match for Module 3 homework Q6, but am close to an option?\",\\n\"How does the instructor suggest handling an exact match issue for Module 3 homework Q6?\"',\n",
       " 'c8ad08b3': '[\\n\"How can I resolve the error while reading the table \\'trips_data_all.external_fhv_tripdata\\'? I got the error message \\'Parquet column \\'DOlocationID\\' has type INT64 which does not match the target cpp_type DOUBLE\\'.\",\\n\"Why do I get a Python \\'invalid start byte\\' error with the message \\'UnicodeDecodeError: \\'utf-8\\' codec can\\'t decode byte 0xa0 in position 41721: invalid start byte\\' when trying to read data?\",\\n\"What encoding should I use when reading data from the web into a pandas dataframe to avoid the \\'invalid start byte\\' error?\",\\n\"How can I ensure that the encoding is correct when writing a pandas dataframe to Google Cloud Storage (GCS) as a csv file?\",\\n\"Are there any alternative methods available to read Parquet files instead of using \\'pd.read_csv\\'?\"',\n",
       " 'd68b433f': '[\"Why do we sometimes encounter error messages like \"Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column \\'DOlocationID\\' has type INT64 which does not match the target cpp_type DOUBLE.\" during data processing?\"]\\n[\"What is the significance of yield keyword in Python?\"]\\n[\"How do generators in Python generate values on-the-fly?\"]\\n[\"Can you explain the memory advantages of using generators in Python?\"]\\n[\"How do generators in Python differ from lists and tuples in terms of the way they store values?\"]',\n",
       " 'e265ee5a': '[\"What if I encounter an error while reading a table, how can I troubleshoot it?\", \"How do I read multiple Parquet files in Python?\", \"Why am I getting an error while reading a Parquet file?\", \"How do I read Parquet files that have integer data types?\", \"What if the Parquet column type doesn\\'t match the target type in C++?\"]',\n",
       " '0e7dfddc': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\\n\"How do I correctly create a new column in pandas DataFrame from an integer column?\", \\n\"Why won\\'t pandas DataFrame types match between Parquet and C++?\", \\n\"In what situations should I use Int64 instead of converting to int?\", \\n\"Can you provide an example of how to adjust DataFrame data types using pandas?\", \\n\"What alternatives are there to using astype(int) in pandas DataFrames?\"\\n]',\n",
       " '0a059700': 'Here are the 5 questions based on the FAQ record:\\n\\n[\"What is causing an error while reading a Parquet table?\", \"What is causing a Prefect Flow to throw an error when loading data to GCS?\", \"Why am I getting a ValueError when running a Prefect Flow?\", \"How do I resolve a ValueError when trying to run a Prefect Flow?\", \"What does cache_key_fn=task_input_hash do and how can it help or hinder my Prefect Flow?\"]',\n",
       " 'feca7402': '[\\n\"How can I use Prefect to download a .csv.gz file from a URL in my project?\", \\n\"What is the correct syntax to use the @task decorator for downloading a file using requests library in Prefect?\", \\n\"Can I modify the download_file function to save the file in a specific directory?\", \\n\"How do I return the downloaded file path as a result of the @flow function in Prefect?\", \\n\"Is it possible to use the filename variable in the download_file function to dynamically set the file path?\"',\n",
       " '1f519b1a': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\\n\"What should I do if I\\'m getting a \\'not found in location us\\' error?\"\\n\"Why is my dbt project creating a prod dataset in the US location instead of the EU location?\"\\n\"How do I fix the error \\'prod dataset not available in location EU\\' when deploying my DBT models to production in DBT Cloud?\"\\n\"Can I create my project or buckets in a different location to fix the issue?\"\\n\"Why is this issue happening only when I run my job in production mode and not in development mode?\"',\n",
       " '43c454c7': '[\\n\"How do I set up a development environment for my project since I\\'m getting an error saying it\\'s not configured?\",\\n\"What is the guide that explains how to solve this issue?\",\\n\"Can you provide more information about the guide mentioned in the error message?\",\\n\"How do I configure my development credentials using the dbt IDE?\",\\n\"What are the additional resources mentioned in the error message, such as the videos @1:42 and slack chat?\"',\n",
       " 'd7ad69da': '[\\n\"What are the common Runtime Error issues when connecting dbt Cloud to BigQuery?\", \\n\"What error message do we see when dbt is unable to connect to the specified BigQuery database?\", \\n\"Why do we get an Access Denied error when trying to connect dbt Cloud to BigQuery?\", \\n\"What steps should I take to resolve the error \\'User does not have bigquery.jobs.create permission in project <project_name>\\'?\", \\n\"What roles do I need to add to my service account to prevent permission issues in the course?\"',\n",
       " '03fdb780': '[\"What causes an error when building a dbt project in dbt Cloud?\", \"Why do I get an error message saying that a valid dbt project was not found?\", \"Can a dbt project be located in a subdirectory of the connected repository in dbt Cloud?\", \"How do I resolve an error that says my dbt project was cancelled?\", \"What is the purpose of the dbt_project.yml config file in a dbt project?\"]',\n",
       " '9c85f3aa': '[\\n\"What is the error message when setting up the repository?\", \\n\"Why am I getting a \\'Permission denied\\' error when trying to clone the repository?\", \\n\"How do I resolve the issue if I don\\'t have permissions to write to the remote repository?\", \\n\"What are the possible solutions to this cloning error, and can I use a different approach?\", \\n\"Can I use an HTTPS link instead of the current method to clone the repository?\"\\n]',\n",
       " '63026349': '[\\n\"Why are DBT jobs triggered by pull requests disabled when trying to create a new Continuous Integration job in DBT Cloud?\",\\n\"Why is the option to set up a CI Job disabled in DBT Cloud when I\\'m on the Developer Plan?\",\\n\"Why can\\'t I set up a Continuous Integration job in DBT Cloud even though I\\'m enrolled in the Team Plan?\",\\n\"How do I enable the Continuous Integration job option in DBT Cloud, and is it available for the Developer Plan?\",\\n\"Why does the option to set up a CI Job in DBT Cloud remain disabled even though I\\'m on the Team Plan (trial period)\"',\n",
       " '6ba02f77': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\"Why is my IDE session unable to start and what can I do to troubleshoot this issue?\", \"Can you provide more information on what might cause the DBT cloud IDE to load indefinitely?\", \"How do I check the dbt_cloud_setup.md file related to this error?\", \"What are the steps to make a SSH Key and use it with gitclone to import a repo into my dbt project?\", \"What are the correct steps to copy and paste the deploy key into my repo settings?\"]',\n",
       " '8b14286c': 'Here are 5 questions based on the FAQ record:\\n\\n[\"What can I do if I\\'m having problems with columns datatype while running DBT/BigQuery?\", \"What causes columns datatype issues with DBT/BigQuery?\", \"How can I resolve a type mismatch error with parquet files in DBT?\", \"How do I specify dtypes when importing a file from csv to a dataframe with pandas?\", \"What are some possible fixes for type mismatches between parquet files and DBT?\"]',\n",
       " '14a876ea': '[\"How do I troubleshoot issues with access denied errors when loading trip data into GCS using the provided quick script?\",\"Why do I need to use GitHub CLI to download the trip data instead of directly accessing the S3 bucket?\",\"Are there alternative ways to download the trip data and S3 buckets?\",\"Can I use the GUI to upload the files to a GCS bucket after downloading them using GitHub CLI?\",\"How do I use the GitHub CLI to download the trip data and upload it to a GCS bucket?\"]',\n",
       " '1cf5be74': 'Here are the 5 questions based on the FAQ record:\\n\\n[\"What happens when I try to convert fhv_tripdata_2020-01.csv using format_to_parquet_task in Airflow and get an error?\", \"Why do I need to do this conversion for homework question 3?\", \"What is the cause of the CSV parse error: Expected 7 columns, got 1: B02765?\", \"How do I fix the error in the file fhv_tripdata_2020-01.csv?\", \"What should I do to re-execute a failed task in Airflow?\"]',\n",
       " '315ac3cc': 'Here is the list of 5 questions this student might ask based on the FAQ record:\\n\\n[\"How can I hack to load yellow and green trip data for 2019 and 2020?\", \"What was the initial approach to load yellow trip data and why did it take forever?\", \"How did you handle schema inconsistency issues when creating a BigQuery table?\", \"How do I load data to BigQuery for Week 4?\", \"What schema changes are required to be done according to the YouTube video?\"]',\n",
       " 'c5c3beba': 'Here are 5 questions based on the FAQ record:\\n\\n[\\n\"How do I move multiple files from a Google Cloud Storage bucket to BigQuery?\",\\n\"What is the correct syntax for referencing multiple files in a Google Cloud Storage bucket?\",\\n\"Can I use the wildcards feature to specify multiple files to move from a Google Cloud Storage bucket to BigQuery?\",\\n\"Does the \\'gs storage_link\\' format apply to uploading files from a Storage bucket to BigQuery\",\\n\"How do I specify multiple files to upload from a Google Cloud Storage bucket to BigQuery using the CLI\"]',\n",
       " 'f19be91b': '[\\n    \"What causes the ssh connection to stop working for my GCP VM after restarting?\",\\n    \"Why did my ssh connection stop working for my VM after I restarted it?\",\\n    \"Can lack of space cause ssh issues for my GCP VM?\",\\n    \"What happens when running prefect several times on my GCP VM?\",\\n    \"How do I fix the ssh issue with my GCP VM after restarting due to lack of space?\"\\n]',\n",
       " '33db7dc7': 'Here are the 5 questions based on the FAQ record:\\n\\n[\\n\"What happens if I lose SSH access to my GCP VM due to lack of space and get a \\'Permission denied (publickey)\\' error?\", \\n\"How do I resolve \\'Permission denied (publickey)\\' error when trying to access my GCP VM after running out of space?\", \\n\"What are the steps to take if I lose SSH access to my GCP VM due to a lack of storage space?\", \\n\"I\\'m having trouble connecting to my GCP VM due to \\'Permission denied (publickey)\\' error after running out of disk space. What should I do?\", \\n\"Can you help me troubleshoot why I\\'m getting a \\'Permission denied (publickey)\\' error when trying to SSH into my GCP VM after running out of free space?\"',\n",
       " '67ef8f87': 'Here are the 5 questions based on the FAQ record:\\n\\n[\"What happens when I get a 404 Not found error for a dataset in BigQuery?\", \"Should I copy the source dataset and schema to the correct region and naming format?\", \"How can I specify the single-region location instead of multi-regional location in BigQuery?\", \"How do I update the location setting in DBT Cloud for BigQuery?\", \"Do I need to re-upload the service account JSON to update the location setting in DBT Cloud?\"]',\n",
       " '6acf2e77': 'Here are the 5 questions:\\n\\n[\\n\"What happens when I run dbt with the latest version of dbt-utils and get a warning?\", \\n\"What do I need to do to fix the error in stg_green_tripdata.sql after upgrading dbt-utils?\", \\n\"Why does my dbt run fail with an Access Denied error after creating fact_trips.sql?\", \\n\"What is the solution to the Access Denied error in BigQuery?\", \\n\"What additional roles do I need to add to my service account in GCS to resolve the Access Denied error?\"\\n]',\n",
       " '18430f10': '[\"How do I resolve the error when dbt_utils is not found in my project?\", \"What do I need to add to my packages.yml file to include dbt_utils?\", \"I\\'m still getting an error after adding dbt_utils to packages.yml. What\\'s going wrong?\", \"Do I need to specify a specific version of dbt_utils in packages.yml?\", \"How do I validate that dbt_utils has been successfully installed?\"]',\n",
       " 'afb7a40a': '[\"How do I fix lineage results being unavailable in my project?\", \"Why do I get compilation errors in my project? How do I resolve them?\", \"What are build logs used for in dbt?\", \"How do I expand and view error messages or warnings in dbt?\", \"What are stages in dbt and how do I access their logs?\"]',\n",
       " 'd6a5b80e': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\"Why does my Fact_trips table only contain data for a few days?\", \"How can I troubleshoot if my Fact_trips table is not getting all the data?\", \"Why does the dbt run command not include all my data?\", \"Can you provide more guidance on how to run dbt with custom variables?\", \"What\\'s the best way to handle variable settings when building my dbt model?\"]',\n",
       " 'de426d2f': 'Here is the list of questions based on the FAQ record:\\n\\n[\\n\"Why do my fact_trips only contain one month of data?\", \\n\"What happens when I specify if_exists=\\'replace\\' while writing data to BigQuery?\", \\n\"Why does setting if_exists=\\'replace\\' cause data loss in BigQuery?\", \\n\"How do I ensure that all months of data are written to BigQuery?\", \\n\"Why does appending data to an existing table in BigQuery solve the single-month problem?\"]',\n",
       " '354f0e10': 'Here is the list of 5 questions the student might ask based on the FAQ record:\\n\\n[\"What happens when I try to run the dm_monthly_zone_revenue.sql model in BigQuery?\", \"Why do I get an error when running the model in BigQuery?\", \"How can I resolve issues with running the dm_monthly_zone_revenue.sql model in BigQuery?\", \"What is the fix for the error when trying to run the model in BigQuery?\", \"Can someone explain why the \\'month\\' in date_trunc function is causing an error?\"]',\n",
       " '98fae8d0': '[\\n\"What are the differences between dbt_utils.surrogate_key and dbt_utils.generate_surrogate_key in dbt?\",\\n\"How do I modify the dbt_utils.surrogate_key function for use in DBT models?\",\\n\"In a dbt project, what is the recommended replacement for dbt_utils.surrogate_key?\",\\n\"Why should I use dbt_utils.generate_surrogate_key instead of dbt_utils.surrogate_key in my DBT models?\",\\n\"Is dbt_utils.generate_surrogate_key a more efficient alternative to dbt_utils.surrogate_key in DBT?\"',\n",
       " 'cb678fde': '{\\n\"questions\": [\\n\"What happens if I update the database location in dbt but still get errors when running dbt ?\", \\n\"Why does dbt continue to give me an error after changing the dataset location?\", \\n\"I changed the dbt location but dbt run is still showing an error, what\\'s the solution?\", \\n\"How do I resolve the issue when dbt run doesn\\'t recognize the new location setting?\", \\n\"Even after updating the dbt location, the run is still failing; what steps can I take to fix this?\"\\n]\\n}',\n",
       " '39bfb043': 'Here are the 5 questions:\\n\\n[\\n\"Why did I get a table of 100 rows when I ran dbt run without specifying a variable, and why did my table still have 100 rows even after specifying the variable value and running again?\",\\n\"Why is my CI/CD job creating a new dataset in BigQuery when I run dbt?\",\\n\"I ran dbt run with a variable value specified, but I still see the same number of rows in BigQuery. What\\'s going on?\",\\n\"Why is dbt creating a new environment in BigQuery instead of updating the existing one when I run my CI/CD job?\",\\n\"I changed the environment setting in my CI/CD job, but it still creates a new dataset in BigQuery. Why is this happening?\"\\n]',\n",
       " '351a078a': '[\\n\"What is the purpose of the staging dataset in the analytics engineering with dbt module?\",\\n\"Can we use the staging dataset for the project, or is it just for the production environment?\",\\n\"How is the staging dataset different from the other datasets in the project?\",\\n\"Why didn\\'t Vic use the staging dataset in the project?\",\\n\"What kind of objects are the datasets in staging materialized as, and how does this differ from the other datasets?\"',\n",
       " '61da1919': '[\\n  \"I\\'m having trouble accessing DBT Docs even though Docker Compose reports that they\\'re available. What can I do to resolve this issue?\",\\n  \"After serving DBT Docs, I\\'m unable to view them in the browser. How do I troubleshoot this problem?\",\\n  \"I\\'ve successfully served DBT Docs, but the link isn\\'t working in my browser. What\\'s going on?\",\\n  \"DBT Docs are being served, but I get an error when trying to access them in the browser. Can you help me fix this?\",\\n  \"I\\'ve navigated to the address where my DBT Docs should be, but the page isn\\'t loading. What\\'s the solution for this?\"\\n]',\n",
       " '6528c6ae': '[\"How can I fix a 404 Not found error when trying to use the BigQuery adapter and getting \\'Dataset was not found in location europe-west6\\'?\", \"Why am I getting a 404 Not found error in BigQuery adapter when my dataset is not found in europe-west6 location?\", \"What do I need to do when BigQuery adapter is throwing a 404 Not found error for a dataset in europe-west6 location?\", \"How do I resolve the BigQuery adapter 404 Not found error that says \\'Dataset was not found in location europe-west6\\'?\", \"What are the steps to follow when I encounter a BigQuery adapter 404 Not found error and my dataset is not found in europe-west6 location?\"]',\n",
       " 'c0d3a2e8': '{\\\\\"questions\\\\\": [\"Why can\\'t we directly edit files in the main branch when using dbt and Git?\", \"How do I resolve the error where dbt fails to deploy models from a new notebook?\", \"Can you explain more about the dbt deployment process with Git?\", \"What should I do if I encounter permission issues when trying to push my changes to the database?\", \"Why do I have to create a new branch when making changes with dbt and Git, and not edit the main branch directly?\"]}',\n",
       " '859a97c5': '[\\n\"How do I resolve that read-only mode issue with Dbt and Git so I can edit the files?\",\\n\"Why do I need to create a new branch in Git when working on a project?\",\\n\"Can I make changes directly on the main branch in Git?\",\\n\"How can I merge my development branch to the main branch in Git?\",\\n\"What if I already have uncommitted changes in my local files, how do I handle them when creating a new branch in Git?\"\\n]',\n",
       " '32469a2d': 'Here is the output in parsable JSON format:\\n\\n[\"Can I create a CI checks job for deployment to Production when using dbt deploy + Git CI?\", \"Do I need to trigger the CI checks job for deployment to Production?\", \"Why doesn\\'t the Git Clone option work for dbt repositories?\", \"How do I fix the error if my dbt repository is not connected through dbt Cloud\\'s native integration with Github, Gitlab, or Azure DevOps?\", \"What should I do differently from the guide on DTC repo when setting up dbt with Github?\"]',\n",
       " 'c599b3a0': 'Here are the 5 questions based on the FAQ record:\\n\\n[\"When I\\'m trying to configure CI with Github, I don\\'t see the \\'Run on Pull Requests\\' option on the job\\'s triggers. What can I do?\", \"How do I reconnect my Github account with my dbt project using a native connection instead of clone by SSH?\", \"How do I disconnect my current Github\\'s configuration from the dbt project?\", \"Why can\\'t I see the \\'Run on Pull Requests\\' option after reconnecting my Github account?\", \"How can I configure CI with Github after disconnecting and reconnecting my Github\\'s configuration?\"]',\n",
       " '179df18d': '[\\n\"Why do I get a Compilation Error in DBT when my model depends on a source that\\'s not found?\", \\n\"How do I resolve a Compilation Error in DBT when my model is dependent on a source that\\'s not found?\", \\n\"What do I do if my DBT model depends on a source named \\'X\\' which was not found?\", \\n\"Why do I get an error message in DBT saying a source named \\'X\\' was not found?\", \\n\"What\\'s a possible solution to a Compilation Error in DBT caused by a missing source?\"',\n",
       " '1ce1a275': '[\"What does the \\'NoneType\\' object is not iterable error mean and why does it happen in macro test_accepted_values?\", \"Why is the test accepted_values_stg_green_tripdata_Payment_type__False___var_payment_type_values_ calling the test_accepted_values_stg macro?\", \"How does the test_macro function work in dbt and what is its purpose in this specific scenario?\", \"Why do I need to add the payment_type_values variable to the dbt_project file?\", \"What are the variables and what values do they need to include in the dbt_project file?\"]',\n",
       " 'b529b0bc': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"What can I do to resolve the error message \\'No matching signature for operator CASE for argument types: STRING, INT64, STRING, INT64, ...\\'?\",\\n\"How do I change the data type of the numbers in my SQL query?\",\\n\"Why am I getting a BadRequest error in BigQuery?\",\\n\"What is the issue with copying and pasting the exact macro from the data-engineering-zoomcamp repo?\",\\n\"Can you provide more information about the \\'data type should be string\\' requirement for the initial \\'payment_type\\' data type?\"',\n",
       " '2e51a111': '[\"How can I identify the problematic line in a query when I encounter a dbt error?\", \"Can I find the query that is causing the error in the dbt error log?\", \"What kind of information can I find in the link provided in the dbt error log?\", \"Will the link in the dbt error log take me directly to the specific query that is causing the error?\", \"How can I use the information in the dbt error log to troubleshoot dbt issues?\"]',\n",
       " '6e1a0834': 'Here are the 5 questions the student might ask:\\n\\n[\"What happens when changing the target schema to “marts” in an analytics engineering with dbt module?\", \"Why doesn\\'t changing the target schema in dbt create a schema with the specified name?\", \"How does dbt handle custom schema for a project in analytics engineering?\", \"Can you override the default custom schema creation behaviour in dbt?\", \"What is the macro \\'generate_schema_name.sql\\' used for in dbt and how does it affect schema naming?\"]',\n",
       " 'a8657e65': 'Here are 5 questions based on the FAQ record:\\n\\n[\"How do I set a subdirectory in GitHub as the root for my dbt project?\", \"Can I specify a different directory inside my GitHub repository for my dbt project?\", \"How do I set up a subdirectory in my GitHub repository as the root for my dbt project?\", \"What setting do I need to use to set a different directory in my GitHub repository as the root for my dbt project?\", \"Is it possible to set a subdirectory of my GitHub repository as the root for my dbt project?\"]',\n",
       " '2678d8c2': '[\\n\"How do I solve a compilation error in dbt, specifically when it says \\'depends on a source named \\'<a table name>\\' which was not found\\'?\",\\n\"Why do I need to modify my .sql models in dbt to read from existing table names in BigQuery/postgres db?\",\\n\"What is the syntax to use source() function in dbt, as mentioned in the answer?\",\\n\"How can I resolve compilation errors in dbt due to missing table sources?\",\\n\"What changes do I need to make in my dbt models to fix a dependency issue with a source table?\"',\n",
       " 'aa85c6ae': '[\"What do I need to do if I\\'m getting a compilation error that says a model depends on a node named \\'<seed_name>\\' which was not found in the production environment?\", \"How do I create a pull request to resolve this error?\", \"Do I need to include my \\'.csv\\' files in my \\'.gitignore\\' file?\", \"Why can\\'t I find the seed file inside my \\'seeds\\' folder if I\\'m getting this error?\", \"Can I resolve this error by pushing my changes from the Development branch to the production branch?\"]',\n",
       " 'de06929d': '[\\n\"Can you help me resolve the \\'Access Denied: BigQuery BigQuery: Permission denied\\' error when executing dbt run after using fhv_tripdata as an external table?\",\\n\"What do I need to do to fix the \\'Access Denied: BigQuery BigQuery: Permission denied\\' error I\\'m getting when running dbt after using fhv_tripdata as an external table?\",\\n\"Executing dbt run after using fhv_tripdata as an external table results in a \\'Access Denied: BigQuery BigQuery: Permission denied\\' error, how can I resolve this issue?\",\\n\"What steps can I take to avoid the \\'Access Denied: BigQuery BigQuery: Permission denied\\' error when running dbt after using fhv_tripdata as an external table?\",\\n\"I\\'m getting a \\'Access Denied: BigQuery BigQuery: Permission denied\\' error when executing dbt run after using fhv_tripdata as an external table, what permissions are required to resolve this issue?\"',\n",
       " 'b087fa95': '[\\n\"How can I automatically infer the data type of my columns in pandas when missing values are present?\",\\n\"What is the best way to handle pandas parsing integer columns with missing values as float type when loading data into BigQuery?\",\\n\"How do I specify the data type when transforming data in pandas to avoid type errors in BigQuery?\",\\n\"Can I use a shortcut to infer data types for all integer columns without having to specify each one individually?\",\\n\"How can I handle type errors in BigQuery when loading data from pandas using the \\'fillna\\' method?\"',\n",
       " '3c41892d': '[\\n\"Can you explain why I\\'m getting an exception when trying to load a GitHub repository, stating that \\'taxi_zone_lookup\\' is not found?\",\\n\"What should I do when I encounter an error saying that \\'taxi_zone_lookup\\' is missing while loading a GitHub repository from GitHub?\",\\n\"I\\'m getting an error when trying to access a GitHub repository, and it\\'s saying that \\'taxi_zone_lookup\\' is not found. Can you help me?\",\\n\"Why am I getting an exception that says \\'taxi_zone_lookup\\' is not found when I\\'m trying to load data from a GitHub repository?\",\\n\"What\\'s going on when I try to load data from a GitHub repository, and it throws an error saying that \\'taxi_zone_lookup\\' is missing?\"\\n]',\n",
       " '4842f3e8': '[\"Why does the \\'.gitignore\\' file affect the \\'taxi_zone_lookup\\' when working with dbt?\", \"What does the error \\'Dbt error 404 was not found in location\\' indicate, and how do I fix it?\", \"Why do I get an error saying a table was not found in its location?\", \"How can I ensure that my datasets are using the correct region in dbt?\", \"Where do I set the manually set location in dbt settings?\"]',\n",
       " '5eaf61fe': 'Here is the list of questions:\\n\\n[\\n\"What are some common reasons for data type errors when ingesting data with Parquet files?\",\\n\"Why do data type errors occur when using Parquet files in DBT, and how can we resolve them?\",\\n\"How can I avoid data type errors when ingesting data with Parquet files?\",\\n\"What are some alternative file types to Parquet for ingesting data in DBT, and are there any benefits?\",\\n\"What is the recommended file type for ingesting data, and how do I set it up in a DBT model?\"\\n]',\n",
       " '8ed36cea': 'Here is the list of questions:\\n\\n[\\n\"What happens when I re-run the fact_trips model?\",\\n\"Why do I get different numbers of rows in the fact_trips table each time I re-run it?\",\\n\"What is the main reason for this inconsistency?\",\\n\"Is it necessary to add columns to the ORDER BY clause?\",\\n\"How can I ensure the number of rows in the fact_trips table remains consistent when re-running the fact_trips model?\"',\n",
       " '46aebc79': 'Here is the list of questions:\\n\\n[\\n\"What happens if I encounter a data type error when running my fact table in Module 4?\", \\n\"I\\'m having trouble with the trip_type column and data type errors, can you provide some troubleshooting steps?\", \\n\"In what situations might I encounter a data type error with the trip_type column in BigQuery?\", \\n\"How do I fix a data type error that\\'s occurring with the data type of a specific column?\", \\n\"What changes can I make to the data type of the trip_type column to resolve a data type error?\"\\n]',\n",
       " 'e2d2bc58': 'Here is the list of 5 questions the student might ask based on the FAQ record:\\n\\n[\\n\"What happens if I use \\'select *\\' query without mentioning the table name?\",\\n\"Why do I get a \\'CREATE TABLE with columns with duplicate name locationid\\' error?\",\\n\"Can you give an example of a query that might cause this error?\",\\n\"What is the issue with using \\'select *\\' queries in dbt?\",\\n\"How do I resolve the \\'CREATE TABLE with columns with duplicate name locationid\\' error?\"',\n",
       " '137aab88': 'Here are 5 questions based on the FAQ record:\\n\\n[\"Why do I get a \\'Bad int64 value: 0.0 error\\' when I try to cast ehail fees to integer?\", \\n\"What does the \\'Bad int64 value: 0.0 error\\' error mean in the context of ehail fees?\", \\n\"How do I resolve the \\'Bad int64 value: 0.0 error\\' issue when working with ehail fees?\", \\n\"What is causing the \\'Bad int64 value: 0.0 error\\' error when casting ehail fees to integer?\", \\n\"How can I use the safe_cast function in dbt to avoid the \\'Bad int64 value: 0.0 error\\'?\"]',\n",
       " 'a260e651': '[\\n\"What can I do when I encounter a \\'Bad int64 value\\' error when building the fact_trips.sql model?\",\\n\"Can you provide a solution to the \\'Bad int64 value\\' error in the payment_type_description field?\",\\n\"How can I handle multiple columns causing the \\'Bad int64 value\\' error, like ratecodeid and trip_type in the Green_tripdata table?\",\\n\"Why do I get a \\'Bad int64 value\\' error when using safe_cast in the payment_type_description field?\",\\n\"Can you explain the use of REGEXP_REPLACE in addressing \\'Bad int64 value\\' errors, specifically in the ratecodeid and trip_type columns?\"',\n",
       " 'da8d9fcc': 'Here are the 5 questions:\\n\\n[\"What is the solution to the error \\'Parquet column \\'ehail_fee\\' has type DOUBLE which does not match the target cpp_type INT64\\' when building fact_trips.sql?\", \"Why did the suggested solutions not work for the person who asked the question?\", \"How can I replace the original ehail_fee line in stg_green_trips.sql?\", \"Is there a way to cast a Parquet column with a different type in dbt?\", \"How does the safe_cast function work in dbt?\"]',\n",
       " '2314e3c4': 'Here is the list of questions:\\n\\n[\"What does the - vars argument need to be in order to work correctly with dbt?\", \"Why do I keep getting a type error when I use the - vars argument?\", \"How do I correctly specify the values for a - vars argument in dbt?\", \"What is the correct syntax for using the - vars argument in dbt?\", \"How do I fix the error when the - vars argument is not interpreted as a dictionary?\"]',\n",
       " 'e7bdbba6': '[\"How do I fix that the Environment Type is greyed out and I\\'m unable to change it?\", \"Why isn\\'t the Environment Type dropdown menu responding to my selections?\", \"What do I need to do to make the Environment Type option available again?\", \"Is there a specific scenario where I wouldn\\'t need to change the Environment Type?\", \"Can I still use the module if I\\'m unable to change the Environment Type?\"]',\n",
       " '52cccade': 'Here are the 5 questions:\\n\\n[\"What does \\'Access Denied\\' error message in dbt project mean?\",\\n\"Why do I need to check the branch I\\'m working on in dbt?\",\\n\"How do I run dbt job on a different branch than the default one in dbt Cloud?\",\\n\"Why do I need to specify the branch name when setting up an environment in dbt Cloud?\",\\n\"What does \\'Could not parse the dbt project\\' error message mean and how can I resolve it?\"]',\n",
       " '11a814ea': 'Here is the list of questions:\\n\\n[\"How can I get the job to run with the changes I made to my modelling files?\", \"I updated my files in the development branch but the job still runs on the old files, what should I do?\", \"Why is my job still running with the old files even after I committed the changes in my development branch?\", \"I made changes to my modelling files in the development branch, but the job is still running with the old files, what\\'s going on?\", \"My job still runs with the old files even after updating my development branch, how can I fix this?\"]',\n",
       " '0d1e02d5': '[\"What if I have set up GitHub and BigQuery with dbt successfully but still nothing shows in my Develop tab?\", \"Can I create a development environment before setting up models on dbt?\", \"Why do I need to set parameters on my development environment after setting up dbt?\", \"How do I deploy developed models on dbt for job creation and running?\", \"What are the requirements for being able to develop data models on dbt?\"]',\n",
       " '0a0cc4c3': 'Here are 5 questions based on the FAQ record:\\n\\n[\"What happens when the Prefect Agent retrieving runs from the queue sometimes fails with httpx.LocalProtocolError?\", \"Why do I receive a Sentry error with ProtocolError \\'Invalid input ConnectionInputs.SEND_HEADERS in state ConnectionState.CLOSED\\' when Prefect Agent retrieving runs from the queue fails?\", \"How do I resolve the Prefect Agent retrieving runs from queue failure with httpx.LocalProtocolError?\", \"Can I re-run the Prefect Agent retrieving runs from queue if it fails occasionally?\", \"Why should I wait a few minutes before re-running the Prefect Agent retrieving runs from queue if it fails occasionally?\"]',\n",
       " 'cb912983': '[\"What can I do if BigQuery returns an error when I run \\'dbt run\\'?\", \"Why does loading my taxi data into GCS result in an error?\", \"Why do different data formats cause issues when running \\'dbt run\\'?\", \"How do I convert column types in dbt to fix errors?\", \"What did others do to resolve this error in the Slack channel?\"]',\n",
       " '2d4e434f': 'Here are the 5 questions based on the FAQ record:\\n\\n[\"What is the correct syntax for running dbt run with specific models and variables?\", \"Why am I not getting any results when running dbt run with models and variables?\", \"How can I modify the dbt run command to specify the models I want to run?\", \"What is the difference between using --models and --select in dbt run command?\", \"Can I use dbt run with multiple models and variables simultaneously?\"]',\n",
       " 'bb6655b9': '[\\n\"How do I troubleshoot DBT with BigQuery on Docker if I encounter an error like \\'ModuleNotFoundError: No module named \\'pytz\\'\\' after setup?\",\\n\"Can I use BigQuery with DBT on Docker and what are some common issues I may face?\",\\n\"I need help with setting up DBT with Docker, where do I add a module for pytz?\",\\n\"What are some best practices for using DBT with BigQuery on Docker?\",\\n\"Why do I get an error while initializing DBT with Docker and how do I resolve it?\"',\n",
       " 'fc2eb036': '[\\n\"What do I do when I encounter a \\'VS Code: NoPermissions (FileSystemError): Error: EACCES: permission denied\\' error in Linux?\",\\n\"How do I resolve issues with editing dbt_project.yml after \\'docker-compose run dbt-bq-dtc init\\'?\",\\n\"What is the command to change the profile \\'taxi_rides_ny\\' to \\'bq-dbt-workshop\\'?\", \\n\"What happens when running dbt debug and how do I resolve this error?\",\\n\"What steps should I take when changing the directory when running dbt debug?\" \\n]',\n",
       " '25daead9': '[\"Why does my table appear to not be in the specified location when running a query on BigQuery?\", \"How do I check the locations of my bucket, datasets, and tables are all on the same one in BigQuery?\", \"How do I change the query settings to the location I am in on BigQuery?\", \"What could cause an error if the paths I\\'m using in my query to my tables are not correct in BigQuery?\", \"What are some common solutions to the \\'this table is not on the specified location\\' error in BigQuery?\"]',\n",
       " '2221d75e': 'Here is the list of questions in parsable JSON:\\n\\n[\"Why do DBT Cloud runs get cancelled because a valid dbt project is not found?\", \"What\\'s the reason for this error that my DBT Cloud run was cancelled?\", \"How do I fix the issue where my DBT Cloud run was cancelled due to a missing dbt project?\", \"Why would I get an error that says \\'a valid dbt project was not found\\'?\", \"Can you show me an example of the DBT project directory path in the Project settings on DBT Cloud?\"]',\n",
       " '94524a9d': '[\\n\"What happens when dbt creates a pull request and runs the CI, what new schema is created on BigQuery?\",\\n\"Why do I get errors when creating a new schema on BigQuery if my dataset, schemas, and tables are on a different location?\",\\n\"What are the optional settings for the BigQuery connection in dbt?\",\\n\"How can I change the location for the BigQuery connection in dbt while creating a new schema?\",\\n\"Why do I need to add the location to the BigQuery connection in dbt to avoid errors?\"',\n",
       " '1f1ecbb7': 'Here is the output in parsable JSON:\\n\\n[\"Why do I get an error when trying to run a dbt project on prod after making changes to the repo in another place?\", \"Why does a dbt project fail to be found when running on production environment?\", \"How do I ensure my dbt_project.yml file is accessible in the project?\", \"What should I check if I get an error about a mismatched dataset name between dbt Cloud and BigQuery?\", \"What should I do first when getting an error during dbt deployment to production?\"]',\n",
       " 'c5af32ab': 'Here is the list of questions:\\n\\n[\"How do I fix a `404 Not found` error in dbt saying a dataset was not found in location EU after building from stg_green_tripdata.sql?\", \"What is the default location for dbt Bigquery and why am I getting this error?\", \"Why does bigquery schema default to the US location?\", \"How do I change the location when adding connection details in dbt?\", \"What is the correct step to add location when editing bigquery connection in dbt?\"]',\n",
       " '1e6b7da1': '[\\n\"Can you explain in detail how to append the URL Template link with \\'?raw=true\\' when loading FHV_20?? data from the github repo into GCS and then into BQ?\",\\n\"Why is it necessary to update the URL_PREFIX to a specific value when loading FHV_20?? data from the github repo into GCS and then into BQ?\",\\n\"What are the specific requirements for the URL link when loading FHV_20?? data from the github repo into GCS and then into BQ?\",\\n\"Why do we need to make sure the URL link does not include the keyword \\'tree\\' when loading FHV_20?? data from the github repo into GCS and then into BQ?\",\\n\"How does the URL_PREFIX impact the loading of FHV_20?? data from the github repo into GCS and then into BQ?\"',\n",
       " '259481c4': '[\\n\"How do I upload datasets from GitHub for the homework?\", \\n\"Can I use the same script as provided in Module 3 for uploading data?\", \\n\"What is the easiest way to upload datasets form GitHub for the homework?\", \\n\"Is there a simple way to ingest data from GitHub for the homework?\", \\n\"Can I utilize a similar script to the one provided in Module 3 for uploading data?\" \\n]',\n",
       " 'edbae698': 'Here are 5 questions this student might ask based on the FAQ record:\\n\\n[\\n\"How can I securely store my project credentials\", \\n\"Should I push my credentials to a public git repository\", \\n\"What are some other options to set environment variables besides .env\", \\n\"What is the easiest way to install and use python-dotenv\", \\n\"Can I set environment variables for a specific script instead of the whole project\"',\n",
       " '67217f4c': '[\"What should I do if I encounter errors regarding date types after ingesting FHV data through CSV files?\",\"How can I create an external table in BigQuery to handle date type errors?\",\"Can I define date fields as timestamp in the external table?\",\"How do I ensure that date fields are parsed as strings and then converted to timestamps in the FHV core model?\",\"Can I use TIMESTAMP to convert strings to timestamps in the FHV core model?\"]',\n",
       " '2aadd232': 'Here are the 5 questions based on the FAQ record:\\n\\n[\"Why do I get invalid data types when ingesting FHV data through parquet files?\", \"How do I resolve date type errors while loading data in a landing table?\", \"Can I load all months\\' data in a single run?\", \"Is it possible to create an external table with a specific schema definition?\", \"What is the correct way to handle NULL values in certain columns?\"]',\n",
       " 'adcd914a': '[\\n\"Why do I see an error message when accessing Looker Studio through the Google Cloud Project console?\",\\n\"What is the workaround to avoid the 30-day trial subscription?\",\\n\"Can I still use Looker Studio without buying its Pro version?\",\\n\"Is it possible to access the free version of Looker Studio?\",\\n\"Why do I get prompted to subscribe to the Pro version of Looker Studio?\"\\n]',\n",
       " 'bbf094b3': 'Here are the 5 questions based on the FAQ record:\\n\\n[\"How can I manage dependencies between models in dbt?\", \"Why is my data in FHV not loading into Mage?\", \"Why is there a region mismatch between DBT and BigQuery data?\", \"Can I manually upload my parquet/csv files to GCP?\", \"How can I ensure the correct region is set in my dbt profile?\"]',\n",
       " '2fdc5057': '[\\n\"What is the best way to import data into a table using dbt?\",\\n\"Can I use a more efficient method to load a large dataset into a database?\",\\n\"Is there a way to upload a large CSV file directly to Postgres without having to load it into memory?\",\\n\"How can I use dbt to transform data quickly and efficiently?\",\\n\"What database feature can I use to upload taxi data to dbt-postgres in the shortest amount of time?\"',\n",
       " '95e302f7': 'Here are the 5 questions based on the FAQ record:\\n\\n[\"How do I properly configure the profiles.yml file for dbt-postgres when using jinja templates with environment variables? Is it an error when using \\'5432\\' in strings instead of integers?\", \"Why am I getting an error in dbt-postgres configuration file when using jinja templates with environment variables and is not valid as an integer?\", \"What is the correct way to configure dbt-postgres profiles.yml file when using jinja templates with environment variables when \\'5432\\' is not automatically converted?\", \"In dbt-postgres profiles.yml file configuration when using jinja templates with environment variables, how do I avoid \\'5432\\' being treated as a string instead of an integer?\", \"What is the solution to the issue when configuring dbt-postgres profiles.yml file with jinja templates with environment variables to handle strings as integers?\"]',\n",
       " '1ac2c13c': '[\"How do I install Java and Spark using SDKMAN on Linux?\", \"Can I install multiple versions of Java and Spark using SDKMAN?\", \"What is the point of running sdk install java 11.0.22-tem and sdk install spark 3.3.2?\", \"How do I check the locations and versions of Java and Spark after installation?\", \"Do I need to run source \\\\\"\\\\$HOME/.sdkman/bin/sdkman-init.sh\\\\\" every time I open a new terminal?\"]',\n",
       " '5cc0e4d9': '[\"What is the guide to use Spark in Google Colab if I\\'m struggling to set it up locally?\", \"What are the main challenges with setting up PySpark \\'locally\\'?\", \"Why is it advisable to set up PySpark locally rather than using Google Colab?\", \"How do I connect to SparkUI using Google Colab?\", \"Where can I find a starter notebook for PySpark in Google Colab?\"]',\n",
       " '17090545': 'Here are the 5 questions based on the FAQ record:\\n\\n[\\n\"What does it mean when I get a java.lang(IllegalAccessError) when using spark-shell in Windows?\"\\n\"What environment variables should I set after installing Java, Hadoop, and Spark?\"\\n\"Why am I getting a java.langAccessException when launching spark-shell at the command line?\"\\n\"Why doesn\\'t Spark support Java 17 or 19?\"\\n\"What version of Java should I use with Spark 3.x, especially if I encounter compatibility issues?\"',\n",
       " 'd17e30c6': '[\"What does this error message \\'PySpark - Python was not found\\' mean when I\\'m trying to execute a user-defined function in Spark?\", \"Why is conda causing issues with the PYSPARK_PYTHON environment variable on Windows?\", \"How can I resolve the issue by modifying the environment path setup?\", \"What does \\'pip install findspark\\' do on the command line and why is it necessary?\", \"What does the \\'import findspark\\' and \\'findspark.init()\\' lines of code do in the script and why are they necessary to resolve the error?\"]',\n",
       " '1520b5bc': '[\"What is the reason for the TypeError: code() argument 13 must be str, not int while executing import pyspark in PySpark on Windows with Spark 3.0.3 and Python 3.11?\", \"How can I resolve the TypeError: code() argument 13 must be str, not int while executing import pyspark in PySpark on Windows with Spark 3.0.3 and Python 3.11?\", \"Why does Python 3.11 have inconsistencies with Spark 3.0.3, and what are the implications for PySpark?\", \"How can I downgrade my Python version for compatibility with Spark 3.0.3 to avoid the TypeError: code() argument 13 must be str, not int while executing import pyspark?\", \"Can I use a newer version of PySpark >= 3.5.1 for compatibility with Python 3.11 and Spark 3.0.3?\"]',\n",
       " 'e86ca928': 'Here are the 5 questions based on the FAQ record:\\n\\n[\"How can I easily set up Java and Spark in a miniconda environment on MacOS?\", \"What are some common causes of Py4J errors and how can I troubleshoot them?\", \"Do I need to install findspark with PySpark version 3.5.0?\", \"How can I resolve a Py4J error when writing a DataFrame to a local file on Windows?\", \"What environment variables do I need to set up to work with Spark on Windows?\"]',\n",
       " '3b5b4eb3': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"How do I fix the lsRuntimeError: Java gateway process exited before sending its port number after installing pyspark?\", \"Why do I need to install findspark and what does it do?\", \"How do I check if pyspark is pointing to the correct location and what should I do if it\\'s not?\", \"Can you explain why setting environment variables on system and user environment variables might help with the error?\", \"What\\'s the purpose of running pyspark.__file__ and what do I do with the output?\"]',\n",
       " '489c366f': 'Here are the questions:\\n\\n[\"What can I do if I still get a \\'Module Not Found Error\\' in my Jupyter Notebook after installing PySpark correctly?\", \"How can I resolve issues with installing PySpark using pip in my Jupyter Notebook?\", \"Why do I get a \\'Module Not Found Error\\' when I try to import PySpark in my Jupyter Notebook even after installation?\", \"How can I use PySpark to filter data based on multiple conditions in pandas?\", \"What is the correct command to install PySpark using pip in Linux, as I\\'m getting an error with simply using \\'pip install\\'?\"]',\n",
       " '59381b15': 'Here are the 5 questions based on the FAQ record:\\n\\n[\"How can I resolve the \\'Py4JJavaError - ModuleNotFoundError: No module named \\'py4j\\'\\' error while executing \\'import pyspark\\'?\",\\n\"How do I know which version of Py4J file I am using?\",\\n\"Can I solve the problem by just updating the export command with the correct version of Py4J?\",\\n\"What if adding the correct version of Py4J doesn\\'t resolve the problem?\",\\n\"How can I ensure that the version of \\'py4j\\' in the spark I\\'m using is correct?\"]',\n",
       " '220b1cf3': '[\\n\"What are the steps to solve a Py4J Error when the error message indicates ModuleNotFoundError: No module named \\'py4j\\' and the current version is outdated?\",\\n\"How do I resolve the issue of an outdated py4j version causing a Py4J Error?\",\\n\"What do I need to add to my .bashrc file to solve the Py4J Error when using a module that is not named \\'py4j\\'?\",\\n\"Why do I get a Py4J Error when I try to use a module that is named \\'py4j\\' but it seems to be missing?\",\\n\"What commands should I run to get the latest version of py4j and resolve the Py4J Error?\"',\n",
       " 'd970a0da': 'Here is the list of questions:\\n\\n[\"How do I fix the exception \\'Jupyter command `jupyter-notebook` not found\\' even after I\\'ve exported my paths correctly?\", \"What if I installed Jupyter but it\\'s not working?\", \"Can you provide the full steps to install Jupyter Notebook?\", \"Why do I need to set up a virtual environment and how can I do it?\", \"What\\'s the purpose of using `sudo` and `pip3` in the installation process?\"]',\n",
       " '5fa98bd0': 'Here are the 5 questions that the student might ask based on the FAQ record:\\n```\\n[\"Why do I get a java.io.FileNotFoundException even though I\\'m sure the files exist?\", \"What is happening behind the scenes when I use pyspark and why is it causing errors?\", \"Can you explain why df.write.parquet(pq_path, mode=\\'overwrite\\') is deleting the files it\\'s trying to read?\", \"How can I avoid deleting the files when using df.write.parquet(pq_path, mode=\\'overwrite\\')?\", \"Is there a workaround to get the desired behavior of overwriting the files without losing them?\" ]\\n```',\n",
       " 'ce508f3c': '[\\n\"What happens when I try to write data in pyspark on Windows and I get a FileNotFoundException with Hadoop referring to a missing bin directory?\", \\n\"How do I resolve a FileNotFoundException issue in pyspark on Windows when trying to write data with Hadoop, since there\\'s no bin directory?\", \\n\"Why do I get a FileNotFoundException in pyspark on Windows when writing data with Hadoop, which seems to be looking for a bin directory that doesn\\'t exist?\", \\n\"When I try to write data with Hadoop in pyspark on Windows, I get a FileNotFoundException saying that the Hadoop bin directory doesn\\'t exist. What\\'s going on?\", \\n\"What needs to be done to fix a FileNotFoundException error in pyspark on Windows when trying to write data with Hadoop, specifically regarding a missing bin directory?\" ]',\n",
       " 'b7b9487d': '[\"Which type of SQL is used in Spark?\", \"What are the similarities and differences between SQL providers?\", \"What are the built-in functions in Spark SQL?\", \"How can I check Spark SQL built-in functions?\", \"What is Spark SQL and what are its benefits?\"]',\n",
       " 'a74de125': '[\"Why doesn\\'t the spark viewer appear on localhost:4040 like expected?\", \"Why is the port changed when I\\'m trying to access the spark viewer?\", \"How do I know which port the spark viewer is actually running on?\", \"What happens when a port is in use when running a notebook?\", \"How do I clean up my environment after a port isn\\'t working?\"]',\n",
       " 'e5270303': '[\"What is the solution to the \\'java.lang.NoSuchMethodError: sun.nio.ch.DirectBuffer.cleaner()Lsun/misc/Cleaner Error during repartition call when I\\'m using conda pyspark installation and Java Developer Kit 11?\", \"How can I resolve the \\'RuntimeError: Java gateway process exited before sending its port number\\' error when the notebook log shows that java_home is not set?\", \"Can you provide a link to the article explaining the \\'RuntimeError: Java gateway process exited before sending its port number\\' error that occurs when java_home is not set?\", \"Is there a link to a tweet explaining the \\'RuntimeError: Java gateway process exited before sending its port number\\' error that occurs when java_home is not set?\", \"What version of Java Developer Kit should I use instead of Java Developer Kit 11 to resolve the errors I\\'m encountering?\"]',\n",
       " 'cabe8a5b': '[\"When does Spark fail when reading from BigQuery and using `.show()` on `SELECT` queries?\", \"How can I use `gcs-connector-hadoop-2.2.5-shaded.jar` with Spark 3.1?\", \"What are the necessary authentication files to auth with GCS?\", \"What are the necessary configurations to create a SparkSession to read from BigQuery?\", \"Can I customize the memory settings for the SparkSession?\"]',\n",
       " 'e3c0f777': '[\\n\"How does Spark automatically configure the BigQuery connector when creating a SparkSession?\",\\n\"What is the required configuration setting to enable automatic configuration of the BigQuery connector?\",\\n\"What SparkSession configuration creates BigQuery connector without needing to manage dependencies?\",\\n\"How do I ensure Spark BigQuery connector is correctly configured when using SparkSession?\",\\n\"What is the recommended way of setting up Spark BigQuery connector and dependency jars?\"',\n",
       " '50c009ef': '[\"How do I download the Cloud Storage connector for Hadoop?\", \"Where do I move the .jar file after downloading it?\", \"What classes do I need to import in my Python script to use Spark with GCS?\", \"How do I set up my configurations before building my SparkSession?\", \"Can I read files straight from GCS with PySpark?\"]',\n",
       " '3fe85b16': 'Here is the list of questions:\\n\\n[\\n\"How can I directly read a small number of rows from a parquet file?\", \\n\"What is the alternative way to read a parquet file without using PyArrow?\", \\n\"How do I convert a table to a manageable-sized pandas DataFrame?\", \\n\"Can I read a parquet file directly into a pandas DataFrame?\", \\n\"What is the purpose of sorting and limiting when reading a parquet file?\"\\n]',\n",
       " '0fe0c76a': 'Here is the list of questions:\\n\\n[\\n\"What happens if I get a data type error when creating a Spark DataFrame with a specified schema?\",\\n\"What are some common issues I might encounter when creating a DataFrame with a schema?\",\\n\"Why do I get a Parquet column cannot be converted error when trying to create a DataFrame?\",\\n\"What is causing the error \\'Expected: int, Found: INT64\\' when creating a DataFrame?\",\\n\"How can I resolve a data type mismatch error when defining a schema for a DataFrame?\"',\n",
       " '18c5bafe': 'Here are 5 questions that the student might ask based on the FAQ record:\\n\\n[\\n\"How do I eliminate spaces from column names in PySpark\", \\n\"How to remove leading and trailing spaces from column names in DataFrame\", \\n\"Do you have an example of renaming column names in PySpark?\", \\n\"Can you help me clean up column names with extra spaces in my Spark DataFrame?\", \\n\"How do I replace spaces in column names with underscores in PySpark?\"',\n",
       " '59e86b40': 'Here is the output in parsable JSON:\\n\\n[\"Why do I get an AttributeError when trying to run spark.createDataFrame(df1_pandas).show()?\", \"Why isn\\'t the csv file being used even though we\\'re on a older version of the video?\", \"Why does the pandas version 2.0.0 cause an error with Spark 3.3.2?\", \"How do I fix this incompatibility between pandas and Spark?\", \"Can I add something after importing pandas to solve this problem instead of downgrading pandas?\"]',\n",
       " '1ac3ea8f': 'Here is the list of questions:\\n\\n[\"Why am I getting an AttributeError when trying to iterate over a DataFrame?\", \\n\"How can I resolve conflicts between different versions of pandas?\", \\n\"Do I need to install a specific version of pandas for pyspark 3.5.1?\", \\n\"Can I use environment variables to configure pyspark settings?\", \\n\"How do I add environment variables to my system?\"]',\n",
       " 'e04529ac': 'Here is the list of questions:\\n\\n[\"How do I start a master node in Spark Standalone Mode on Windows?\", \"What is the command to start a worker node in Spark Standalone Mode on Windows and how do I specify its host?\", \"What is the purpose of the --host option when starting a worker node?\", \"How can I access the Spark UI?\", \"Where can I find the correct homework file for Module 5?\"]',\n",
       " 'a602a7f8': '[\\n\"How do I temporarly modify the PYTHONPATH in a new Linux session?\",\\n\"What should I add to my .bashrc file to set the PYTHONPATH permanently?\",\\n\"Can I set the PYTHONPATH before running a new Python script?\",\\n\"How do I make the changes to the PYTHONPATH session-specific?\",\\n\"What commands should I run at the beginning of my homebook to initialize pyspark?\"',\n",
       " '9336ce2c': '[\\n  \"How can I resolve a \\'Compressed file ended before the end-of-stream marker was reached\\' error when trying to read a file in pyspark?\",\\n  \"Why do I need to unzip the file before creating head.csv in pyspark?\",\\n  \"What is the reason behind getting this \\'Compressed file ended before the end-of-stream marker was reached\\' error during the file compression process?\",\\n  \"What steps can I take to prevent the \\'Compressed file ended before the end-of-stream marker was reached\\' error in pyspark data compression?\",\\n  \"What is the purpose of unzipping the file in pyspark before creating head.csv in a given scenario?\"\\n]',\n",
       " 'bac4e0f7': 'Here is the list of questions in the desired format:\\n\\n[\\n\"Can I get a clarification on why zcat output is gibberish when I\\'m trying to uncompress gzipped CSV files?\",\\n\"Why do I need to uncompress the CSV files twice?\",\\n\"I\\'m following along with the code in Video 5.3.3, but I\\'m downloading the data from the course repository instead of the NYT website. Should I still gzip the files?\",\\n\"What causes the compression issue when I run the zcat command?\",\\n\"Can you explain why I should not zip the files downloaded from the course repository?\"',\n",
       " '13dad632': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"What causes a PicklingError: Could not serialise object: IndexError: tuple index out of range when running spark.createDataFrame(df_pandas).show()?\",\\n\"What is the recommended Python version for Spark till date?\",\\n\"How can I create a new environment with a specific Python version on the virtual machine?\",\\n\"What is the advantage of creating a conda environment with python 3.10 over another version?\",\\n\"What is the correct way to switch between different conda environments?\"',\n",
       " 'ddc3c75b': 'Here is the list of questions in JSON format:\\n\\n[\"How do I connect from local Spark to a Google Cloud Storage (GCS) bucket?\", \"Why can\\'t Spark find my google credentials as shown in the video?\", \"What should I do if Spark doesn\\'t recognize my GCP credentials?\", \"How do I ensure my GCP credentials are properly set up for Spark?\", \"What location should I store my GCP credentials in my virtual machine for Spark to recognize them?\"]',\n",
       " '095b667f': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"How do I build Bitnami Spark Docker?\", \"What is the purpose of the Dockerfile in the Bitnami Spark setup?\", \"How do I configure the environment variables in the Docker Compose setup?\", \"What is the significance of the `spark://spark:7077` URL in the Docker Compose setup?\", \"What is the command to deploy the Docker Compose setup?\"]',\n",
       " '56a67c23': 'Here are the 5 questions based on the FAQ record:\\n\\n[\\n\"How do I access data stored in GCS on my local computer using pandas?\",\\n\"How can I read GCS data with my local computer and pandas?\",\\n\"What are the steps to read data from GCS using pandas with my local machine?\",\\n\"How do I download GCS data using pandas on my local computer?\",\\n\"Can I use pandas to read data from GCS storage on my local computer?\"',\n",
       " '7fed7813': 'Here is the list of questions:\\n\\n[\"What is the reason for TypeError when using spark.createDataFrame function on a pandas df?\", \"Why is the type StringType more suitable for the \\'Affiliated_base_number\\' column?\", \"How can I set the suitable type for the \\'Affiliated_base_number\\' column?\", \"Why does \\'inferSchema\\' work more accurately in Spark compared to Pandas?\", \"How can I remove null values in the pandas df before converting it to a pyspark df?\"]',\n",
       " 'a0e7e259': 'Here are the 5 questions based on the FAQ record:\\n\\n[\"What is the default amount of memory allocated to an executor in PySpark? \", \"Why do I get an error saying \\'MemoryManager: Total allocation exceeds 95.00%\\' when working with the homework dataset? \", \"What happened when I increased the row group sizes? \", \"Why did I need to restart the Jupyter session after changing the executor memory? \", \"What does \\'Scaling row group sizes\\' mean in the error message? \"]',\n",
       " '4ca14331': '[\"How do I change the working directory to the spark directory on a Windows OS?\", \"What is the command to start the Spark Master?\", \"How do I start a local Spark cluster?\", \"Can I see the command to create a Local Spark Cluster?\", \"How do I specify the host for the Spark Master and Worker?\"]',\n",
       " '6fdd09eb': '[\"What happens when I set environment variables in ~/.bashrc and why are they not loaded in Jupyter in VS Code?\", \"How do I refresh changes to ~/.bashrc in VS Code?\", \"Are there alternative methods to configuring environment variables instead of using ~/.bashrc?\", \"Can I configure environment variables globally versus locally?\", \"How do I import pyspark in a Jupyter notebook opened in VS Code?\"]',\n",
       " '64bfb2c3': '[\"How can I port forward my code outside VS Code so I can ssh?\", \"Can someone explain how to port forward outside VS Code as I\\'m having trouble understanding the instructions?\", \"I\\'m stuck on port forwarding, can you please provide more information on how to do it outside VS Code?\", \"I\\'m facing an issue with transferring files outside VS Code, how does port forwarding work?\", \"I\\'m new to this, can you explain and provide a step-by-step guide on how to port forward outside VS Code?\"]',\n",
       " '33dd4516': 'Here are 5 questions based on the FAQ record:\\n\\n[\"Why does \\'wc -l\\' give a different result than shown in the video?\",\"How can I get the correct result for \\'wc -l\\' in a gzip file?\", \"What should I do if the results of \\'wc -l\\' are different from the output shown in the video?\", \"How do I ensure I get the accurate output when using \\'wc -l\\'?\", \"What is the cause of the discrepancy between the video result and my \\'wc -l\\' output?\"]',\n",
       " '504b8570': '[\\n\"What should I do when I encounter errors like WARN Utils: Your hostname, <HOSTNAME> resolves to a loopback address..\",\\n\"Why do I get an Exception in thread \\'main\\' org.apache.spark.SparkException when trying to run a pyspark script?\",\\n\"How do I fix the error Setting default log level to \\'WARN\\' and enable a different log level?\",\\n\"What are some common mistakes that cause spark-submit errors\",\\n\"What are the correct syntax and alternatives for the \\'--master\\' option when using spark-submit\"',\n",
       " '42e933c5': 'Here are the 5 questions based on the FAQ record:\\n\\n[\\n\"What should I do if I\\'m seeing an \\'Hadoop - Exception in thread \"main\" java.lang.UnsatisfiedLinkError...\\' error when writing to parquet?\",\\n\"Why might I see an \\'Hadoop - Exception in thread \"main\" java.lang.UnsatisfiedLinkError...\\' error when running pyspark on Windows?\",\\n\"What is causing the \\'Hadoop - Exception in thread \"main\" java.lang.UnsatisfiedLinkError...\\' error when writing to parquet on Windows\",\\n\"How do I set the HADOOP_HOME User Variable to fix the \\'Hadoop - Exception in thread \"main\" java.lang.UnsatisfiedLinkError...\\' error?\",\\n\"What are some additional tips for fixing the \\'Hadoop - Exception in thread \"main\" java.lang.UnsatisfiedLinkError...\\' error on Windows?\"\\n]',\n",
       " 'fe9240b0': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"What do I need to do if I encounter a Java.io.IOException while running pyspark?\",\"Can I use multiple versions of Hadoop on the same Windows machine? And if so, how do I do it?\",\"Is there a specific setup of Hadoop I need to use for pyspark?\",\"How do I createProcess error=216 on windows?\",\"Why am I encountering compatibility issues with the version of Windows I\\'m using?\"]',\n",
       " 'c0a46e5d': '[\"What should I do when I encounter the ERROR: The required property [project] is not currently set while submitting a pyspark job to Dataproc?\", \"How can I temporarily set a project ID when running a Dataproc command?\", \"Can I set a project ID for all future Dataproc commands?\", \"What is the purpose of the --project flag in Dataproc job submission?\", \"How do I get my project ID from Google Cloud Dashboard?\"]',\n",
       " '943c2466': '[\"What steps do I need to follow to run a local cluster of Spark in Windows 10 using the Command Prompt?\", \"How can I run the Spark Master using the command line?\", \"What is the correct syntax for running the Spark Worker? \", \"How do I set up a new Jupyter notebook to use the local Spark cluster?\", \"How can I view the information displayed on the Spark UI for my local cluster?\"]',\n",
       " 'f41ef231': 'Here are the 5 questions emulated by a student based on the FAQ record:\\n\\n[\"What do I need to do if I get an lServiceException: 401 Anonymous caller does not have storage.objects.list access to the Google Cloud Storage bucket?\", \"Why do I need to log in using gcloud auth login when using Google Cloud Storage?\", \"How do I set the project ID using gcloud config set project?\", \"What is the purpose of using gsutil -m cp -r pq/ in uploading a pq directory to a GCS Bucket?\", \"What do I need to do if I close the browser tab after logging in with gcloud auth login?\"]',\n",
       " '6b26d73c': '[\\n\"What can cause a py4j.protocol.Py4JJavaError when submitting a job on GCP?\",\\n\"Why did I get a py4j.protocol.Py4JJavaError when I submitted my job on GCP and how can I resolve it?\",\\n\"What happens if I change the Versioning Control when creating a cluster on Dataproc?\",\\n\"Why did I switch to Ubuntu 20.02-Hadoop3.3-Spark3.3 for Versioning Control on Dataproc?\",\\n\"Can anyone recommend a guide for using the same Ubuntu version on Dataproc as on my laptop and how does it work?\"',\n",
       " '830e2936': '[\\n\"How do I ensure that the Dataframe is repartitioned into 6 partitions using df.repartition(6)?\",\\n\"Why did I get 8 partitions instead of 6 after using df.repartition(6)?\",\\n\"What is the purpose of using both repartition and coalesce in pyspark?\", \\n\"What does df.write.parquet(\\'fhv/2019/10\\', mode=\\'overwrite\\') do in the given context?\", \\n\"In which mode should I write the Dataframe to a file using df.write.parquet?\"',\n",
       " '02007b7c': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"How do I resolve Jupyter Notebook not loading properly at localhost after port forwarding from VS code?\", \"Can I use VS code to forward a port instead of the ssh cli?\", \"Why does the ssh connection break when I log out from the session?\", \"How do I access localhost ports from a GCP VM if I\\'m having trouble accessing them?\", \"What are the common ports that fail to run and what can I do to avoid issues?\"]',\n",
       " '1ebb9a47': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"How do I check for available Java SDK versions in my codespace?\",\\n\"How do I install a specific version of Java on my codespace?\",\\n\"Why do I need to click on Y when prompted to change the default Java version?\",\\n\"Can I see the Java version I have installed after installation?\",\\n\"What if I encounter issues with the Java version I installed?\"]',\n",
       " '80125745': '[\"What are the common causes of insufficient SSD total GB quota error in PySpark?\", \"How can I create a dataproc cluster on GCP when there are insufficient resources?\", \"Is it possible to change the type of boot disk to solve the SSD total GB quota error in PySpark?\", \"Why do regions on GCP sometimes have insufficient resources to allocate a cluster?\", \"What are the possible solutions to overcome insufficient SSD total GB quota error in PySpark?\"]',\n",
       " 'f01df45b': 'Here is the list of questions:\\n\\n[\"How do I convert the time difference of two timestamps to hours using Pyspark?\", \"Can I use a function like datediff to achieve this?\", \"What are the parameters for the datediff function?\", \"How do I convert the result of datediff to hours?\", \"Can I use the total duration in hours without having to manually convert days, seconds, and microseconds?\"]',\n",
       " '06014eec': 'Here are the 5 questions:\\n\\n[\"Why am I getting a PicklingError when trying to serialize my object with PySpark?\", \"How can I resolve the IndexError when working with tuples in PySpark?\", \"Which versions of PySpark and Pandas do I need to use to avoid this error?\", \"Can you provide any additional tips if the error persists after trying the suggested versions?\", \"How can I decode the error message in terms of PySpark and Pandas versions?\"]',\n",
       " '54653ca9': '[\"What happens when a Py4JJavaError occurs while calling o180.showString?\", \"How can I fix an org.apache.spark.SparkException: Job aborted due to stage failure?\", \"Why did Task 0 in stage 6.0 fail 1 times?\", \"What is causing my Python worker to fail to connect back?\", \"How do I run the correct SparkSession before calling showString?\"]',\n",
       " 'f95304db': '[\"How do I resolve the RuntimeError Python in worker has different version 3.11 than that in driver 3.10, PySpark cannot run with different minor versions?\",\"What environment variables need to be correctly set for PySpark to run?\",\"How can I confirm that the minor versions of Python in my worker and driver are the same?\",\"Is it possible to run PySpark on Google Dataproc on Google Kubernetes Engine (GKE)? If so, what are the pricing details?\",\"Can I set the Python interpreter used by PySpark using environment variables?\"]',\n",
       " '591df4e6': '[\"Is it necessary to run Dataproc on Google Compute Engine VM?\", \"Can I use gsutil to submit a job to DataProc from my local computer?\", \"How do I submit a job to DataProc using gsutil commander?\", \"Do I need a local cluster to submit a job to DataProc?\", \"Can I configure gsutil to execute a specific command when submitting a job to DataProc?\"]',\n",
       " '5cb7f597': 'Here is the list of 5 questions in parsable JSON format:\\n\\n[\"What can I do if I\\'m still getting an error after trying multiple times to create a Dataproc Cluster, as mentioned in the FAQ, and receiving an error message saying \\'Insufficient \\'SSD_TOTAL_GB\\' quota\\'?\"\\n\"When trying to run spark.createDataFrame(df_pandas).show() in module 5.3.1, why do I get an AttributeError: \\'DataFrame\\' object has no attribute \\'iteritems\\'?\"\\n\"How do I set up a Dataproc Cluster in DE Zoomcamp 5.6.3, specifically regarding the allocation of memory for master and worker nodes?\"\\n\"What are the specific machine type and primary disk size specifications for master and worker nodes in a Dataproc Cluster?\"\\n\"Why do I get an error when trying to run spark.createDataFrame(df_pandas).show(), saying that the method refers to a package that has been deprecated?\"',\n",
       " 'c5de1f96': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n1. How do I set the JAVA_HOME environment variable on an Apple Silicon mac since the default install location is different from that on Intel-based Macs?\\n2. Why do I have to set JAVA_HOME to /opt/homebrew/ in my .bashrc or .zshrc file when I have an Apple Silicon mac?\\n3. How do I confirm that my path was correctly set after setting the JAVA_HOME environment variable on an Apple Silicon mac?\\n4. What should I expect to see when running the command \"which java\" after setting the JAVA_HOME environment variable?\\n5. What are the reference links provided for installing and setting up Java on Apple Silicon macs?',\n",
       " '70ac8e80': '[\"What are the necessary details to configure in the docker-compose.yaml file\", \"Why did Docker Desktop show running images that weren\\'t displayed using \\'docker ps\\'\", \"How do I troubleshoot configuration errors in the docker-compose.yaml file\", \"Why did deleting old images in Docker Desktop resolve the issue\", \"Can I assume the default configurations are sufficient for starting the control-center service in docker-compose.yaml\"]',\n",
       " 'f6551ffb': '[\\n\"Why does the \\'kafka\\' module not get recognized when trying to run the producer.py file?\",\\n\"How do I create a virtual environment to run the requirements.txt and python files?\",\\n\"How do I activate the virtual environment?\",\\n\"What are the different commands to activate and deactivate the virtual environment?\",\\n\"What are the differences in activating the virtual environment on MacOS, Linux, and Windows?\"',\n",
       " '0ec021de': 'Here is the list of questions:\\n\\n[\"Why do I get an error when trying to import the cimpl dll while running Avro examples?\", \"What causes the \\'Error importing cimpl dll\\' problem when running Avro examples?\", \"How do I fix the \\'DLL load failed\\' error when importing the cimpldll?\", \"What is the best way to avoid encountering \\'The specified module could not be found\\'?\", \"How do I resolve the issue of \\'ImportError: DLL load failed while importing cimpl\\'?\"]',\n",
       " '1edd4630': '[\"Why do I get a ModuleNotFoundError: No module named \\'avro\\' when trying to install confluent-kafka?\",\\n\"What do I need to install in addition to confluent-kafka to resolve this error?\",\\n\"Is it common for Conda to miss including a package when installing via pip?\",\\n\"Why are there issues with installing confluent-kafka on Anaconda?\",\\n\"How can I troubleshoot similar issues related to importing the Producer from confluent-kafka?\"]',\n",
       " '4664ae28': 'Here are the 5 questions:\\n\\n[\"What is the error message I get while running the command python3 stream.py worker?\", \"Why am I getting an error while running the command python3 stream.py worker?\", \"What causes the error while running the command python3 stream.py worker?\", \"What is the use of Redpanda in Kafka?\", \"How does Redpanda differ from Kafka in terms of architecture?\"]',\n",
       " '676e1b76': '[\\n\"What happens when I encounter a \\'Negsignal:SIGKILL\\' error while converting large DTA files to Parquet format?\",\\n\"Can I increase the memory allocation for the Docker container to prevent memory exhaustion?\",\\n\"How can I efficiently process large DTA files to avoid memory issues?\",\\n\"What is a common workaround for resolving memory issues when converting DTA files to Parquet format?\",\\n\"What are some strategies for handling large data files when working with Kafka?\"',\n",
       " 'a3c84279': '[\\n\"What if I can\\'t find the \\'rides.csv\\' file in the Python example?\",\\n\"Why is the \\'rides.csv\\' file missing in the Python example?\",\\n\"Can I use a different file instead of \\'rides.csv\\'?\",\\n\"I\\'m having trouble finding \\'rides.csv\\' for the Python example, can I use the file from the Java example instead?\",\\n\"Is there an alternative to the \\'rides.csv\\' file for the Python example?\"',\n",
       " '119c917d': '[\"What can I do if the audio in the Kafka-Python videos is low and hard to follow?\", \"Where can I find a clear explanation of the Rides.csv data used in the producer.py python program?\", \"Is there a way to improve the audio quality of the Kafka-Python videos without downloading them?\", \"How can I use YT to generate auto captions for the Kafka-Python videos?\", \"Can you show me the Rides.csv data used in the producer.py python program?\"]',\n",
       " 'f1284c1f': '[\"What are the common issues that might lead to the kafka.errors.NoBrokersAvailable: NoBrokersAvailable error in Module 6?\", \"How can I confirm that the Kafka broker docker container is running correctly?\", \"What are the steps to start all the Kakfa instances in the docker-compose.yaml file?\", \"Why do I need to run docker-compose up -d in the docker-compose.yaml file folder?\", \"What does the \\'docker ps\\' command do in the context of Module 6?\"]',\n",
       " '49a7db28': '[\"What is the option in Kafka that supports scaling concept more than the others?\", \"Can horizontal scaling be applied from the consumer end?\", \"How do I think about scaling in Kafka?\", \"Can horizontal scaling be applied for consuming messages?\", \"What does \\'horizontal scaling\\' mean in the context of Kafka?\"]',\n",
       " '196cb0f2': 'Here is the list of questions:\\n\\n[\\n\"How do I resolve issues with DockerCompose pulling the Spark-3.3.1 image?\", \\n\"What do I need to do to make the spark images available for use in my project?\", \\n\"How can I troubleshoot DockerCompose errors related to not being able to pull the spark-3.3.1 image?\", \\n\"What is the step-by-step process to build the spark and jupyter images in the spark folder?\", \\n\"Why do I receive an error message when trying to pull the spark-3.3.1 image from DockerHub and how can I resolve this issue?\"\\n]',\n",
       " '1e50eab7': '[{\"What if I get a \\'./build.sh: Permission denied Error\\' when running the Python Kafka command? Is there a specific solution or fix?\"},\\n {\"How can I use Spark with Kafka in Python, considering I\\'ve got a message saying \\'./build.sh: Permission denied Error\\'?\"},\\n {\"What should I do to overcome this \\'./build.sh: Permission denied Error\\' when building Spark with Kafka?\"},\\n {\"Can you resolve this \\'./build.sh: Permission denied Error\\' in Python when working with Kafka?\"},\\n {\"Why do I get this \\'./build.sh: Permission denied Error\\' while running the build.sh file and how can I fix it?\"}]',\n",
       " 'a7a6d0d7': 'Here are 5 questions based on the FAQ record:\\n\\n[\"What happens when running stream-example/producer.py results in \\'KafkaTimeoutError: Failed to update metadata after 60.0 secs.\\'?\", \"How do I resolve \\'KafkaTimeoutError\\' when running stream-example/producer.py?\", \"Why do I encounter \\'KafkaTimeoutError: Failed to update metadata after 60.0 secs.\\' in stream-example/producer.py?\", \"What should I do if I get \\'KafkaTimeoutError: Failed to update metadata after 60.0 secs.\\' when running stream-example/producer.py?\", \"What is the solution to \\'KafkaTimeoutError: Failed to update metadata after 60.0 secs.\\' issue in stream-example/producer.py?\"]',\n",
       " '0996213a': 'Here are the 5 questions this student might ask based on the FAQ record:\\n\\n[\"Why do I get an error when running ./spark-submit.sh streaming.py with Python Kafka, and how can I resolve it?\", \"What is the reason for the failed connection in the logs of spark-master in the Docker container?\", \"How can I check the Spark version on my local machine, and what information will it provide?\", \"What is the impact of a mismatch between PySpark versions, and how can I avoid this issue?\", \"How do I downgrade my local PySpark to the same version as specified in the Dockerfile?\"]',\n",
       " '311bf368': '[\"What are some common reasons for Spark master connection failures and how do I diagnose the issue in my Python Kafka application?\", \"Can I troubleshoot Spark master connection failures without restarting the driver?\", \"How do I access the logs for Spark master to find out what\\'s happening when it fails to connect?\", \"What are some common error messages I might see when Spark master connection fails and how do I resolve them?\", \"Is there a way to debug Spark master connection failures without having to manually check the logs every time?\"]',\n",
       " 'c1551650': 'Here are 5 questions based on the FAQ record:\\n\\n[\\n\"Why do I get an error while running my Python Kafka job using Apache Spark?\", \\n\"What can cause the Java error in Spark\\'s JavaSparkContext when I run a process?\", \\n\"Can you help me with the error in running my streaming.py file with Spark?\", \\n\"Is there a specific Java version required for Spark to work properly?\", \\n\"How do I select a specific Java version as the default one if I have multiple versions installed?\"\\n]',\n",
       " 'f9b673cf': '[\\n\"How do I fix Java Kafka errors package xxx does not exist even after gradle build with my <project_name>-1.0-SNAPSHOT.jar?\", \\n\"What causes package errors in Java Kafka projects even after successful Gradle builds?\", \\n\"Why does my Gradle build fail to include all necessary dependencies in my Java Kafka project?\", \\n\"How do I properly configure Gradle to bundle all required dependencies in my Java Kafka project?\", \\n\"What are the necessary steps to successfully build and run my Java Kafka project using Gradle?\"',\n",
       " '5479dce2': 'Here are the 5 questions that the student might ask:\\n\\n[\"Can I install Faust Library for Module 6 Python Version due to dependency conflicts?\", \"How do I install confluent-kafka and fastavro for python3 06-streaming/python/avro_example/producer.py?\", \"Are the Faust repository and library no longer maintained?\", \"Can I install the Python Videos 6.13 & 6.14 on YouTube?\", \"Do I need to know Java to understand the concept of streaming in this module?\"]',\n",
       " '02cf2317': '[\"How can I run a Java Kafka producer in the terminal?\", \"Can you provide a command to run a Kafka consumer?\", \"How do I execute a Kafka Streams application?\", \"What is the correct syntax to run multiple Kafka components in the terminal?\", \"Can you give an example command to run a Java Kafka application from my project directory?\"]',\n",
       " '947c07a6': '[\"When running the producer/consumer Java scripts, no results are retrieved or no message is sent?\", \"When running JsonConsumer.java, I got \\'Consuming form kafka started\\' but no actual results?\", \"When running JsonProducer.java, I got an \\'Authentication failed\\' exception?\", \"What is the correct server URL for StreamsConfig.BOOTSTRAP_SERVERS_CONFIG in org/example/JsonConsumer.java?\", \"How do I update the cluster key and secrets in org/example/Secrets.java?\"]',\n",
       " 'bea22953': '[\\n\"Why do I not see the triangle icon next to each test in VSCode?\",\\n\"What are the steps to fix the issue with Java Kafka tests not being picked up in VSCode?\",\\n\"Can you explain how to find the icon next to each test in VSCode?\",\\n\"What is the solution to make VSCode recognize Java Kafka tests?\",\\n\"How can I add classes and packages in VSCode without creating files in the project directory?\"',\n",
       " 'a1603359': '[\\n  \"Where can I find the schema registry URL in Confluent Cloud, as I\\'m trying to set up my streaming with Kafka and don\\'t know where to find this information?\",\\n  \"I\\'m new to using Confluent Cloud, can you please tell me how to find the Stream Governance API to access the schema registry URL?\",\\n  \"I\\'m having trouble finding the schema registry URL in Confluent Cloud, can you point me to the right navigation menu so I can access it?\",\\n  \"I want to create credentials for my schema registry URL in Confluent Cloud, can you tell me where to find the Credentials section?\",\\n  \"How do I find the actual URL for schema registry in Confluent Cloud, as I need to use it for my streaming workflow with Kafka?\"\\n]',\n",
       " 'a85a6a91': '[\"How do I ensure the correctness of my local scala installation?\", \"How can I confirm the compatibility of my locally installed Spark version with the version used in the container?\", \"What are the recommended steps to ensure compatibility of local and container Spark versions?\", \"Can I know the steps to validate the compatibility of my local Spark version?\", \"How do I verify the compatibility between my local Spark installation and the one used in the container?\" ]',\n",
       " '343864f5': 'Here are the 5 questions based on the FAQ record:\\n\\n[\"What is causing the error \"ModuleNotFoundError: No module named \\'kafka.vendor.six.moves\\'\" in my project?\", \"Why do I get a \\'ModuleNotFoundError\\' when importing \\'kafka.vendor.six.moves\\' in my code?\", \"Can anyone help me resolve this \\'ModuleNotFoundError\\' issue I\\'m facing in my project?\", \"I\\'m getting a \\'ModuleNotFoundError: No module named \\'kafka.vendor.six.moves\\'\\'\", \"How do I fix this error \\'ModuleNotFoundError: No module named \\'kafka.vendor.six.moves\\'\\' in my Kafka project?\"]',\n",
       " '6cb3b4a9': '[\"What is the process of evaluating my capstone project?\", \"How many students will evaluate my submitted project?\", \"Will I also be responsible for grading fellow students\\' projects?\", \"What will happen if I don\\'t comply with the peer review rules?\", \"How is my final grade determined in the project evaluation process?\"]',\n",
       " '5959ea3c': '[\"What happens if I fail the first project attempt?\", \"Do I need to submit two projects for this course?\", \"Is there only one project for the entire course duration?\", \"Can I use the second attempt if I don\\'t have enough time?\", \"What are the two chances to pass the course mentioned in the FAQ?\"]',\n",
       " '202af70b': '[\"What are the requirements for submitting our project?\", \"Can we make changes to our project after submitting it?\", \"Are group projects also an option?\", \"How much weightage does the project carry in our final grade?\", \"Does anyone have suggestions for nice and relatively large datasets?\"]',\n",
       " 'f2705fe7': '[\"What is the correct way to execute a Python script as a startup script?\", \"Why can\\'t I submit my project with a non-standard Python version?\", \"Can I use a different version of Python for my project if the one provided by the course has limitations?\", \"Is there a tutorial or guide on how to create a Python environment for my project?\", \"How do I make Python scripts run automatically at startup on different operating systems?\"]',\n",
       " '74f412c4': 'Here is the output:\\n\\n[\"How do I start multiple Spark Streaming queries in the same Spark Session?\", \"Can I read from multiple topics in the same Spark Streaming query?\", \"How do I handle errors in Spark Streaming queries?\", \"Can I use SparkSession to create multiple Spark Streaming queries?\", \"What is the difference between Spark Streams awaitAnyTermination() and awaitTermination() methods?\"]',\n",
       " '5214eb93': '[\"What is the recommended method for transferring data from Databricks to Azure SQL Database?\", \"Can we directly move data from Databricks to Azure SQL Database?\", \"Is there a specific data processing technique for transferring data from Databricks to Azure SQL Database?\", \"How do we handle data transformation during the migration process from Databricks to Azure SQL Database?\", \"What are the best practices for ensuring data integrity during the transformation and migration process from Databricks to Azure SQL Database?\"]',\n",
       " '3cfd16a7': '[\"How do I orchestrate my dbt project with Airflow using the trial dbt account?\", \"What specific steps do I need to take to add a job manually in Airflow using the dbt API?\", \"What information do I need to provide when using the dbt API with Airflow, and how do I avoid committing it to Github?\", \"Where can I find more detailed information about orchestrating dbt with Airflow?\", \"Can you provide an example of using the dbt API with Airflow in Python?\"]',\n",
       " 'a7cecdf9': '[\\n\"How do I orchestrate DataProc with Airflow?\",\\n\"What are the roles that I need to assign to my service account to use DataProc with Airflow?\",\\n\"What are the available operators that I can use to interact with DataProc from Airflow?\",\\n\"What additional configuration do I need to add to the DataprocSubmitPySparkJobOperator?\",\\n\"Does DataProc already have the BigQuery Connector, or do I need to provide it myself?\"',\n",
       " '2aad1011': 'Here is the list of 5 questions:\\n\\n[\"How do I trigger my dbt job in Mage pipeline?\",\\n\"How do I get my dbt cloud api key in the settings?\",\\n\"What is the correct way to add the dbt cloud api key to my project?\",\\n\"How do I create a custom Mage Python block for triggering dbt jobs?\",\\n\"What are the necessary variables and headers required for the HTTP request in Mage pipeline?\"]',\n",
       " 'cb478996': '[\\n\"How can I evaluate if a peer reviewer will be able to follow the documentation provided, even if I put a lot of effort into it?\",\\n\"What if I don\\'t have the time to re-run the entire project, how will I meet the reproducibility criteria?\",\\n\"Are there any alternative measures to re-run the project to ensure reproducibility, besides just re-running everything?\",\\n\"Will spotting potential errors and missing instructions in the code be sufficient for fulfilling the reproducibility criteria?\",\\n\"If I can\\'t re-run the project due to time constraints, are there any specific steps that I can take to ensure that it\\'s still reliable?\"',\n",
       " 'b4ef8ca7': '[\"How can the key vault in Azure cloud store sensitive information for different tech stacks?\", \"What is an example of using the key vault in Azure cloud?\", \"Can I store passwords for SQL databases in the key vault?\", \"How can I use the key vault in other Azure stacks?\", \"Should I expose passwords in my SQL database or use the key vault?\"]',\n",
       " '8e74f943': '[\\n\"How can I get the correct version of py4j when trying to run spark within the docker container if I encounter a `ModuleNotFoundError: No module named \\'py4j\\'` when executing `import pyspark`?\",\\n\"How do I check the version of py4j installed in the spark container?\",\\n\"Is it possible to check the py4j version within the docker container using some command?\",\\n\"What command can I use to check the version of py4j within the docker container?\",\\n\"What is the correct command to execute in order to see the py4j versions available inside the spark docker container?\"',\n",
       " 'a73ed357': '[\"What if psycopg2 complains of an incompatible environment, for example, x86 instead of AMD?\",\"How do I prevent psycopg2 from complaining about incompatible environment issues when using conda and pip together?\", \"Is there a specific way to install psycopg2 when using conda to manage my virtual environment?\", \"Can I use pip to install psycopg2 instead of conda?\",\"How do I handle architecture compatibility issues with psycopg2?\"]',\n",
       " 'd5b6ef5d': 'Here are the 5 questions based on the FAQ record:\\n\\n[\"How do I set up dbt locally with Docker and Postgres?\", \"What are the required fields I need to fill in the profiles.yml file?\", \"Can I create a new project from scratch or do I need to clone the dbt-starter-project?\", \"Why do I need to align the profile name in profiles.yml with the dbt_project.yml file?\", \"What is the purpose of the config-version: 2 line in the dbt_project.yml file?\"]',\n",
       " 'b406d90e': '[\\n\"Can you explain how to connect Pyspark with BigQuery, so I can use the data from my project?\",\\n\"What\\'s the proper way to set up SparkSession to work with BigQuery, as I\\'m having trouble with the initial configuration?\",\\n\"I\\'m having issues integrating my Pyspark project with BigQuery, can you show me an example code snippet for how to do this?\",\\n\"How do I properly configure Pyspark to connect with BigQuery, considering the configuration options provided?\",\\n\"I\\'m new to Pyspark, how do I set up the project to work with BigQuery, including the necessary code and settings?\"',\n",
       " '0002ab8b': 'Here are 5 questions based on the FAQ record:\\n\\n[\"How do I install the astronomer-cosmos package as a dependency?\", \"How do I organize my dbt-core project structure within a Composer GCP bucket?\", \"How do I configure my profiles.yml file for service account authentication?\", \"How do I create a new DAG using the DbtTaskGroup class?\", \"What should the structure of my dbt lineage graph look like when running as an Airflow Task Group?\"]',\n",
       " '138b55c7': '[\\n  \"How can I change my display name on the course leaderboard?\",\\n  \"Can I use URLs from external websites to create tables in BigQuery?\",\\n  \"Do I need to enter my real name when setting up my course profile certificate?\",\\n  \"Can I edit the display name on the leaderboard to a nickname?\",\\n  \"Are there any data stores other than Bigtable and Google Drive that can be used with BigQuery?\"\\n]',\n",
       " '154d7705': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\\n\"How do I ensure that the necessary dependencies are installed to run the code provided in the workshop?\",\\n\"When installing dependencies for the code, should I do it locally?\",\\n\"What is the purpose of installing the \\'duckdb\\' package mentioned in the instructions?\",\\n\"Can I use a different package instead of \\'dlt[duckdb]\\' to run the code?\",\\n\"How do I know if I have successfully installed the required dependencies for the code?\"',\n",
       " 'f96517d9': '[\"What packages are required to run the starter Jupyter Notebook that the teacher has provided? I notice that some packages are missing from the list.\", \"How do I know if I need other packages to run the starter Jupyter Notebook in a new Codespace or Virtual Environment?\", \"Can I assume that all the necessary packages are included in the original list provided by the teacher?\", \"I\\'m trying to run the starter Jupyter Notebook in a new Codespace, but it\\'s not working. Can you explain what packages I need to install?\", \"I\\'m running Jupyter Notebook on a local machine. Do I need to install additional packages in my Virtual Environment?\"]',\n",
       " '773587dd': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\\n\"How do I use dlt with multiple files at once\",\\n\"Can I still use the command line interface for dlt with this in-memory database\",\\n\"Will there be any issues with dlt loading my data into the in-memory database\",\\n\"Can I also use the in-memory database for my large dataset\",\\n\"How do I save my dlt data to the in-memory database in the first place\"\\n]\\n\\nLet me know if this is what you\\'re looking for!',\n",
       " '73aff710': '[\"Why do I only get 8 records after running the homework\\'s dlt Exercise 3, given that the question asks for merging a generator concerns? Are there any specific conditions or steps I should follow to get the desired number of records? Can I use the provided raw string in the FAQ to modify the file path and load the data correctly? Is it possible to filter out individuals with \\'occupation\\' equal to \\'None\\' and calculate their sum of ages in the provided dlt exercise 3? How do I fix the issue of the \\'Load to Parquet file\\' section in the dlt workshop notebook, as it\\'s still saving the dlt files to my C drive despite changing the file path? Can the provided output of parquet files be converted to JSON without using code blocks?\"]',\n",
       " '0728ca67': '[\"What happens when I get a \\'source: no such file or directory: command.sh\\' error while running the workshop?\", \"Why isn\\'t the command.sh file showing up when I use \\'ls\\' to check the repository contents?\", \"How can I make sure I cloned the correct repository?\", \"What do I need to do if the command.sh file is not in the root folder of the repository?\", \"Can you suggest a way to verify that I\\'m in the correct repository directory?\"]',\n",
       " '49a51e24': 'Here are the 5 questions a student might ask based on the FAQ record:\\n\\n[\"Why do we need an alternative to psql instead of using it directly?\", \"Can I still use psql if I\\'m not using a container for PostgreSQL?\", \"How do I install usql on my Mac using homebrew?\", \"Is usql available for all operating systems, or are there other alternatives?\", \"How do I run the taxi_trips.sql script with usql once it\\'s installed?\"]',\n",
       " 'f0d552a7': 'Here are the 5 questions based on the FAQ record:\\n\\n[\\n\"Can I remove the hyphen from \\'docker-compose\\' in the command.sh file if I\\'m getting an error saying it\\'s not found?\",\\n\"What is the typical way to run docker compose, and why is it different in the command.sh file?\",\\n\"Is it possible that docker compose is installed on my system, but still giving an error \\'docker-compose\\' not found?\",\\n\"Can you show an example of how to edit the command.sh file to fix the \\'docker-compose\\' not found error?\",\\n\"Why do I need to specify the file \\'docker-compose.yml\\' in the docker compose command in the command.sh file?\"',\n",
       " '9c750080': '[\"What is the problem with the top-level property \"x-image\" in the Compose file?\", \"Why am I seeing this error when trying to start a cluster?\", \"What are the valid top-level sections for the Compose file?\", \"How can I fix this issue with the Compose file version?\", \"What are some alternatives if I\\'m having trouble updating docker-compose on my Ubuntu machine?\"]',\n",
       " '6f4998e6': 'Here are the 5 questions based on the FAQ record:\\n\\n[\"Is it expected that the records are being ingested 10 at a time?\", \"How do the queries with the now()filter work?\", \"Why is it necessary to change the date timestamp to the current time?\", \"Can I pull the latest changes from the repository?\", \"Can I increase the number of records being ingested at a time?\"]',\n",
       " '97170587': 'Here are the 5 questions:\\n\\n[\"Is kafka install required for the RisingWave workshop?\", \"Can I use a Mac to setup the RisingWave environment?\", \"How do I check if I have the required dependencies installed for the RisingWave workshop?\", \"Do I need to have prior experience with RisingWave to follow the workshop?\", \"Is it possible to do the RisingWave workshop on a virtual machine?\"]',\n",
       " '4def6541': 'Here are the 5 questions:\\n\\n[\"How do I determine the free disk space for all the containers to be provisioned?\", \"Can I use existing taxi data for the psql-ingest process?\", \"What role does the psql still need to run in the setup process?\", \"Why does the psql still need to run in the setup process?\", \"What should be the total free disk space needed for a successful setup?\"]',\n",
       " '66e117dd': '[\\n  \"What should I do if I\\'m experiencing issues when running the stream-kafka script using Psycopg2?\",\\n  \"Why am I getting errors when trying to use psycopg2==2.9.9 in the requirements.txt file?\",\\n  \"Is there a specific change I need to make when running the psql command in a new terminal session?\",\\n  \"How do I ensure I\\'m correctly running the stream-kafka script with Psycopg2?\",\\n  \"What are some common issues I might encounter when using Psycopg2 with the stream-kafka script?\"\\n]',\n",
       " '94fd2476': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\"Why do I need to install GCC in my Anaconda environment?\",\"How do I install GCC on my system?\",\"Why does Conda not have GCC installed by default?\",\"Can I install GCC within my virtual environment or does it need to be installed at the system level?\",\"Why is GCC required to install pyproject.toml-based projects?\"]',\n",
       " '70d83d78': '[\"What are the steps to rectify the Psycopg2 InternalError: Failed to run the query when running the seed-kafka command after initial setup?\", \"How do I activate the python venv from the git bash terminal in Windows?\", \"Why do I need to modify the first line of the seed_kafka.py file?\", \"How do I run the seed-kafka command successfully from the git bash terminal?\", \"How do I connect to the RisingWave cluster from PHP script, or is it only available from PowerShell?\"]',\n",
       " 'accb7285': 'Here is the output:\\n\\n[\"What happens when running a stream-kafka script gets stuck on a loop with Connection Refused?\",\"Why does the message_queue container in docker keep restarting with insufficient physical memory error?\",\"How do I resolve the error when running psql -f risingwave-sql/table/trip_data.sql after starting services with default values using docker-compose up?\",\"Why do I get an syntax error at or near \".\" when running psql -f risingwave-sql/table/trip_data.sql?\",\"How can I fixed the issue of insufficient physical memory error when using docker-compose file in RisingWave?\"]',\n",
       " 'cbca4495': '[\\n\"Can you clarify the requirements for the homework questions in terms of the number of records that need to be processed to obtain the final answer?\",\\n\"What specific configuration allows us to obtain a static set of results for the homework questions?\",\\n\"Is there a minimum requirement for the number of records that must be processed for the homework questions?\",\\n\"Can I get a clarification on how many records are sufficient for the homework questions to get the correct answer?\",\\n\"How many records do I need to process for the homework questions to get the final answer?\"\\n]',\n",
       " '78fce6ad': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"How can I guarantee consistent results when using materialized views?\", \"What are the correct steps to complete the homework assignment?\", \"Why did the provided options in the homework not match the expected answers?\", \"Is it possible to use the order by and limit clause on the materialized view during creation to get consistent results?\", \"What is the difference between using seed-kafka and stream-kafka in the homework assignment?\"]',\n",
       " '68842c02': '[\\n\"How do I install Postgres on a Linux-like operating system for this workshop?\",\\n\"How can I install Postgres using the steps provided, specifically the commands?\",\\n\"What are the required steps to install Postgres on my terminal for this workshop?\",\\n\"What commands do I need to run to install Postgres and its additional features?\",\\n\"Can I check if the Postgres service is running and start it if it\\'s down?\"',\n",
       " '71b1984b': 'Here is the output in parsable JSON:\\n\\n[\"Why cannot I open the RisingWave dashboard with xdg-open?\", \"How do I open the index.html file in a different browser?\", \"Is there an alternative to w3m for opening the dashboard?\", \"How can I open the dashboard if xdg-open is not working?\", \"What are the steps to follow if I am on WSL and having issues with xdg-open?\"]',\n",
       " 'd452b490': 'Here is the list of questions:\\n\\n[\"What are the common error messages we might encounter when trying to execute a Python script in a Unix-like environment?\", \"How can we determine the correct path to the Python 3 interpreter in the current environment?\", \"What is the significance of the \\\\r character in the shebang line and how does it affect script execution?\", \"Can you show an example of a corrected shebang line after updating the interpreter path?\", \"What is the dos2unix command-line tool used for and how does it help with script compatibility?\"]',\n",
       " '707cae8f': '[\"How does the time-based boundary for windowing work in streaming SQL?\", \"Can I specify a custom time interval for windowing in Sql?\", \"How does windowing help in managing and organizing streaming data?\", \"Does windowing also work with row-based boundaries in Sql?\", \"Can I use windowing to analyze data over a specific number of events in Sql?\" ]',\n",
       " 'ffbf3311': 'Here is the list of questions:\\n\\n[\"Why am I encountering the error \"ModuleNotFoundError: No module named \\'kafka.vendor.six.moves\\'\" when running \"from kafka import KafkaProducer\" in Jupyter Notebook for Module 6 Homework?\", \"Why do individual blocks run successfully in the Mage pipeline, but they fail when executing the pipeline as a whole?\", \"How can I resolve the issue of Mage blocks failing to generate OAuth and authenticate with GCP?\", \"How do I set the path to profiles.yml in Mage pipeline?\", \"Can I use dbt pipeline to do data partitioning and clustering in BigQuery?\"]',\n",
       " '3916f4a9': 'Here are the 5 questions based on the FAQ record:\\n\\n[\"How do I create a Docker image from a base image?\", \"What are the basic Docker commands\", \"How do I attach to a stopped container in Docker?\", \"How do I copy files between a host and a container in Docker?\", \"How do I create an SSH key for GCP Compute Engine VM Instance?\"]',\n",
       " '0227b872': '[\"How do I sign up and attend live sessions without internet connectivity?\", \"Can I attend the course with a prior background in computer science, despite lacking direct experience with machine learning?\", \"Are there any prerequisites to take this course besides basic programming skills?\", \"Is it mandatory to attend live sessions or can I learn through pre-recorded videos?\", \"Are the coding exercises hands-on and will I need to install any software to complete them?\"]',\n",
       " '39fda9f0': '=[\"Will the course videos be live and when can I start watching them?\",\\n\"Are the office hours live as well?\",\\n\"Can I access the course videos and office hours from a specific place on YouTube?\",\\n\"Are the course videos recorded?\",\\n\"If the office hours are live, will they also be recorded?\"]',\n",
       " '5170565b': '[\"What if I miss a live session\", \"How can I catch up on the material\", \"What if I have questions after a session\", \"How do I ask questions if I miss a session\", \"Are recordings available after a session\"]',\n",
       " 'ecca790c': '[\"Will you cover a lot of theoretical background in the course?\", \"How much time will be dedicated to mathematical derivations and proofs?\", \"Will I need to write my own code for logistic regression from scratch?\", \"Can I rely on other courses for more in-depth coverage of certain topics?\", \"How much focus will there be on building intuition and understanding key concepts?\"]',\n",
       " 'c25b3de4': '[\"Can I still take the course if I\\'m not familiar with certain concepts?\", \"Are there a lot of formulas in the course?\", \"How will you teach linear algebra in the course?\", \"Will there be a lot of math in the course?\", \"What if I have questions about the linear algebra concepts?\"]',\n",
       " '6ba259b1': '[\\n\"I filled the course form, but I haven\\'t received a confirmation email yet because I thought it might take some time after I hit submit? Is that normal?\",\\n\"I\\'m having trouble receiving course updates because I unsubscribed from the newsletter, am I going to miss out on important information?\",\\n\"Why doesn\\'t the automated system send confirmation emails instantly if it\\'s fully automated?\",\\n\"If my confirmation email goes into my spam folder, can I still join the course and will I still receive course updates?\",\\n\"Can I troubleshoot my email issue by checking my email settings or do I need to contact someone for help?\"',\n",
       " '67e2fd13': ' [\\n\"How long does the course typically take to complete? Can I prolong it?\", \\n\"Are there any additional assignments outside of the regular coursework?\", \\n\"What if I encounter problems or have questions about the course? Who can I contact?\", \\n\"How often will I receive feedback on my work throughout the course?\", \\n\"What if I feel overwhelmed with the workload, can I adjust my pace?\"\\n]',\n",
       " 'a6897e8c': '[\\n\"How do I know if I\\'ve spent the right amount of time on the course?\",\\n\"What if I need more time for a specific module or project?\",\\n\"Can I spend more time on the course if I\\'m really interested in the subject?\",\\n\"Is the course intense or relaxing?\",\\n\"Is there a way to measure if I\\'m dedicating the right amount of time?\"\\n]',\n",
       " '2eba08e3': '[\"What is needed to get a certificate in this course?\", \"Can I customize my certificate?\", \"Do I need to complete all projects to get a certificate?\", \"What if I miss the deadline for project reviews?\", \"How do I know my certificate is valid?\"]',\n",
       " '1d644223': '[\"What if I missed the project deadline, will I still get a certificate?\", \"Is it okay if I don\\'t complete the project on time?\", \"Will a late submission be acceptable for the project?\", \"Can I expect to receive a certificate even if I didn\\'t meet the project deadline?\", \"How will the certificate process be affected if I miss the midterm project submission date?\"]',\n",
       " '14890cd2': '[\"What amount of Python programming experience should I have before taking this course?\", \"How can I improve my Python skills to meet the course requirements?\", \"What are the most important Python basics for data analysis that I need to know?\", \"How can I learn Python basics for data analysis for free using OpenClassrooms?\", \"What are some key concepts I need to cover before starting the course?\"]',\n",
       " 'a4fad482': '[\"What are the particular hardware requirements for the course, as I couldn\\'t find any information on this in the FAQ?\", \"Do I need a specific laptop or will everything be mostly cloud-based?\", \"Can I use any cloud for the Deep Learning part or is Saturn cloud the only option?\", \"Is a working laptop with internet connection sufficient for the Machine Learning part?\", \"Are there any specific laptop hardware specifications that I need to meet for the course?\"]',\n",
       " '34b7fd35': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"Can I set up TensorFlow with GPU support on Ubuntu?\", \"How do I get TensorFlow to work on my Ubuntu machine?\", \"What are the steps to follow to enable GPU support for TensorFlow?\", \"Can TensorFlow be used with a GPU on Linux?\", \"Is there a specific guide for setting up TensorFlow with GPU support on Ubuntu?\"]',\n",
       " '4930aa19': '[\\n\"I\\'m new to Slack and can\\'t seem to find the course channel. How do I locate it? \",\\n\"How do I know when to provide the GitHub link for my code corresponding to homework questions? Do I need to share the URL every time?\",\\n\"Are there any specific rules or guidelines I should follow when joining a Slack channel for a course? Especially since I\\'m new to Slack?\",\\n\"I\\'m having trouble finding the \\'All channels\\' option at the top of my sidebar. Is there a reason why I might not see this option?\",\\n\"When submitting my code on GitHub, what exactly should I provide as the GitHub link to get a grade for my homework? Do I need to provide the entire repo URL or just the specific files?\"',\n",
       " 'ee58a693': '[\"What can I expect if I join the course after it has already started and I\\'m not able to submit all the homeworks?\", \"Can I still get a certificate if I join later in the course?\", \"How do I know which assignments I\\'ll be able to participate in if I join the course in the middle?\", \"Will I still have a chance to take part in the course if I miss the start?\", \"What\\'s the deadline to get a certificate if I join the course?\",]',\n",
       " '636f55d5': '[\"What are the start dates for future iterations of the course?\", \"Can I still enroll in the course even if it\\'s not a cohort iteration?\", \"What does the self-paced mode actually mean?\", \"How often do the cohort iterations take place?\", \"Is there a specific date for the next cohort iteration in 2024?\"]',\n",
       " 'c839b764': '[\"Can I still submit my assignments even if I miss the deadline slightly?\", \"What happens if I miss the deadline completely?\", \"Is it a must for me to do all the homeworks to pass the course?\", \"Will there be any leniency if I submit my assignments late?\", \"Are there any exceptions to the no-late-submission rule?\"]',\n",
       " '0a278fb2': 'Here is the list of 5 questions in Parsable JSON:\\n\\n[\\n\"How can I gain access to all the course materials?\", \\n\"I\\'m having trouble finding the course syllabus, can you give me the direct link?\", \\n\"Are the videos and office hours only available for specific cohorts?\", \\n\"Can I watch the videos on any device or platform?\", \\n\"What if I need help with something during the course, are there any resources available for me?\"\\n]',\n",
       " '8de4fefd': 'Here are 5 questions based on the FAQ record:\\n\\n[\"What are the procedures for submitting assignments on time?\", \"Are there any extensions given for late submissions?\", \"How do I access the course materials\", \"Why do I have to wait for a human evaluator if I can use AI-based tools?\", \"What if I missed the deadline for submitting my assignment? Can I still get an extension?\"]',\n",
       " '94e86808': '[\"What\\'s the difference between the previous iteration of the course and this one, specifically regarding the 2022 and 2023 versions?\", What modules were added or removed from the 2022 version and are still the same in 2023?, What changes were made to the assignments and homework for the 2023 course compared to 2022?, Are there any major differences in the course structure or content between 2022 and 2023?, Are there any modules in this year\\'s course that were not included in the previous iteration?\"]',\n",
       " 'e7ba6b8a': '[\"What is the plan for new course videos compared to the previously recorded ones?\", \"Will you be re-recording the course videos from scratch?\", \"Why are the course videos from the previous iteration still relevant and useful for new students?\", \"Will learning from the old videos put me at a disadvantage?\", \"Can I still benefit from watching the 2021 course videos if I\\'m relatively new to the course?\"]',\n",
       " 'f7bc2f65': '[\"What happens when I post my learning in public links on social media and how do I get extra scores?\", \"How do I format multiple links when submitting my homework?\", \"Is there a limit to the number of points I can get for posting my learning in public links?\", \"Can I post the same content to multiple social sites and still earn the maximum points?\", \"Do the points for midterms/capstones get doubled if I post more links than the limit?\"]',\n",
       " 'ae52a907': '[\\n\"How can I add my own notes to the community notes section?\",\\n\"What is the best way to organize my notes and homework for the course?\",\\n\"Can I use an existing GitHub repository for the course or do I need to create a new one?\",\\n\"Is there a specific format for adding links to my notes in the community notes section?\",\\n\"How do I ensure that my fork is up to date with the original course repository?\"',\n",
       " 'dab5a24a': '[\\n\"How can I compute the hash for the leaderboard and project review?\", \\n\"What is the purpose of the provided Python code?\", \\n\"What is the correct format for calling the compute_hash function?\", \\n\"Is it possible to manually compute the hash using an online tool?\", \\n\"Can I directly enter the hashed email in the \\'research\\' bar of the leaderboard?\"\\n]',\n",
       " '49f9bda9': '[\"Why do I get “wget is not recognized as an internal or external command”?, How do I install wget on Ubuntu?, Can I use Chocolatey to install wget on Windows?, Is there an easier way to install wget on a Mac?, Can I use Python wget library instead of wget command?\"]',\n",
       " 'd44de7d1': 'Here is the list of 5 questions:\\n\\n[\"How can I retrieve a csv file inside a Jupyter Notebook?\", \"Is there a way to execute shell commands directly in a notebook?\", \"Can I download data directly inside a notebook?\", \"How do I move files across directories using shell commands in a notebook?\", \"What are some other common shell commands that can be used in a notebook?\"]',\n",
       " '314ebe32': 'Here is the list of 5 questions:\\n\\n[\"What if I have a Windows 11 device and I want to use the built-in WSL to access Linux?\", \"Is there a specific page I can visit to set up a WSL development environment if I have a Windows 11 device?\", \"Can I remotely connect to my WSL Ubuntu instance from VS Code?\", \"What is the Microsoft verified VS Code extension to download that allows me to remotely connect to my WSL Ubuntu instance?\", \"How will using WSL and VS Code extension allow me to access my WSL Ubuntu instance?\"]',\n",
       " '98cff602': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"What\\'s a good tutorial for using GitHub, especially for first-timers?\", \"How do I fix errors when trying to upload my homework to GitHub?\", \"Why am I getting the error \\'src refspec master does not match any\\' when pushing to my repository?\", \"Is there an alternative to using the command line to upload my homework to GitHub?\", \"Can I directly share my code from Google Colab to my GitHub repository?\"]',\n",
       " '54ec0de4': '[\"What causes singular matrix errors in matrix inversion?\", \"Why do I get a singular matrix error when inverting a matrix?\", \"What should I do when I encounter a singular matrix during matrix multiplication?\", \"Can you explain why not every matrix can be inverted?\", \"How do I ensure I\\'m respecting the order of operations during matrix multiplication?\"]',\n",
       " 'f81f4ecb': '[\"Why does the command \\'conda create -n ml-zoomcamp python=3.9\\' not work in my terminal?\", \"How do I use the Anaconda terminal on Windows?\", \"Why do you recommend installing Anaconda or Miniconda?\", \"What happens if I don\\'t have Anaconda or Miniconda?\", \"Why are versions 3.8, 3.9, and 3.10 acceptable for the command?\"]',\n",
       " 'be760b92': 'Here is the output:\\n\\n[\"How do I get started with machine learning?\",\"I have a CSV file in a specific location on my computer, how do I read it in Python?\",\"What are escape sequences in Python?\",\"How do I load a dataset with Pandas in Windows?\", \"Will the code I write for machine learning work on any operating system?\"]',\n",
       " 'a2cfa1c9': '[\"How do I resolve the \\'403 Forbidden\\' error when trying to push to my GitHub repository?\", \"Can I change the URL of my remote repository?\", \"Why do I need to change the URL format of my repository?\", \"How do I verify the changes made to my repository URL?\", \"What if I\\'m still getting errors after changing the URL format?\"]',\n",
       " '7b907071': 'Here are the 5 questions this student might ask based on the FAQ record:\\n\\n[\"What does it mean when I get an authentication failed error while trying to push my code?\", \"Why was support for password authentication removed by GitHub?\", \"How do I still authenticate with GitHub if password authentication is no longer supported?\", \"What is a personal access token and how do I use it to push my code?\", \"How do I learn more about currently recommended modes of authentication for GitHub?\"]',\n",
       " 'fc2e0a61': '[\"What happens when I get the error \\'wget: unable to resolve host address \\'raw.githubusercontent.com\\'\\' when trying to import data from a public repository in Kaggle?\", \"Why do I have to turn on the Internet in my Kaggle notebook settings, and what is the verification process for phone number?\", \"How can I resolve the temporary failure in name resolution when trying to !wget a dataset from a public repository?\", \"Is this an issue with my connection or the Kaggle platform?\", \"What is the relationship between the Internet setting in Kaggle and preventing wget from resolving the host address?\"]',\n",
       " 'd43e5742': '[\\n\"How do I create a virtual environment for Python using VS Code?\",\\n\"How can I run Jupyter Notebooks in VS Code without opening a web browser?\",\\n\"Can I use VS Code for remote Jupyter Notebooks execution?\",\\n\"How do I work with Git and Github from VSCode?\",\\n\"Is there native Jupiter Notebooks support in VS Code?\"\\n]',\n",
       " '32bc0538': '[\\n\"Do we need to run the command \\'conda create -n .......\\' and \\'conda activate ml-zoomcamp\\' every time we open vs code to work on the project?\",\\n\"Can we create a conda environment with one specific command?\",\\n\"How do we reproduce an existing conda environment?\",\\n\"Can we recreate a conda environment from a YAML file?\",\\n\"Is \\'conda create -n .......\\' only run the first time we create an environment?\"\\n]',\n",
       " 'b6730228': 'Here is the output:\\n\\n[\"Why did I get unexpected results when inverting a matrix in Week1 Homework, even though the inverse should return an Identity matrix when multiplied by the original matrix?\", \"What happened when I multiplied the inverse with the original matrix and got a non-Identity matrix?\", \"Can you explain why floating point precision is an issue when doing matrix operations on a computer?\", \"How does floating point math not work well on computers, and what are the implications for my machine learning work?\", \"Is there a way to avoid or mitigate the effects of floating point precision issues in matrix operations?\"]',\n",
       " '3ce9bbb8': '[\"What is the main purpose of using pandas.DataFrame.info() function?\", \"What information does the pandas.DataFrame.info() function provide about a dataset?\", \"Can you please explain the output of pandas.DataFrame.info() function?\", \"How can we use pandas.DataFrame.info() function to get information about our dataset?\", \"What are some common use cases for pandas.DataFrame.info() function?\"]',\n",
       " '4e584d06': '[\\n\"Can you explain why I\\'m getting a NameError for np and pd variables when I use them in my code?\", \\n\"What libraries do I need to import before using np and pd?\", \\n\"How do I avoid getting a NameError for np and pd variables?\", \\n\"Should I always import numpy and pandas at the beginning of my code?\", \\n\"Why do I get a NameError if I haven\\'t imported numpy and pandas?\"',\n",
       " 'ff4da2b6': '[\"How can I efficiently retrieve columns with numeric data in a pandas dataframe?\", \"Can I filter my dataframe to only include columns with object data?\", \"What if I want to select columns with a specific data type in my pandas dataframe?\", \"How do I get the columns with numeric data in pandas dataframe?\", \"In pandas, is there a quick way to get only the object columns in a dataframe?\"]',\n",
       " '58c1c168': '[\"How do I use .shape to identify the shape of a dataset?\", \"Can I use .shape to identify the number of columns in a dataset?\", \"What if I want to know the total number of rows in my dataset?\", \"Is there a way to inspect my dataset\\'s shape using only a single line of code?\", \"How do I modify the .shape attribute to get the number of rows and columns in my dataset?\"]',\n",
       " '96076a1a': '[\"What are some general tips to avoid Value errors with array shapes in homework?\", \\n\"How can I ensure that the order of matrix multiplication does not affect the result?\", \\n\"What is the most efficient way to perform matrix-matrix multiplication to avoid Value errors?\", \\n\"How do I check that the number of columns in the 1st matrix matches the number of rows in the 2nd matrix?\", \\n\"What are some common issues that can arise when performing matrix-matrix multiplication and how can I troubleshoot them?\"]',\n",
       " '3218389a': '[\"What do we do to replace the NaN values with the average of a column in a dataset? Why do we replace them in the first place if there\\'s valuable information?\", \"How do we figure out the nan values in a column in the dataset? Can we ignore them during our analysis?\", \"What happens if we have nan values in our dataset and we don\\'t replace them? Does it affect our models\\' performance?\", \"Why do we use NaN values in a column if there\\'s valuable information in the rest of the columns?\", \"Is there a common method to replace NaN values in a column with the average of the same column?\"]',\n",
       " '183a1c90': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\\n\"What is the process of calculating linear regression? Can you explain it in detail?\"\\n\"How can I apply the matrix form of linear regression given the target y?\"\\n\"What is the difference between using ordinary least squares and multiple linear regression?\"\\n\"Can you recommend some resources for further learning about the pseudoinverse solution to OLS?\"\\n\"What is the role of the unknown weights w in linear regression? Can you elaborate on their significance?\"',\n",
       " 'f0bc1c19': '[\"Why does the final multiplication not have 5 columns?\", \"What could be the reason for not having 5 columns in the final multiplication?\", \"Why is it likely that I interchanged the first step of the multiplication?\", \"What is the difference between and ?\", \"Why do I need to choose between and ?\"]',\n",
       " '735e6c78': '[\"What is the difference between the * operator and the @ operator when performing matrix multiplication?\", \"How is multiplication performed on sparse or large matrices especially when using numpy?\", \"Can I still use the * operator for matrix-matrix multiplication if I know it\\'s not the recommended approach?\", \"What is meant by element-wise multiplication (Hadamard product) and when would I use it?\", \"When is numpy.multiply() or * operator recommended for multiplication by a scalar?\"]',\n",
       " 'b8ca1cd3': '[\"What should I do if I encounter an error when launching a Jupyter notebook for a brand new environment and I see an ImportError error message?\", \"Can I launch a new notebook without any issues after installing nbconvert using pip in the main environment?\", \"How can I resolve an ImportError error when importing a filter from a jinja2 library?\", \"Is there a specific workaround for launching a new notebook in a brand new environment after installing Anaconda?\", \"Can I upgrade the nbconvert library in a brand new environment without encountering any errors?\"]',\n",
       " 'efdb235f': '[\"What happens when I run wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv on my MacOS Ventura M1 and it keeps hanging?\", \"Is it necessary to go to System Settings > Network > my network connection > Details > Configure IPv6 > set to Manually > OK to fix the issue?\", \"Why do I see IPv6 addresses in the terminal after running the command?\", \"How can I resolve the hanging issue with wget and IPv6 addresses on my MacOS Ventura M1?\", \"What steps should I follow to get wget to work correctly on my network connection?\"]',\n",
       " '355348f0': 'Here is the list of questions:\\n\\n[\"What if I\\'m using a Mac and having trouble with WGET? \", \"How do I retrieve information from the internet if WGET isn\\'t compatible with my system? \", \"What alternative can I use instead of WGET if I\\'m on a Mac? \", \"Can you show me an example of how to use the alternative to WGET? \", \"What does the -o option do in the example provided? \"]',\n",
       " '67afabf5': '[\"How can I limit the decimal places when displaying a result?\", \"Can you show me an example of using the round function to achieve this?\", \"Is there a way to use the round function for an entire pandas Series?\", \"Do you have any examples of using f-strings for this?\", \"Can you recommend a resource for learning more about this?\"]',\n",
       " '50d737e7': 'Here\\'s the output:\\n\\n[\"How do I know what week we are on and what are the important links for each week?\", \"Where can I find the calendar for our weekly meetings?\", \"What is the link for submitting Week 2\\'s homework?\", \"Can I find all the homework files for the course?\", \"What is the GitHub repository for the course theory and where can I find it?\"]',\n",
       " 'bbc0fca3': '[\"How do I determine if my regression data has a long tail? Can I use a histogram to check? What does a histogram tell me about my regression data? Can I calculate the skewness of my data to check for a long tail? How do I calculate the skewness of a variable in Python?\"]',\n",
       " '6f3bdd20': '[\"Why do I get a Singular Matrix error when following the machine learning videos for regression?\", \"What causes a LinAlgError: Singular matrix in machine learning for regression?\", \"Is it normal to encounter Singular Matrix errors during machine learning regression?\", \"Why do I get a Singular Matrix error in addition to the X inverse calculation in my code?\", \"What will be explained in the Regularization video regarding Singular Matrix errors in machine learning for regression?\"]',\n",
       " '27c2d90a': '[\"What is the source of the California housing dataset used in the Machine Learning for Regression course?\", \"How can I find more information about the variables in the California housing dataset?\", \"Is the California housing dataset publicly available?\", \"Can I use the California housing dataset for machine learning projects?\", \"How do I access the detailed description of the California housing dataset provided in the course material?\"]',\n",
       " '88e9600a': '{\"questions\": [\\n\"What causes NaNs after applying the .mean() function when calculating the root mean squared error (RMSE)?\",\\n\"Why do i get NaNs when calculating the RMSE with my for loops?\",\\n\"Can NaNs occur when using for loops to apply RMSE to lists of y_val and y_pred?\",\\n\"Why did my for loops to get RMSE from all seed values give NaNs?\",\\n\"What is the best way to handle NaN values in my dataset to avoid getting NaNs when applying .mean()?\"\"]}',\n",
       " 'd59d8df7': '[\"What are the reasons behind transforming the target variable to logarithm distribution exactly?\", \"Do I need to always transform the target variable in machine learning projects?\", \"How do I know if my target variable is highly skewed?\", \"Why do we need to plot the distribution of the target variable in the first place?\", \"What is skewness and why is it important in machine learning?\"]',\n",
       " '0b3eaf92': '{\"questions\": [\"How can I read the dataset directly from GitHub?\", \"What technique is used to read the dataset from GitHub?\", \"Can I read a CSV file from GitHub using pandas directly?\", \"Is it possible to read a dataset from GitHub without manually downloading it?\", \"How do I access a GitHub dataset without having to copy and paste the URL?\"]}',\n",
       " '8fe56032': 'Here is the list of questions in JSON format:\\n\\n[\\n\"How do I load the dataset directly through Kaggle Notebooks while taking the Machine Learning for Regression course?\",\\n\"What is the correct command to use in Kaggle Notebooks to download a dataset from GitHub?\",\\n\"Can I load multiple datasets from GitHub simultaneously in Kaggle Notebooks?\",\\n\"Do I need to use the \\'!\\'\\' before the \\'wget\\' command when downloading a dataset in Kaggle Notebooks?\",\\n\"How can I read the loaded dataset using pandas in Kaggle Notebooks once it is downloaded\"',\n",
       " 'af833e0a': '[\\n\"How do I filter a dataset by using its values?\", \\n\"How can I use the AND operator to filter a dataset?\", \\n\"Can I use a list of values to filter a dataset?\", \\n\"What is the alternative way to filter a dataset by using its values?\", \\n\"Can I use the OR operator to filter a dataset?\"',\n",
       " '8d209d6d': 'Here is the list of 5 questions based on the FAQ record:\\n\\n[\"How can I load the dataset from the provided URL in a way other than directly from GitHub?\", \"Is there a way to download the dataset using the `requests` library?\", \"Is the dataset that can be loaded from GitHub different from the one that uses `requests`?\", \"Can I use the same `requests` library to load any dataset from GitHub?\", \"What happens if the download fails when using the `requests` library?\"]',\n",
       " '0bc4c3da': '[\\n\"What happens when I create a duplicate of my dataframe and why do I still see a null column despite applying .fillna()?\",\\n\"What is the difference between a shallow copy and a deep copy when creating a duplicate of my dataframe?\",\\n\"What is the purpose of using .copy() when creating a duplicate of my dataframe?\",\\n\"How do I ensure that a null column is not created when making a copy of my dataframe?\",\\n\"What causes the re-appearance of a null column even after filling it with a specific value?\"',\n",
       " 'c0ee2665': '[\\n\"Can I use Scikit-Learn\\'s train_test_split function in my own scatter plot code?\",\\n\"Can I implement it myself again in the future in this course?\",\\n\"What are the main reasons for comparing Scikit-Learn\\'s train_test_split function with your own code?\",\\n\"Is Scikit-Learn\\'s train_test_split function always recommended to use for regression prediction?\",\\n\"Are there any limitations or potential drawbacks to relying solely on Scikit-Learn\\'s train_test_split function for this week?\"\\n]\\n\\nNote: Some questions are slightly rephrased to be more specific and natural-sounding, while still staying very close to the original FAQ record.',\n",
       " '3f60871d': '[\"Can I use LinearRegression from Scikit-Learn for next week\\'s assignment?\", \"How do I preprocess my dataset for use in a regression model?\", \"What should I focus on while implementing regression algorithms?\", \"Are there any specific requirements for choosing the right algorithm for regression?\", \"What are some of the common techniques used to optimize regression model performance?\"]',\n",
       " 'f30217a7': '[\"What are the corresponding Scikit-Learn functions for linear regression models with or without regularization used in week 2?\", \"Can I use the same function for linear regression without and with regularization?\", \"Are the Scikit-Learn functions for linear regression compatible with the ones used in the course so far?\", \"What are the equivalents in Scikit-Learn for models without and with regularization in the context of week 2?\", \"How do I implement regularization in Scikit-Learn for linear regression?\"]',\n",
       " '91fc573d': 'Here are the 5 questions this student might ask based on the FAQ record:\\n\\n[\"What is the `r` parameter in the `train_linear_regression_reg` function if it\\'s not the same as `alpha` in sklearn.Ridge?\", \"What is the role of `alpha` in the sklearn.Ridge() function?\", \"How is regularization strengthened as the value of `alpha` or `r` increases?\", \"Why is `r` used to add \\'noise\\' to the main diagonal of the matrix in the `train_linear_regression_reg` function?\", \"What is the main difference between `r` and `alpha` in terms of the mathematical formulas used in the two scenarios?\"]',\n",
       " 'fe3139f6': 'Here are the 5 questions based on the FAQ record in JSON format:\\n\\n[\"Why do I often have to trade off between fitting a model to the training data perfectly and avoiding overfitting?\", \"Why does the model not \\'overfit\\' the training data by learning the noise?\", \"Why do non-linear models not perform better than linear models even when the relationship between features is non-linear?\", \"How do I avoid the model adapting to random fluctuations in the training data rather than the underlying patterns?\", \"Why does the model perform poorly on new data even if it fits the training data well?\"]',\n",
       " '48aac030': '[\"Why does using the same random seed for splitting my dataset always result in having all missing values in the training set?\", \"How do I ensure that my model generalizes well using a random seed for splitting my dataset?\", \"When using a random seed, why do I get different results for each run?\", \"Can I use any random seed value, or are there specific values that work better?\", \"Why do I need to specify a random seed value when splitting my dataset?\"]',\n",
       " '28321bc2': 'Here are the 5 questions based on the FAQ record:\\n\\n[\"How can I shuffle the initial dataset using pandas\\' built-in function? ?\", \"What will happen if I set frac=1 when using the pandas.DataFrame.sample function? ?\", \"What is the purpose of setting random_state=seed when using the pandas.DataFrame.sample function? ?\", \"Can I use pandasBuilt-in function to get the same randomization used in the course resources? ?\", \"How can I reset the index after shuffling the dataset? \"]\\n\\nNote that I\\'ve made sure to use as few words as possible from the record to formulate the questions.',\n",
       " 'edb92d22': '[\\n\"I saw in the FAQ that differences in computer environments can cause different results for homework questions. Can you explain why this might happen and how it affects our work in this course?\"\\n\"What are some common differences between computer environments that might cause discrepancies in our results?\"\\n\"I\\'m trying to select the closest option for a homework question with an unconventional answer. Can you give me some tips on how to do this effectively?\"\\n\"I noticed some students on the discussion board mentioned different Python versions being used. Can we get clarification on which version is recommended for this course?\"\\n\"Are there any specific libraries or software that we need to have installed on our computers to match the environments used in the course?\"',\n",
       " 'f488ce85': 'Here is the list of questions in JSON format:\\n\\n[\\n\"Can you explain what we should compute the mean of exactly in question 3 of homework 2?\",\\n\"What are the different data sets that are being referred to in the homework 2, question 3?\",\\n\"What do we use to calculate the mean for the training data set in homework 2, question 3?\",\\n\"What kind of results can we expect when we calculate the mean using only the training data set in homework 2, question 3?\",\\n\"In homework 2, question 3, can we use any other data set to calculate the mean or do we have to use only the training data set?\"\\n]',\n",
       " 'bf395099': '[\"When should we consider transforming the target variable for regression with machine learning?\", \"Can we use logarithm transformation for target variables with negative values?\", \"How do we decide whether to transform the target variable for regression?\", \"Can the target variable have other types of distributions besides the long tail distribution?\", \"What method should we use to perform logarithmic transformation for the target variable?\"]',\n",
       " '01cd3b35': '[\"What are the scenarios when broadcasting of shapes or dimensions can occur or fail?\", \"Why does the * operator solve the ValueError: shapes not aligned error when the dot() method doesn\\'t?\", \"What is the difference between performing arithmetic operations with the dot() method and the * operator?\", \"Can you provide examples of when we might need to use one over the other?\", \"How does the * operator convert the error to a dot product when the shapes are not aligned?\"]',\n",
       " '5551c92e': '[\"How do I copy a dataframe without affecting the original dataframe?\", \"Why does copying a dataframe only provide a view?\", \"Is there a difference between copying a dataframe in Python?\", \"How can I ensure that changes to a copied dataframe don\\'t affect the original?\", \"What happens if I use X_copy = X to copy a dataframe?\"]',\n",
       " '94f928d2': 'Here are 5 questions based on the FAQ record:\\n\\n[\"What is the difference between the normal distribution and the long tail distribution?\", \\n\"What is meant by \\'under the same value\\' in the context of the normal distribution?\", \\n\"Can you explain why the mean is no longer representative in the long tail distribution?\", \\n\"In the normal distribution, what is the relationship between mean, median, and mode?\", \\n\"Why is the range larger in the long tail distribution compared to the normal distribution?\"]',\n",
       " '266faa6d': '[\"What is meant by a low standard deviation in statistics?\", \"How does a high standard deviation affect the values in a set?\", \"Can you explain what variation or dispersion refers to in the context of standard deviation?\", \"How does the standard deviation change if the values in a set are spread out?\", \"What is the formula for calculating standard deviation given in the answer?\"]',\n",
       " 'c21f99f5': '[\\n\"Do we need to apply regularization techniques to prevent overfitting with small datasets?\",\\n\"What are the conditions under which regularization techniques are necessary for complex models?\",\\n\"Why is it generally recommended to consider regularization when training machine learning models?\",\\n\"Can you provide an example of a scenario where regularization is not necessary?\",\\n\"Do machine learning models without regularization always suffer from overfitting?\"\\n]',\n",
       " '13702957': 'Here is the output in parsable JSON format:\\n\\n[\"How can I speed up the execution of my code by preparing dataframes and y_vectors?\", \"Is it necessary to use methods like rmse() and train_linear_regression() from the class notebook?\", \"Can fillna() be applied before splitting the initial dataframe?\", \"Are there other functions I can reuse besides prepare_df()?\", \"What benefits does preparing functions in advance provide for my development process?\"]',\n",
       " '7cd652c5': '[\\n\"What is the correct way to use pandas to calculate the standard deviation of a list of data?\",\\n\"How do I find the standard deviation of a pandas series?\",\\n\"Can I use pandas to calculate the standard deviation of a list of data?\",\\n\"Is there a pandas function that directly calculates the standard deviation of a series?\",\\n\"How do I use pandas to determine the variance of my data?\"',\n",
       " 'e1f93d10': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\\n\"What is the difference in how Numpy and Pandas calculate standard deviation?\",\\n\"Does Numpy\\'s standard deviation computation use the same assumption as Pandas\\'?\",\\n\"Why does Pandas calculate standard deviation using one degree of freedom by default?\",\\n\"Can I change the degree of freedom in Numpy to make it similar to Pandas\\' unbiased estimator?\",\\n\"How do I tell Numpy to use the unbiased estimator like Pandas\\' sample standard deviation?\"',\n",
       " '36b9d1b7': 'Here are 5 questions based on the FAQ record:\\n\\n[\"How do I get the standard deviation of a Pandas DataFrame column?\", \"Can I get the standard deviation of multiple columns at once?\", \"What is the syntax to use the Pandas std() function?\", \"Can I use the Pandas std() function to calculate the standard deviation of multiple columns at once?\", \"How do I get the standard deviation using Pandas without using the built-in function?\"]',\n",
       " '3c8b32a1': 'Here are the 5 questions based on the FAQ record:\\n\\n[\\n\"How can I combine the training and validation datasets?\", \\n\"What are some methods for combining two dataframes in pandas?\", \\n\"Can I use a function to concatenate numpy arrays?\", \\n\"How do I combine the y-values from the training and validation sets?\", \\n\"What is the purpose of the pd.concat function in this context?\"\\n]',\n",
       " '05fb3a16': 'Here are the 5 questions that a student might ask based on the FAQ record:\\n\\n[\\n\"How do we understand the Root Mean Squared Error (RMSE) metric in regression models?\",\\n\"How do we calculate the average deviation between predicted and actual values in a regression model?\",\\n\"Can you explain how to use Python libraries to calculate the RMSE score?\",\\n\"What is the purpose of using the Root Mean Squared Error (RMSE) to evaluate a regression model\\'s performance?\",\\n\"How does the Root Mean Squared Error (RMSE) compare to other metrics for evaluating regression model performance?\"',\n",
       " '225506b9': 'Here are the 5 questions that a student might ask based on the FAQ record:\\n\\n[\\n\"What is the syntax for using logical AND and OR in Pandas for multiple conditions?\",\\n\"Can I use the ampersand (&) symbol for OR operations in Pandas?\",\\n\"How do I combine multiple conditions using logical operators in Pandas?\",\\n\"What is the correct syntax for using the OR operator in Pandas?\",\\n\"Is there a specific syntax for combining multiple conditions using the AND operator in Pandas?\"\\n]',\n",
       " 'bd4a1395': '[\\n\"How can I ensure I\\'m getting the most out of the video explaining the normal equation for regression?\", \\n\"What software tools are commonly used to implement the normal equation for regression in real-world scenarios?\", \\n\"Is there a way to apply the normal equation for regression for a polynomial regression model?\", \\n\"Can the normal equation for regression be used with other machine learning algorithms besides linear regression?\", \\n\"How does the normal equation for regression compare to other methods for solving linear regression, such as gradient descent?\"',\n",
       " '81b8e8d0': 'Here is the list of questions:\\n\\n[\\n\"How do I handle missing data in machine learning for regression problems?\", \\n\"What are some ways to treat missing values in my dataset for regression task?\", \\n\"Can you recommend a useful resource for learning about missing data treatment in regression models?\", \\n\"In regression tasks, what are the common ways to address missing values in the data?\", \\n\"What techniques or methods are used to handle missing values in regression analysis?\"',\n",
       " 'a7f6a33c': 'Here is the output:\\n\\n[\\n\"Can you confirm the instruction for applying log transformation to the target variable in the Week-2 homework, as I got stuck on Q5?\", \\n\"What is the recommended approach for applying log transformation in Week-2? Should I include it in the preprocessing step?\", \\n\"I understand I need to apply log transformation to the target variable, but can you remind me where I can find the instruction for doing so in the Week-2 homework?\", \\n\"I\\'m having trouble understanding the log transformation process for Week-2. Can you provide more details on when to apply it?\", \\n\"I seemed to forget to apply log transformation to the target variable in Week-2, resulting in a huge RMSE. How can I avoid this mistake in the future?\"\\n]',\n",
       " '129b4ac0': 'Here are 5 questions that a student might ask based on the FAQ record:\\n\\n[\"What version of sklearn does Alexey use in his YouTube videos\", \"Can I use Python 3.8.11 in the course projects\", \"Is Diego Giraldo the only one who has contributed to the FAQ section\", \"Can I use earlier or later versions of Python in the course projects\", \"Does the course only focus on classification of data with machine learning\"]',\n",
       " 'b8cca8b7': 'Here is the output in parsable JSON format:\\n\\n[\"What is the link to access Week 3\\'s homework?\", \"Where can I submit my Homework Week 3?\", \"Where can I find all the homeworks for this course?\", \"What is the link to the Evaluation Matrix?\", \"Can I also access the theory materials on GitHub?\"]',\n",
       " '1091b10f': 'Here is the list of questions:\\n\\n[\"What are the possible causes of the error message \\'could not convert string to float:\\'\", \"How can I resolve the issue where a machine learning model is expecting numerical input but receives a string instead?\", \"Can you provide an example of how to perform one-hot encoding in pandas?\", \"How do I encode categorical variables like car brands into numerical values?\", \"What are some common methods for encoding categorical variables?\"]',\n",
       " '0c7715a1': '[\"Why do we have to change the targets to binary format when calculating mutual information score?\", \"Why was a binary format used instead of the original target format?\", \"What\\'s the reason behind changing the target to binary\", \"What happens if we didn\\'t convert the target to binary format?\", \"Why can\\'t continuous variables be used for mutual information score calculation?\"]',\n",
       " 'd2043cf5': '[\"What data should we use to convert median_house_value from numeric to binary? Knowing the correlation matrix\", \"What is the training dataset in the context of the correlation matrix?\", \"How do we ensure we are dealing with the right dataset when calculating the correlation matrix?\", \"Can we make conclusions based on the validation data in this context?\", \"Why are we not considering df_train_full when calculating the correlation matrix?\"]',\n",
       " '44d22817': 'Here are 5 questions that the student might ask based on the FAQ record:\\n\\n[\"How do I color the background of a pandas DataFrame?\", \"Can I use the same method to color other dataframes?\", \"What are some examples of color maps I can use to color the background?\", \"Can I use the background gradient method to color a dataframe with non-numerical values?\", \"How do I apply the background gradient method to a correlation matrix in a pandas DataFrame?\"]',\n",
       " '1f76dbeb': 'Here are the 5 questions based on the FAQ record:\\n\\n[\"How can I easily identify highly correlated feature pairs in my dataset?\", \"Can I use a library other than pandas to work with correlation matrices?\", \"How do I visualize the correlation between numerical features?\", \"Can I plot the correlation between just a subset of features?\", \"How can I create a more complex heatmap, such as one that only shows upper triangle and in different colors?\"]',\n",
       " 'b8071a54': '[\"What data set should I use for Exploratory Data Analysis when performing EDA on my training data?\", \"I noticed the FAQ mentioned including validation data for EDA, what\\'s the preferred practice?\", \"Can the test data be used for any part of EDA?\", \"Is it really necessary to avoid the test data during EDA?\", \"Can you elaborate on why we shouldn\\'t look at the test data even for just a glance?\"]',\n",
       " 'b8da9037': 'Here is the list of questions:\\n\\n[\"What happens when you apply the fit method on a DictVectorizer instance?\", \"How does the fit method figure out how to map the values of dictionary?\", \"Why is it not recommended to fit the validation model with DictVectorizer?\", \"Do I need to initialize another instance of DictVectorizer after fitting it on the train set\", \"How should I use the fit_transform function with DictVectorizer?\"]',\n",
       " '467e0cec': '[\\n\"What is the correct approach for calculating the smallest difference in accuracy when performing feature elimination in homework? Should we consider the smallest absolute difference or the smallest real difference?\",\\n\"In the context of feature elimination, what exactly does it mean when the difference in accuracy is negative?\",\\n\"What is the key difference between the \\'smallest\\' and \\'lowest\\' in the context of feature elimination?\",\\n\"How does the outcome of feature elimination change if the difference in accuracy is negative?\",\\n\"Can you provide more clarification on why we should reason in absolute values when determining the smallest difference in accuracy?\"',\n",
       " 'b69f32f6': 'Here is the output:\\n\\n[\"What should we use instead of the deprecated function get_feature_names to resolve the FutureWarning?\", \"Why is the get_feature_names function deprecated?\", \"What is the alternative method to use for DictVectorizer to get feature names?\", \"Why aren\\'t we worried about the warning since there won\\'t be any after the update?\", \"How can we resolve the warning for the get_feature_names function?\"]',\n",
       " '3b3b1989': '[\"How can I avoid my logistic regression model crashing the Jupyter kernel when I call predict()?\", \"Why does fitting the logistic regression model take so long?\", \"Is there a specific way to format my target variable for the logistic regression model?\", \"What do I need to do to prevent the kernel from crashing when calling predict() with my fitted model?\", \"Can logistic regression only be used for binary classification problems?\"]',\n",
       " 'eb5771a0': '[\\n\"What does Ridge regression do to mitigate multicollinearity and prevent overfitting in predictive modeling?\",\\n\"What is the sag solver and why is it particularly suitable for large datasets?\",\\n\"What affects the strength of regularization in Ridge regression?\",\\n\"What is the effect of a higher alpha value on the model\\'s coefficient values?\",\\n\"How do I use Ridge regression with scikit-learn?\"\\n]',\n",
       " 'bca10281': 'Here is the list of questions:\\n\\n[\"What is the main difference between DictVectorizer(sparse=False) and DictVectorizer(sparse=True)?\", \"Can I use pandas.get_dummies() and DictVectorizer(sparse=False) for high amount of classes with no convergence warnings?\", \"Are the results produced by DictVectorizer(sparse=False) and pandas.get_dummies() \\'worse\\' compared to DictVectorizer(sparse=True)?\", \"Does DictVectorizer(sparse=True) store non-zero values and indices in a particular format?\", \"Why does using \\'sparse\\' format take longer and produce worse results for high amount of classes?\"]',\n",
       " '34a8edb0': 'Here are 5 questions based on the given record:\\n\\n[\"What happens if I don\\'t scale the features correctly in Ridge regression with sag solver?\", \"How can I fix a ConvergenceWarning when training a model?\", \"What kind of scalers should I use for numeric and categorical features in a classification problem?\", \"Why is it important to separate numeric and categorical features when using OneHotEncoder?\", \"Can using StandardScaler for numeric fields and OneHotEncoder for categorical features solve convergence problems?\"]',\n",
       " 'f625307b': '[\"What are the steps to take when encountering convergence errors during the training of a Ridge regression model? How can I ensure my numerical features are on a similar scale to prevent convergence issues?\", \"How do I handle categorical features in my dataset when training a Ridge regression model?\", \"Can I combine features after normalizing numerical features and encoding categorical features?\", \"What encoding method is recommended for handling categorical features in a Ridge regression model?\", \"Where can I find an example of how to address convergence errors in Ridge regression model training?\"]',\n",
       " '7fa98526': '[\"How does a sparse matrix compare to a dense matrix in terms of memory usage?\", \"Is the default DictVectorizer configuration a sparse matrix?\", \"Is it commonly used with large datasets that contain many zero or missing values?\", \"What are the benefits of using a sparse matrix when training a model for week3 Question 6?\", \"Is using the default sparse matrix more performant and less prone to error messages compared to dense mode?\"]',\n",
       " '0807f0f3': '[\"How do I handle the warning messages in Jupyter Notebooks? \", \"What can I do to avoid getting warnings in my Jupyter code? \", \"Can I disable the warnings in Jupyter Notebooks? \", \"How can I turn off warning messages in my Jupyter Notebook? \", \"Are there any ways to ignore warning messages in Jupyter? \"]',\n",
       " '6d0fb418': '[\"How do we decide on the correct score to choose in the context of RMSE?\", \"How do we determine the lowest alpha if the RMSE scores are equal?\", \"Can you explain the process of selecting the correct alpha in a study group?\", \"What is the general formula for finding RMSE?\", \"How would you describe the best approach to selecting the correct alpha in this specific homework problem?\"]',\n",
       " 'fbda1f40': '[\"What are we supposed to calculate the mutual information score with for HW3 Q3?\", \"How do I calculate the mutual information score with the binarized price?\", \"What is the relevance of the training set in calculating the mutual information score?\", \"What is the significance of using only the original categorical variable?\", \"Can we use any categorical variable in calculating the mutual information score?\"]',\n",
       " '0f88b7ac': '[\"What are the specific features required for the homework? Do we need to train the model using only the features total_rooms, total_bedrooms, population, and households, or can we use all the available features?\", \"How do we calculate the difference between accuracy scores when dropping one feature at a time? Are we supposed to take the smallest or smallest absolute difference?\", \"What is the specific approach for comparing accuracy scores during training? Do we train the model once and then drop features one by one, or do we have to train the model once for each feature?\", \"How do we determine which feature has the smallest absolute accuracy difference?\", \"When calculating absolute differences, do we have to take into account the order of subtraction between the two accuracy scores?\"]',\n",
       " '9ffcc895': '[\"What is the difference between OneHotEncoder and DictVectorizer, and how do they handle categorical features for machine learning classification?\", \"Can I use an array as input for DictVectorizer, and how does it compare to using an array with OneHotEncoder?\", \"How do the sorting and stacking of features differ between OneHotEncoder and DictVectorizer?\", \"For specific use cases, which of these encoders is generally more suitable for conversion from categorical to numerical variables?\", \"What are the advantages of using DictVectorizer over OneHotEncoder, and vice versa, in terms of converting categorical features for machine learning classification?\"]',\n",
       " '94a3b2fb': '[\\n\"What is the main difference between pandas\\' get_dummies and scikit-learn\\'s OnehotEncoder when it comes to one-hot-encoding categorical variables?\",\\n\"How do the input and output types of pandas\\' get_dummies and scikit-learn\\'s OnehotEncoder differ?\",\\n\"What happens to missing values when using pandas\\' get_dummies and scikit-learn\\'s OnehotEncoder?\",\\n\"Why would I choose pandas\\' get_dummies over scikit-learn\\'s OneHotEncoder in a machine learning pipeline?\",\\n\"What are the ideal scenarios for using pandas\\' get_dummies and scikit-learn\\'s OneHotEncoder respectively?\"',\n",
       " 'fb9a45d8': '[\\n\"How do I correctly use the random_state parameter in the test_train_split function for the week 3 homework?\", \\n\"What is the advantage of reusing the same random_state for both train and test sets in the test_train_split function?\", \\n\"In the week 3 homework, do I need to feed the random_state parameter in the train_test_split function multiple times for both train and test sets?\", \\n\"Will changing the random_state parameter value have a significant impact on my machine learning model\\'s performance in the week 3 homework?\", \\n\"Can I use any random number as the random_state in the train_test_split function for the week 3 homework, or is there a specific value that is recommended?\"\\n]',\n",
       " 'e31051f7': '[\"How do I know when to calculate correlation in machine learning, specifically whether it should be done before or after splitting the data?\", \"How do I determine the two most correlated features in my dataset?\", \"Can I find the correlation matrix of my train dataset using a specific method?\", \"Should I calculate the correlation matrix before or after splitting my data, and what is the impact of this choice?\", \"How can I determine the two most correlated features in terms of their correlation coefficient?\"]',\n",
       " '493b7b59': '[\"What type of features should I use in a Ridge Regression model in terms of numerical or categorical values?\", \"Can I use categorical features in a Ridge Regression model?\", \"How do I handle categorical features in a Ridge Regression model?\", \"Why do I need to transform categorical features before training a machine learning model?\", \"What is sparse=True used for when transforming categorical features?\"]',\n",
       " '4a55c510': 'Here are the 5 questions:\\n\\n[\"How do I handle column information for Homework 3 Question 6?\", \"Do I need to use all features for the target in Question 6?\", \"Should I include the average variable created before in Question 6?\", \"What should I do if I encounter convergence errors when using DictVectorizer in Question 6?\", \"Should I use StandardScaler for numerical variables when solving Question 6?\"',\n",
       " '3ca0b489': '[\\n\"How do I transform categorical columns into numerical columns without losing information?\", \\n\"How can I handle encoding and scaling in a way that preserves the order of ordinal categorical data?\", \\n\"Can I use the same encoder for both categorical and numerical data?\", \\n\"Is there a recommended order for applying preprocessing techniques to categorical and numerical columns?\", \\n\"Are there any best practices for selecting the appropriate encoder and scaler for my specific classification problem?\"\\n]',\n",
       " '690d97f1': '[\"What is the better option between FeatureHasher and DictVectorizer for categorical features?\", \"I have a high cardinality for categorical features, which method is recommended?\", \"Is it better to use FeatureHasher or DictVectorizer when preserving feature names is important?\", \"Can I use FeatureHasher to store the big vocabulary, or is it a bad idea?\", \"Is there a specific advantage of using FeatureHasher over DictVectorizer?\"]',\n",
       " 'eb5a25cb': 'Here is the output:\\n\\n[\"Isn\\'t it easier to use DictVertorizer or get dummies before splitting the data into train/val/test? Is there a reason we wouldn\\'t do this? Or is it the same either way?\", What is the reason to do text feature engineering after splitting data into training and test sets?, Can you use the same data to train the model and to evaluate its performance? Won\\'t using different data for each purpose cause data leakage?, Is it not better to use a simpler method to handle missing values in the test set instead of imputing them with mean/median, like replacing them with a separate class instead?\",\"Why do we need to handle overlapping words in text classification?\"]',\n",
       " '6d9e0a6f': '[\"What should I do if I\\'m getting an accuracy of 1.0 for my model?\", \"Can dropping a column solve overfitting?\", \"How can I adjust my model to avoid 1.0 accuracy?\", \"Is overfitting always accompanied by an accuracy of 1.0?\", \"How can I identify if my model has overfitted?\"]',\n",
       " '618ad97a': '[\\n\"How do we implement Root Mean Squared Error calculation in our classification models?\", \\n\"What packages do we need to use in order to calculate Root Mean Squared Error?\", \\n\"Can we use a function to calculate Root Mean Squared Error instead of writing the formula manually?\", \\n\"Is there a way to improve the accuracy of our Root Mean Squared Error calculation?\", \\n\"How can we apply the Root Mean Squared Error formula to real-world data using Python?\"\\n]',\n",
       " '683495d2': '[\"What are some common errors I might encounter when using the DictVectorizer object in scikit-learn, and how can I resolve them?\", \"How do I access the feature names generated by the DictVectorizer object?\", \"I\\'m getting the AttributeError \\'DictVectorizer\\' object has no attribute \\'get_feature_names\\' when trying to use the object in my code. Why is this happening?\", \"Can you provide some more details or documentation on how to use the \\'get_feature_names\\' method in DictVectorizer?\", \"What is the difference between \\'get_feature_names\\' and \\'get_feature_names_out\\' methods in DictVectorizer?\"]',\n",
       " 'dc1897b5': '[\"How can I calculate Root Mean Squared Error without using math or numpy?\", \"What is the difference between mean squared error and root mean squared error?\", \"Can I calculate root mean squared error from actual and predicted values?\", \"Does sklearn.metrics have a function for calculating root mean squared error?\", \"How can I use the mean_squared_error function in sklearn.metrics to calculate root mean squared error?\"]',\n",
       " '826098f2': 'Here are 5 questions that a student might ask based on the FAQ record:\\n\\n[\\n\"Can you provide more information on the different encoding techniques for categorical variables?\",\\n\"What are some common pitfalls to avoid while using encoding techniques?\",\\n\"How do encoding techniques affect the performance of machine learning models?\",\\n\"Can you recommend some resources for learning more about encoding techniques for categorical variables?\",\\n\"What are some best practices for selecting the right encoding technique for a given problem?\"',\n",
       " '821dfc08': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\"What is the correct way to use accuracy_score from sklearn in a Jupyter notebook?\", \"Why do I get a TypeError when using accuracy_score?\", \"How do I fix the error when using accuracy_score?\", \"Can you provide an example of correct syntax for accuracy_score?\", \"Why is my accuracy_score not working as expected?\"]',\n",
       " '27c8d5da': 'Here are the 5 questions based on the FAQ record:\\n\\n[\\n\"How do I access the Week 4 homework?\",\\n\"Where can I find all the homework assignments for this course?\",\\n\"What does the Evaluation Matrix document contain?\",\\n\"How can I access the course notes and theory on GitHub?\",\\n\"What YouTube video covers the 4.X section of the course?\"',\n",
       " 'a52d4739': '[\"What do I do if I want to use a variable to score my classification metrics?\", \"Can I apply evaluation metrics to multiple variables or just one?\", \"Are you able to explain how to use metrics on a dataframe?\", \"Is there a specific way to combine multiple metrics?\", \"Can you show an example of a series being used for evaluation metrics?\"]',\n",
       " 'dc55359c': '[\\n\"Why do we occasionally use random_state in our code when we\\'re working with datasets?\", \\n\"What is the purpose of setting random_state and shuffle parameters in certain situations?\", \\n\"Why do some projects use random_state while others don\\'t?\", \\n\"What is the significance of ensuring reproducibility in dataset shuffling?\", \\n\"What is the relationship between random_state and the randomness used in dataset shuffling?\" ]',\n",
       " '2ab49e43': '[\"How do I calculate precision, recall, and F1 score simultaneously for classification?\", \"What is the best way to get all classification metrics at once?\", \"Can I use sklearn to get classification metrics?\", \"Are there any tips on how to optimize classification metrics?\", \"How do I get these metrics while examining the classification report?\"]',\n",
       " 'b431e7eb': 'Here are the 5 questions this student might ask based on the FAQ record:\\n\\n[\"How do I know if I\\'m doing something wrong when I get multiple thresholds with the same F1 score?\", \"Can I just pick the lowest threshold when getting multiple thresholds with the same F1 score?\", \"Is there a method for choosing a single threshold from multiple thresholds with the same F1 score?\", \"How can I verify the results of my own classification model using scikit-learn?\", \"How do I use classification_report from scikit-learn?\"]',\n",
       " 'c5fdeba9': '[\\n\"How can I resolve the ValueError when using a classification solver with a dataset that only contains one class?\",\\n\"Can I duplicate the data frame to fix the issue with single-class data?\",\\n\"Why do I get a ValueError when my dataset only contains one class?\",\\n\"Is it possible to delete one cell to fix the problem with the classifier?\",\\n\"Can I match the error message to identify the cause of the issue with single-class data?\"\\n]',\n",
       " 'b8c9eaf1': '[\"How can I get a beautiful classification report that includes a detailed comparison of my classifier\\'s performance?\", \"How can I use visualization tools to better understand my classifier\\'s strengths and weaknesses?\", \"What is the most effective method for displaying classification report metrics?\", \"Is there a library that combines scikit-learn with visualization tools to produce beautiful classification reports?\", \"Can I use Yellowbrick to generate a classification report that highlights the most important features of my model?\"]',\n",
       " 'c54058a1': '[\"What if my result isn\\'t exact in a homework problem\", \"How can I find the best score if I\\'m not getting the exact result\", \"What do I do when my answer isn\\'t precise in the assignment\", \"Can I still use a similar answer if my result is not exact\", \"How do I deal with less precise results in the classification problems\"]',\n",
       " 'b4b85c4b': 'Here is the list of questions in parsable JSON:\\n\\n[\\n\"Are there specific solutions from previous iterations that can help me evaluate feature importance of numerical variables in classification problems?\",\\n\"When would I typically use roc_auc_score to evaluate feature importance?\",\\n\"In what section of the course can I find information on evaluation metrics for classification to help me determine feature importance?\",\\n\"Can you confirm if roc_auc_score is the correct metric to use in all cases when evaluating feature importance of numerical variables?\",\\n\"How can I apply AUC to determine feature importance, especially when dealing with numerical variables in a classification context?\"\\n]',\n",
       " '7d40f6f6': '[\\n\"How to calculate the ROC AUC score when I have a numerical value in my dataframe that should be used as a score?\",\\n\"What does sklearn.metrics.roc_auc_score expect as parameters to compute the AUC?\",\\n\"How do I use a numerical value in my dataframe as the score to calculate the AUC?\",\\n\"What should I pass as the \\'y_true\\' parameter when calculating the ROC AUC score with sklearn.metrics.roc_auc_score?\",\\n\"How do I define my \\'y_true\\' and \\'y_score\\' parameters when using sklearn.metrics.roc_auc_score for AUC calculation?\"\\n]',\n",
       " 'f5dc446c': '[\\n\"What are the key factors that I should consider when evaluating the performance of my classification model?\",\\n\"What are the most common challenges in evaluating classification models, and how can I overcome them?\",\\n\"Can I use the same dataset for training and evaluating my classification model, or do I need to separate the datasets?\",\\n\"How can I use the `dt_val` dataset to compute the metrics required in Question 3 and onwards?\",\\n\"What is the purpose of using the `dt_val` dataset, and why is it more suitable than other datasets for evaluating classification models?\"\\n]',\n",
       " 'd30fc29d': 'Here is the list of questions:\\n\\n[\"What does KFold do and why doesn\\'t it make a difference if I do it inside or outside the loop?\", \"Can using a different random_state in KFold make a difference?\", \"Does KFold just make separate pairs of datasets (train+val)?\", \"How does KFold work and why doesn\\'t it matter where I generate the object?\", \"Is it better to loop through different values of Cs as explained in the video?\"]',\n",
       " '8eca9f73': '[\"What does the error ValueError: multi_class must be in (\\'ovo\\', \\'ovr\\') mean and how can I fix it when using roc_auc_score to evaluate feature importance of numerical variables in multi-class classification?\",\"I\\'m getting an error when trying to implement multi-class classification with roc_auc_score. Can you provide an example of the correct parameters I need to pass?\",\"How do I ensure I\\'m passing the parameters correctly for roc_auc_score when evaluating feature importance of numerical variables in multi-class classification?\",\"I\\'m struggling with passing the parameters for roc_auc_score in a multi-class classification model. Can you provide more information on the required input parameters and how to use them?\",\"What is the correct way to pass the parameters to roc_auc_score for evaluating feature importance of numerical variables in multi-class classification?\"]',\n",
       " '7b9eb7f7': 'Here are the questions in a parsable JSON format:\\n\\n[\"What is used to monitor wait times and progress of the code execution?\", \"What is Tqdm terminal progress bar used for?\", \"How do you import Tqdm for code execution monitoring?\", \"Can Tqdm be used for other purposes besides monitoring wait times?\", \"Who is Krishna Anand and how is their name related to Tqdm?\"]',\n",
       " 'c4aaeed9': '[\"What is the use of inverting or negating the variables less than the threshold in the context of classification?\", \"How does inverting or negating variables with ROC AUC scores less than the threshold improve feature importance and model performance?\", \"Can you give an example of a scenario where inverting or negating variables would be useful for improving feature importance and model performance?\", \"How does inverting or negating variables with negatively correlated features affect the direction of the correlation?\", \"Are there any specific machine learning algorithms where inverting or negating variables is particularly effective?\"]',\n",
       " '3af31e2a': '[\\n\"Why do binary classification predictions using predict(X) lead to incorrect evaluation values?\",\\n\"What are the binary classification predictions returned by predict(X)?\",\\n\"What is the main difference between predict(X) and predict_proba(X) when using them for classification tasks?\",\\n\"What does predict_proba(X)[:, 1] return in a classification task?\",\\n\"What exactly do predict(X) and predict_proba(X)[:, 1] predict/classify in the context of classification?\"\\n]',\n",
       " '746342ff': 'Here are the 5 questions based on the FAQ record in a parsable JSON format:\\n\\n[\"Why do the FPR and TPR become equal to 0.0 when the threshold value is 1.0?\", \"What happens when the threshold is equal to 1.0 and the FPR and TPR are both 0.0?\", \"Why does the condition for belonging to the positive class (churn class) not get satisfied when the threshold is 1.0?\", \"Why can\\'t the churn-condition be satisfied in the scenario with threshold equal to 1.0?\", \"What values does the sigmoid function take for a binary classification problem?\"]',\n",
       " 'bda2c9b3': '[\"How do I annotate a graph in Python?\", \"How can I find the optimal threshold and F1 score for my graph?\", \"What are the common annotation methods for a graph?\", \"How can I customize the appearance of the annotation in my graph?\", \"What libraries can I use for graph annotation and visualization?\"]',\n",
       " '41521c92': '[\"Are there any specific requirements or skills I need to understand the ROC curve? \", \"Can I proceed with the rest of the course if I don\\'t fully comprehend the ROC curve?\", \"Can you recommend any additional resources or materials to help me better understand the ROC curve? \", \"Is the ROC curve only used in Binary Classification models, or is it applicable to other classification problems? \", \"How long do you expect me to take to understand the ROC curve? \"]',\n",
       " '25481ce5': 'Here is the list of questions as requested:\\n\\n[\"Why do I consistently achieve different accuracy values on the homework assignments compared to the ones provided in the optional solutions?\", \"What if I use different data splitting methods, does it affect the accuracy?\", \"How can I ensure the data separation ratios remain consistent in my work?\", \"Can I use a different way of splitting my data for the homework assignments?\", \"Why does it matter which method I choose to split my data?\"]',\n",
       " '1427d567': 'Here is the list of questions:\\n\\n[\"How do I find the intercept between precision and recall curves by using numpy?\", \"Can I use numpy to get the index of the intersection point between precision and recall curves?\", \"How do I know the sign of the difference between precision and recall changes in the context of finding the intercept?\", \"What is the purpose of using `np.sign(np.array(df_scores[\"precision\"]) - np.array(df_scores[\"recall\"]))` in this context?\", \"In what condition does the sign of the difference between precision and recall changes?\"]',\n",
       " '76c91dfb': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\\n\"How do we compute the evaluation metrics for classification?\", \\n\"Can you show us an example of calculating precision and recall manually?\", \\n\"Is it possible to calculate precision, recall, and F1 Score using the Scikit Learn library?\", \\n\"How do we define true positive, true negative, false positive, and false negative?\", \\n\"What does the \\'average\\' parameter mean when using precision_score, recall_score, and f1_score?\"\\n]',\n",
       " 'e4dd91cf': '[\\n\"Why do we use cross-validation specifically when training and evaluating our models?\",\\n\"Why does choosing the right value for the hyperparameter \\'C\\' matter for the performance of our models?\",\\n\"What are some potential problems that can occur when \\'C\\' is set to a very large value?\",\\n\"What happens when \\'C\\' is set to a very small value?\",\\n\"How does splitting the dataset into folds in cross-validation help us evaluate our models?\"',\n",
       " 'cc53ae94': '[\"How can I easily compute model evaluation metrics in a course project?\",\"How do I calculate model evaluation metrics from scratch using numpy and pandas libraries?\", \"What off the shelf calculations are available in scikit learn library for model evaluation?\", \"Can I use scikit learn to compute multiple evaluation metrics at once?\", \"What is the purpose of using the roc_auc_score function in scikit learn for model evaluation?\"]',\n",
       " '403bbdd8': '[\"How can I compute Precision, Recall and F1 score in different ways in the scikit-learn library?\", \"What is another way to calculate Precision, Recall and F1 score?\", \"Can I use scikit-learn to compute multiple evaluation metrics at once?\", \"Are there alternative methods for calculating Precision, Recall and F1 score in this course?\", \"How do I use precision_recall_fscore_support in scikit-learn?\"]',\n",
       " '7c68ace0': '[\"When do I use ROC curves in imbalanced datasets?\", \"What is the main difference in representation between ROC and Precision-Recall curves?\", \"Why do ROC curves not accurately represent model performance in imbalanced datasets?\", \"How do class distributions affect metrics like accuracy, precision, and lift?\", \"Can ROC graphs accurately capture model performance in datasets with class imbalance?\"]',\n",
       " '147577f5': '[\"How do I evaluate feature importance for numerical variables using AUC metric from section 4?\", \"Can I use the roc_auc_score function from sklearn.metrics module to evaluate feature importance for categorical variables?\", \"What is the purpose of passing the target variable as the first argument for roc_auc_score function?\", \"How can I interpret the output of the roc_auc_score function in the context of evaluating feature importance?\", \"Can I use roc_auc_score function to compare the feature importance between different models?\"]',\n",
       " 'd3ffb802': '[\"What does the F-score depend on in classification problems?\", \"Does the F-score depend on the ratio of positive to negative test cases?\", \"Can the F-score be compared across different problems with the same class ratio?\", \"Does a high F-score mean the model is good when there\\'s class imbalance?\", \"Is using a standard class ratio a good approach to solve the problem of class imbalance for F-score comparison?\"]',\n",
       " 'cc04d27a': '[\"How do I quickly plot a Precision-Recall Curve in this evaluation metric section?\", \"Can I use scikit-learn to plot a Precision-Recall Curve from scratch?\", \"What are some tips for plotting a Precision-Recall Curve in Python?\", \"Why do I need to import precision_recall_curve from scikit-learn?\", \"How can I add a label to the Precision-Recall Curve for better visualization?\"]',\n",
       " '927b5e09': 'Here are 5 questions based on the FAQ record:\\n\\n[\\n\"What is the difference between a regular k-fold and a stratified k-fold?\",\\n\"How can you ensure that a data set remains balanced when splitting it?\",\\n\"Why is it important to maintain class balance in multiclass classification?\",\\n\"Can you give an example of when you would use stratified k-fold instead of regular k-fold?\",\\n\"How do you implement stratified k-fold in scikit-learn?\"',\n",
       " 'd22efea7': 'Here is the output:\\n\\n[\\n\"How do I get started with the deployment of machine learning models?\",\\n\"Where can I find the Week 5 homework?\",\\n\"What is the evaluation matrix for the homework?\",\\n\"Can I find the solution for previous homeworks?\",\\n\"How do I access the course materials on GitHub?\"\\n]',\n",
       " 'd1409f67': 'Here are the 5 questions based on the FAQ record:\\n\\n[\"What can I do with just about any default environment/local setup for weeks 1-4?\",\"Can I follow week 5 without a cloud provider?\",\"How can I prepare my homework environment with a cloud provider?\",\"Are there free options for AWS EC2 instances?\",\"What alternatives are available for setting up my environment besides AWS and the guide in the MLOPS course?\"]',\n",
       " 'e07759e9': '[\"How do I create a Kaggle account for seamless CSV download with Jupyter NB?\", \"What is the process to export my Kaggle API token to Jupyter NB?\", \"How do I set the environment variable KAGGLE_CONFIG_DIR in my Jupyter NB?\", \"What command should I use to download the CSV data via Kaggle API after setting up everything?\", \"Can I download and extract the CSV file directly in my Jupyter NB?\"]',\n",
       " '620fb76e': 'Here are 5 questions based on the FAQ record:\\n\\n[\"How do I go back to the previous directory in Ubuntu?\", \"What are the basic commands I need to know for navigating folders in Ubuntu?\", \"Can I edit a text file in Ubuntu using a specific command?\", \"What is the command to view the current working directory in Ubuntu?\", \"How do I specify a path in Ubuntu using the cd command?\"]',\n",
       " '957280d8': 'Here are the 5 questions that the student might ask based on the FAQ record:\\n\\n[\\n\"How can I check what Python version I currently have installed on my laptop?\"\\n\"What are the steps to download and install a higher version of Python 3.10 on my Windows machine?\"\\n\"Should I check a box during Python installation to add Python to PATH?\"\\n\"What is the command to upgrade Python using pip for a Python 3 installation?\"\\n\"What are the official sources I can use to download and install Python?\"',\n",
       " '185096ad': 'Here are the 5 questions based on the FAQ record:\\n\\n[\\n\"How to install Linux distributions on Windows 10 and 11?\",\\n\"What are the system requirements for installing WSL?\",\\n\"How to switch between Windows and Linux systems in WSL?\",\\n\"How to disable bell sounds in Ubuntu terminal\",\\n\"How to solve the error message when trying to install pipenv?\"\\n]',\n",
       " 'ec88d101': 'Here are 5 questions this student might ask based on the FAQ record:\\n\\n[\"What happens when building Docker images on Mac with M1 silicon?\", \"Do build errors occur when using Mac with M1 chipset?\", \"Why do I get the error \\'/lib64/ld-linux-x86-64.so.2\\': No such file or directory?\", \"What changes should I make to the Dockerfile to resolve the issue?\", \"How long does it take to build Docker images using the suggested fix?\"]',\n",
       " '7156679d': '[\"What is the recommended method to find the version of any installed Python libraries in Jupyter Notebook during the deployment of machine learning models?\", \"How can I use Jupyter Notebook to verify the installed Python libraries and their versions before deploying machine learning models?\", \"Can I use any specific Python libraries to check the version of installed libraries in Jupyter Notebook when deploying machine learning models?\", \"What if I need to check the version of a specific Python library while deploying machine learning models in Jupyter Notebook?\", \"Is there any shortcut in Jupyter Notebook to find the version of all installed Python libraries during the deployment of machine learning models?\"]',\n",
       " '4b2a3181': 'Here are the 5 questions this student might ask:\\n\\n[\"Cannot connect to the docker daemon when running hello-world. Is Docker installed?\", \"Why am I getting the error when trying to run hello-world and not connected to the docker daemon?\", \"How do I check if the Docker daemon is running?\", \"Is there a solution to this issue if I\\'m using WSL?\", \"What commands can I use to start the docker daemon on Linux?\"]',\n",
       " '73bd7fa1': 'Here are the 5 questions:\\n\\n[\"How can I fix an error where \\'/bin/sh -c pipenv install --deploy --system &&  rm -rf /root/.cache\\' returns a non-zero code 1?\",\"Why is a Docker image not being created after executing \\'docker build -t churn-prediction .\\'?\",\"How do I correctly specify the Python version in my Dockerfile?\",\"Can you explain how to find the correct Python version for my Dockerfile?\",\"What could be the cause of an error when building a Docker image using \\'docker build -t churn-prediction .\\'?\"]',\n",
       " 'a4d3b1e5': 'Here are the 5 questions this student might ask based on the FAQ record:\\n\\n[\"Why do errors occur when trying to install a specific version of scikit-learn using pipenv?\", \"Can you help me troubleshoot installing scikit-learn for homework?\", \"How do I ensure a successful installation of scikit-learn in my virtual environment?\", \"Why is the facilitator\\'s method not working for me when installing scikit-learn?\", \"What are some best practices for installing specific versions of libraries in a virtual environment?\"]',\n",
       " '1d462fe0': 'Here is the output:\\n\\n [\\n\"Why do we need to run docker containers with --rm flag\",\\n\"What is the reason we don\\'t want to keep the docker image in our system\",\\n\"Why do we remove the docker container in our system\",\\n\"Why do we accumulate containers in a stopped state during development and testing\",\\n\"What is the difference between a \\'docker image\\' and a \\'docker container\\'?\" \\n]',\n",
       " '366d7563': '[\"When I create the Dockerfile, should its name include the extension \\'.\\', such as \\'Dockerfile.dockerfile\\', or not?\", \"If I have accidentally named my Dockerfile with an extension, can I still use it and can I expect any errors?\", \"Can I name my Dockerfile anything other than \\'Dockerfile\\' and still expect it to work?\", \"What could be the cause of an error when trying to build a docker image?\", \"Can I reuse a Dockerfile that has already been created, or do I need to create a new one each time?\"]',\n",
       " 'cef156d1': '[\"How do I install Docker on my MacBook to deploy machine learning models?\", \"Can I still deploy machine learning models without having a virtual machine?\", \"What are some best practices for deploying machine learning models in production?\", \"How do I ensure that my machine learning models are scalable and robust?\", \"Can I integrate my machine learning models with other data tools using Docker?\"]',\n",
       " 'b632d2ea': '[\"What is the problem when using the default tag with the docker pull command?\", \"How do I avoid getting an error when pulling the image?\", \"What is the correct tag to use when pulling the image?\", \"Why do I get a manifest unknown error when trying to pull the image?\", \"What should I use instead of the docker pull command to avoid the problem?\"]',\n",
       " '514e27bb': '[\"How do I get docker to only show me the size of all local Docker images?\", \"Is there a way to get docker to only show me the size of a specific Docker image?\", \"Can I use the docker images command to retrieve the size of a Docker image?\", \"How do I use the --format option with docker image ls?\", \"Can I use docker images instead of docker image ls to get the size of a Docker image?\"]',\n",
       " '5c67e086': '[\"Where does pipenv create its virtual environments?\", \"How does pipenv determine the name of its virtual environments?\", \"Can I manually specify the name of the virtual environment?\", \"How do I activate a pipenv virtual environment?\", \"Where are the installed libraries located in a pipenv virtual environment?\"]',\n",
       " '63a81b57': '[\"How do I troubleshoot a machine learning model that\\'s not performing well?\", \"Why do I need to use a specific command to start a docker container?\", \"Can I debug a running docker container if it was started without the interactive flag?\", \"What does the -it flag do in the docker run command?\", \"Is there a way to access a running docker container\\'s command line without restarting it?\"]',\n",
       " '047f57fb': '[\\n\"What happens when running docker in interactive mode and the input device is not a TTY?\",\\n\"Can I use mintty to resolve the issue when the input device is not a TTY?\",\\n\"Why is a TTY important in this context?\",\\n\"What is meant by \\'winpty\\' and how does it relate to Windows?\",\\n\"What is the difference between a terminal and a shell?\"\\n]',\n",
       " '11f7371c': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\"How can I resolve the \\'Cache key not found\\' error in deploying my machine learning model?\",\\n\"What if I don\\'t have the original model file?\",\\n\"Why is COPY [\"model2.bin\", \"dv.bin\", \"./\"] not working in MINGW64 (git bash) on Windows?\",\\n\"What is the purpose of using COPY [\"*\", \"./\"] in docker?\",\\n\"Why did copying the original model file and its dependencies incorrectly result in a \\'not found\\' error?\"]',\n",
       " '45f39b76': '[\"What are the steps to create a virtual environment for the machine learning project?\", \"How do I write the project\\'s dependencies to pipfile and piplock files?\", \"Can you elaborate on the command to use when facing issues with pipfile and piplock?\", \"Are there any specific tools required to write the dependencies in a text file?\", \"Can you provide more clarification on the best way to manage project dependencies for machine learning models?\"]',\n",
       " '94e17563': '[\\n  \"What is the correct syntax for using f-strings in Python, especially when it comes to combining string literals with variables?\",\\n  \"When should I use parentheses when dumping an object to a file using the pickle module?\",\\n  \"Why do I keep getting an error after importing the pickle module, and how can I troubleshoot the issue?\",\\n  \"How can I properly key in an f-string when working with binary files, and what are some common mistakes to watch out for?\",\\n  \"What is the correct way to use pickle to serialize a machine learning model, and what are some tips for avoiding errors?\"',\n",
       " '9dd8efd2': '[\\n\"How do I fix the error \\'pipenv\\' is not recognized as an internal or external command, operable program or batch file when trying to run pipenv commands?\",\\n\"What if I\\'m using Anaconda for Window\\'s deployment?\",\\n\"How do I verify if \\'pipenv\\' is installed correctly?\",\\n\"What is causing this error when running pipenv --version or pipenv shell?\",\\n\"How do I temporarily resolve this issue for my current project?\"',\n",
       " '9531dc92': 'Here are the 5 questions in a JSON format:\\n\\n[\"What happens when we use pipenv to install Python libraries following the instruction from video week-5.6?\", \"Why do we get an AttributeError when trying to import \\'collections.MutableMapping\\'? \", \"Why must we ensure we are using Python 3.9 when working on Deploying Machine Learning Models?\", \"What importance does the first lesson of the zoomcamp hold in terms of Python versions?\", \"Why can\\'t we use Python 3.10 for Deploying Machine Learning Models?\"]',\n",
       " '14e0e697': 'Here is the list of questions:\\n\\n[\"How do I avoid errors when installing packages after entering pipenv shell?\", \"Why does entering pipenv shell cause errors when trying to install packages?\", \"How do I fix the PATH after using pipenv shell?\", \"Can you clarify what \\'being in the shell\\' means in the context of Windows?\", \"Why is the removed-envname folder important in resolving the error?\"]',\n",
       " '6189375f': 'Here is the output in parsable JSON:\\n\\n[\"What do I need to set to \\'0.0.0.0\\' when running a Flask app and Dockerfile?\", \"How can I fix a problem where a remote connection is being abruptly closed without a response?\", \"How do I resolve a connection error when trying to run a URL?\", \"What step can I take to resolve a stuck connection when running a machine learning model?\", \"Can I run a URL using localhost when deploying a machine learning model?\"]',\n",
       " '3419ee27': '[\"What can cause a \\'docker build\\' command to fail with the error [x/y] COPY...?\", \"When deploying a machine learning model, why do I need to ensure that filenames are quoted correctly?\", \"How can I troubleshoot issues with a \\'docker build\\' command in the context of deploying machine learning models?\", \"What specific problems can arise when filenames are not quoted correctly during a \\'docker build\\' command?\", \"What steps can I take to resolve the issue of single quotes being used around filenames during a \\'docker build\\' command?\"]',\n",
       " '8b8c1603': '[\\n\"What does it mean by \\'fix error during installation of Pipfile inside Docker container\\'?\", \\n\"Why wasn\\'t the solution to update the Pipfile.lock successful?\", \\n\"How do I switch to pipenv installation with --system, --deploy, and --ignore-pipfile options?\", \\n\"What is the use of \\'pipenv install\\' command with these options?\", \\n\"Can the pipenv installation with these options be used to resolve similar issues in the future?\"\\n]',\n",
       " 'e54d5411': '[\"What is the cause of an error after running the Docker run command?\", \"How do I remove an orphaned container?\", \"What can I do if another instance of gunicorn is running?\", \"Why do I need to rebuild the Docker image?\", \"What are the correct steps to stop and remove a container?\"]',\n",
       " 'f7b38587': '[\"What should I do if I encounter a \\'Bind for 0.0.0.0:9696 failed: port is already allocated\\' error when rebuilding a docker image?\", \"Why am I still getting an error despite the port not being allocated?\", \"Can I resolve the issue by running a specific command?\", \"What is the correct solution to resolve this issue?\", \"I encountered this error while attempting to deploy a machine learning model. How can I bypass it?\"]',\n",
       " 'be86b333': 'Here are 5 questions that a student might ask based on the FAQ record:\\n\\n[\"What does the error message \\'Bind for 127.0.0.1:5000 showing error\\' mean?\", \"Why am I getting a \\'Connection aborted\\' error on the client side?\", \"Why is gunicorn showing errors when used on the server side?\", \"What is the solution to the \\'Remote end closed connection without response\\' error?\", \"What is the significance of using \\'0.0.0.0:8000\\' or \\'0.0.0.0:9696\\' instead of \\'127.0.0.1:5000\\'?\"]',\n",
       " '4ea80460': 'Here are the 5 questions:\\n\\n[\"How do I install md5sum on my Mac if I don\\'t have it?\", \"Is there a specific command I need to use to install md5sum?\", \"Can I install it using a software manager like Homebrew?\", \"What is the purpose of using md5sum on my file?\", \"How do I verify if my file hash is the same as the one provided?\"]',\n",
       " '8006b496': '[\"How do I start a web-server that is still running in a separate terminal window while I\\'m working on other tasks?\", \"Can I access a web-server from a Python script and make a request to it?\", \"How do I run a Python script that makes a request to a web-server that is already running?\", \"Can I run multiple scripts simultaneously, including one that is a web-server and another that makes requests to it?\", \"Is it possible to run a Python script while another process, like a web-server, is still running in the background?\"]',\n",
       " '704f95d8': 'Here is the list of 5 questions based on the FAQ record:\\n\\n[\"When I run pipenv shell and then pipenv run gunicorn --bind 0.0.0.0:9696 predict:app, I get a warning about unpickling an estimator DictVectorizer, what does this warning mean?\", \"Why do I get a UserWarning when trying to unpickle an estimator DictVectorizer?\", \"Is there a way to resolve the version conflict between Scikit-Learn versions when deploying a machine learning model?\", \"How can I use the same version of Scikit-Learn for training the model and for creating a virtual environment?\", \"Can I use a different version of Scikit-Learn in my project than the one used for training the model?\"]',\n",
       " 'a5b3296b': '[\"What happens when I get an error that includes \\'pipenv vendor plette models base ValidationError: {\\'python version\\': \\'3.9\\', \\'python full version\\': \\'3.9.13\\'\"}\", \"How do I fix the error \\'python version\\' must not be present with \\'python full version\\'\", \"Should I remove the \\'python full version\\' line or the \\'python version\\' line in my Pipfile?\", \"When I remove the line, do I need to restart something?\", \"What should I do after making the changes to the Pipfile and creating the Pipfile.lock?\"]',\n",
       " 'a23b276a': 'Here is the list of questions in JSON format:\\n\\n[\\n\"What happens if I get an error like \\'Your Pipfile.lock (221d14) is out of date\\' during a Docker build?\",\\n\"Is there a way to delete the pipfile.lock and rebuild the lock via pipenv lock before retrying the docker build command?\",\\n\"Can I remove the pipenv environment, Pipfile, and Pipfile.lock and create a new one before building docker again?\",\\n\"What is the correct command to delete the pipfile.lock and rebuild the lock?\",\\n\"What commands should I run to remove the pipenv environment and removing pipfiles?\"\\n]',\n",
       " '3537eeee': 'Here are 5 questions that the student might ask based on the FAQ record:\\n\\n[\"What if Waitress stops working after a few runs, making it impossible to run MLflow server on Windows?\", \"How can I fix issues with Waitress and MLflow server running on Conda environment?\", \"Do I need to reinstall Waitress every time I run MLflow server?\", \"Can I still use my Docker image after reinstalling MLflow?\", \"Why does reinstalling MLflow solve issues with MLflow server not running on Windows?\"]',\n",
       " '1d6d5b51': '[\"When I completed creating the environment locally, why couldn\\'t I find it on AWS?\", \"How do I ensure I\\'m in eu-west-1 (ireland) when checking the EB environments?\", \"Why might I not be in the right region in my AWS console?\", \"What region should I be in on AWS to find my environment?\", \"Can I find my environment on AWS if I\\'m not in eu-west-1 (ireland) region?\"]',\n",
       " '3a98b6b7': 'Here is the output in parsable JSON format:\\n\\n[\"How do I install waitress on Windows via GitBash without getting a \\'waitress-serve\\' command not found error?\", \"How do I download the executable file \\'waitress-serve.exe\\'? \", \"Why did I get a warning message when installing waitress using Jupyter notebook?\", \"How do I add the path to \\'waitress-serve.exe\\' to GitBash\\'s PATH?\", \"Do I need to restart GitBash after adding the path to PATH?\"]',\n",
       " 'd42eb923': '[\"What is the environment variable LANG used for?\", \"Can I skip the warning about LANG not being set?\", \"Is there a specific version of Scikit-Learn that I need to install?\", \"How do I install Scikit-Learn version 1.3.1?\", \"Is the warning about LANG not being set fatal?\"]',\n",
       " '42aebe10': 'Here is the list of 5 questions:\\n\\n[\"What models are provided in the svizor/zoomcamp-model:3.10.12-slim image for deployment?\", \\n\"How do I know which DictVectorizer to use with the provided model?\", \\n\"Can I use the DictVectorizer from a different image for question 6?\", \\n\"What if the \\'model2.bin\\' file is not present in the provided image?\", \\n\"What should I use the provided \\'dv.bin\\' file for in question 6?\"]',\n",
       " 'e4f62713': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"What terminal is used in Week 5 videos?\",\\n\"What terminal do we use in Week 5?\",\\n\"Is the terminal mentioned in the Week 5 videos?\",\\n\"What terminal did the instructor use in Week 5?\",\\n\"Can you provide more information about the terminal used in Week 5 videos?\"]',\n",
       " 'c13d811f': '[\\n\"Can I deploy my machine learning models, and if so, what are the common issues I might face?\",\\n\"What should I do if I get a Malformed application error when running waitress-serve?\",\\n\"I am getting an exception (ValueError) when importing my module. What does this mean?\",\\n\"Why do I need to rename my python file when running waitress-serve?\",\\n\"Are there any best practices for deploying machine learning models to production?\"',\n",
       " 'dfb41f7e': 'Here are the 5 questions based on the FAQ record:\\n\\n[\"How can I test HTTP POST requests from the command line?\", \"What are the examples of using curl to send a JSON request?\", \"Is it possible to use curl with a JSON file?\", \"Can I use curl with a Linux or MacOS system?\", \"How do I add an additional JSON data to the curl request?\"]',\n",
       " 'd04e77f8': '[\\n\"Why can I get a NotSupportedError when running \\'eb local\\'?\",\\n\"What are the two possible ways to fix the NotSupportedError with \\'eb local\\'?\",\\n\"Can I use \\'eb local\\' with any Docker platform?\",\\n\"Is there a disadvantage to editing the \\'.elasticbeanstalk/config.yml\\' file directly?\",\\n\"What Docker platform should I choose by default when initializing elasticsearch?\"',\n",
       " '451c067f': '[\"What happens when I get an error saying \\'No connection adapters were found for \\'localhost:9696/predict\\' when using the requests library?\", \"Why do I need to include the protocol scheme when making requests to a local server?\", \"What if my URL starts with HTTP://, will that also cause an issue?\", \"Is it necessary to make sure the protocol scheme is in lowercase when using the requests library?\", \"Can I avoid the \\'No connection adapters were found\\' error by simply adding \\'http://\\' before the URL?\"]',\n",
       " '9fbfcd61': 'Here is the list of questions in parsable JSON format:\\n\\n[\"What happens if I get the same result while running the docker image?\", \"Why do I get the same result even though I\\'ve changed the model?\", \"How do I ensure that I\\'m using the correct model while running my prediction test?\", \"What should I do if I get the same result again after changing the model?\", \"How can I avoid getting the same result in the future?\"]',\n",
       " '1ed8cfde': '[\\n\"What if I\\'m having trouble running a Docker image I built and it says the container process is unable to start?\",\\n\"I built a Docker image but when I try to start it, I get an error that the container process can\\'t start\",\\n\"I\\'m attempting to deploy my Docker image but it keeps saying the container process can\\'t initialise\",\\n\"I\\'m having issues running my Docker image and the error message says the container process can\\'t be started\",\\n\"I built a Docker image but when I run it, I get an error saying the container process won\\'t start\"',\n",
       " '3f97f50f': 'Here is the list of questions based on the FAQ record:\\n\\n[\"How do I deploy machine learning models to a Docker container?\", \"Can I use the docker cp command to transfer files from my local machine to a directory within the Docker container?\", \"How do I copy a directory from my local machine into a running Docker container?\", \"What is the basic syntax of the docker cp command for copying files?\", \"Can I use docker cp to transfer files from a directory within the Docker container to my local machine?\"]',\n",
       " 'a24a874a': '[\"What are the steps to copy files from a different folder into a Docker container\\'s working directory?\", \"Can I copy files from my local machine into a Docker container using a command?\", \"How do I provide the folder containing the files to be copied over in the Dockerfile?\", \"What is the basic syntax to copy files using the Dockerfile?\", \"Can I use docker cp command to copy files from local machine into Docker container?\"]',\n",
       " 'bf563b1f': 'Here are 5 questions based on the FAQ record:\\n\\n[\"Can I create the environment on AWS Elastic Beanstalk with the command proposed during the video without any issues?\", \"What is the correct command to initialize the environment on AWS Elastic Beanstalk?\", \"Why do I face an error when running the command eb local run --port 9696?\", \"Can \\'eb local\\' be used with all Docker platforms?\", \"How do I resolve the error NotSupportedError - You can use \\'eb local\\' only with preconfigured, generic and multicontainer Docker platforms?\"]',\n",
       " '21e9facf': '[\"What happens when I try to create a AWS ElasticBean environment without a Dockerfile?\", \"Why do I get an error saying that both Dockerfile and Dockerrun.aws.json are missing when deploying an instance?\", \"Why do I need to commit my Dockerfile after modifying it?\", \"How do I resolve the \\'Dockerfile\\' missing error when creating a AWS ElasticBean environment?\", \"What do I need to include for a successful deployment of my instance?\"]',\n",
       " 'aef786aa': 'Here is the list of questions the student might ask based on the provided FAQ record:\\n\\n[\"How do I get started with Week 6 of the course?\", \"Where can I find all the homeworks for this course?\", \"Can I see the solution to HW 4?\", \"What is the evaluation matrix for the course?\", \"Where can I find the theory GitHub link for this course?\"]',\n",
       " '68858294': 'Here is the list of 5 questions:\\n\\n[\"How do I get the training and validation metrics from XGBoost?\", \"What happens during XGBoost training?\", \"How do I plot the training and validation metrics?\", \"Can I store the training and validation metrics in a dataframe?\", \"How do I update the dictionary for each tree in XGBoost?\"]',\n",
       " '85ac722e': '[\\n\"How to apply random forest for regression problems in scikit-learn?\",\\n\"How do I create a RandomForestRegressor object in scikit-learn for classification?\",\\n\"How can I use random forest for regression tasks with random forest in scikit-learn?\",\\n\"What is the similarity between RandomForestRegressor and RandomForestClassifier in scikit-learn?\",\\n\"Can you provide more information on RandomForestRegressor in scikit-learn?\"',\n",
       " 'b61d2e92': '[\\n\"What causes a ValueError: feature_names must be string, and may not contain [, ] or < when creating a DMatrix?\",\\n\"Can we replace special characters in feature names before creating a DMatrix?\",\\n\"Is it necessary to replace all special characters in feature names or just certain ones?\",\\n\"Can we use only the replace() function to fix the error without using a loop?\",\\n\"What types of special characters can we find in feature names that would cause this error?\"',\n",
       " '8d7392cb': '```\\n[\"When training an xgboost model, how do I handle the TypeError: Expecting a sequence of strings for feature names, got: <class \\'numpy.ndarray\\'> error if the feature names are a numpy array?\", \"How can I convert the output of dv.get_feature_names_out() into a list?\", \"Can you provide a solution for \\'TypeError: Expecting a sequence of strings for feature names, got: <class \\'numpy.ndarray\\'> when training xgboost model in Python?\\'\", \"What do I need to do to fix the TypeError: Expecting a sequence of strings for feature names, got: <class \\'numpy.ndarray\\'> when using decision tree and ensemble learning?\", \"How to resolve \\'TypeError: Expecting a sequence of strings for feature names, got: <class \\'numpy.ndarray\\'> when training a model using xgboost in Python?\"]',\n",
       " 'c920eef3': 'Here are the 5 questions this student might ask:\\n\\n[\\n\"Q: What is the reason for TypeError when setting xgb.DMatrix with feature_names?\",\\n\"Q: Why is my feature_names output a numpy array instead of a list?\",\\n\"Q: How can I convert my DictVectorizer output to a string?\",\\n\"Q: Why am I getting a ValueError when using feature_names with xgb.DMatrix?\",\\n\"Q: How can I fix the feature_names string to avoid symbols like [,],< while creating xgb.DMatrix?\"',\n",
       " '5017c9a4': '[\\n\"How do I install Xgboost in my Jupyter notebook?\",\\n\"What is the minimum version of pip required for Xgboost installation?\",\\n\"Can I upgrade my existing pip version for Xgboost installation?\",\\n\"What are the steps to update pip to the latest version?\",\\n\"Are there any additional resources available for understanding and installing Xgboost?\"',\n",
       " '6ffe101d': '[\"What is the learning rate of the XGBoost model\", \"How does XGBoost\\'s gradient descent algorithm help the model learn the data\", \"What role do the tunable hyperparameters play in the XGBoost model\", \"How does the XGBoost model use the weights to learn the features\", \"What is the use of gradient descent in XGBoost\\'s training process\"]',\n",
       " 'a55b29ff': 'Here is the output:\\n\\n[\"What is bagging used for in ensemble algorithms?\", \"How does boosting reduce bias and variance compared to bagging?\", \"Can you explain how bagging reduces overfitting in ensemble algorithms?\", \"Why are models in boosting given different weights?\", \"Is boosting always better than bagging for ensemble algorithms?\"]',\n",
       " 'eac70ce3': '[\"How can we capture the stdout for each iteration of a loop separately? I want to avoid running the same cell multiple times and editing code manually. Is there a way to do this?\", \"What is the best way to capture the output from xgboost training for multiple eta values to a dictionary? I want to avoid having to run the same cell multiple times and copy code for a second eta value.\", \"Can I use the magic cell command \\'%%capture output\\' to capture the output for each iteration of a loop? If so, how do I do this?\", \"What is the difference between capturing the complete output for all iterations and capturing the output for each iteration separately?\", \"How do I use the \\'capture_output\\' function from IPython.utils.capture to capture the stdout for each iteration of a loop?\"',\n",
       " '5f91f8ca': '[\"What does ValueError: continuous format is not supported mean?\", \"Why am I getting an error when I\\'m trying to calculate the Area under the ROC Curve?\", \"What is the correct way to pass arguments to the roc_auc_score function?\", \"Why does roc_auc_score need y_train and y_pred as input?\", \"How do I ensure I\\'m passing the right format to roc_auc_score for it to work correctly?\"]',\n",
       " 'a3be507a': '[\"What happens when the RMSE goes up at a certain number of n_estimators but then goes back down lower than it was before?\", \"How do I determine the optimal number of n_estimators when the RMSE initially increases but then decreases?\", \"Is the optimal number of n_estimators the one after which the RMSE first goes up or the one at which it\\'s lowest?\", \"How do I know when the RMSE has stopped improving?\", \"Is the optimal number of n_estimators the one at which RMSE is at its minimum?\"]',\n",
       " '9a8faa50': '[\\n\"How do I visualize a decision tree in this course?\", \\n\"What is the role of the \\'feature_names\\' parameter in exporting a decision tree?\", \\n\"Are there any alternatives to using \\'export_graphviz\\' to visualize decision trees?\", \\n\"Can I use \\'plot_tree\\' to visualize a decision tree if I\\'m not using scikit-learn?\", \\n\"In what format can I export the decision tree visualization from the \\'export_graphviz\\' function?\" ]',\n",
       " 'a6e384fe': '[\"What causes the ValueError: Unknown label type: \\'continuous\\'\\'\", \"What type of DecisionTree should I use for classification versus regression?\", \"Is the DecisionTreeClassifier incompatible with continuous labels?\", \"Can the DecisionTreeRegressor be used for classification?\", \"Why did Alejandro Aponte mention this problem?\"',\n",
       " 'ddc14ada': 'Here is the output:\\n\\n[\"Why when I run DecisionTreeClassifier in Jupyter, each time I re-run it or do (restart kernel + run) I get different values of auc?\", \"When I run DecisionTreeClassifier(), how can I get the same values of auc every time I re-run it?\", \"Are different values of auc normal when re-running DecisionTreeClassifier() in Jupyter?\", \"Why do I get varying values of auc every time I re-run DecisionTreeClassifier() in Jupyter?\", \"How can I solve the problem of getting different values of auc when re-running DecisionTreeClassifier() in Jupyter?\"]',\n",
       " '593f7569': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\\n\"What is the purpose of using ensemble learning in machine learning?\",\\n\"How do I specify different evaluation metrics in the decision tree algorithm?\",\\n\"Can I use the same dataset for training and testing my decision tree model?\",\\n\"Are there any specific libraries required for building decision trees in Python?\",\\n\"In a decision tree, what is the purpose of the root node and leaf nodes?\"',\n",
       " '6cb56405': 'Here are the 5 questions based on the FAQ record:\\n\\n[\"Why can\\'t I import the \\'ping\\' function?\", \"Why do I keep getting an error when running a decision tree example?\", \"How can I make sure the \\'ping\\' function is imported correctly?\", \"Do I need to install a specific package for decision trees to work?\", \"Can I use the \\'ping\\' function with other data structures other than decision trees?\"]',\n",
       " 'a22a93f1': '[\"How do I get the feature names when using a DictVectorizer?\", \"Can I analyze feature importance when using DictVectorizer for one hot encoding?\", \"What type of array does get_feature_names_out() return?\", \"Do I need to fit the predictor and response arrays before using get_feature_names_out()?\",\"How do I convert the returned ndarray array to a standard list of strings?\"]',\n",
       " 'b6259dea': '[\\n\"What causes the ValueError: feature_names must be string, and may not contain [, ] or <?\",\\n\"Can I replace problematic characters in the feature names?\",\\n\"How do I encode feature names with special characters?\",\\n\"What are the supported characters in feature names?\",\\n\"How can I ensure consistent feature names with no special characters?\"\\n]',\n",
       " 'bcfdc6f4': '[\\n\"How to extract the feature importances from the model?\",\\n\"Can I use a different type of chart to visualize feature importance?\",\\n\"What is the point of sorting the dataframe by feature importances in descending order?\",\\n\"Are there any specific parameters or options I can use when creating the horizontal bar chart?\",\\n\"Can I customize the appearance of the bar chart further, for example, changing the color palette?\"',\n",
       " 'a7e7cdd2': 'Here are the 5 questions in JSON format:\\n\\n[\\n\"How do I calculate RMSE when the formula requires taking the square root of the mean squared error?\", \\n\"What is the best way to calculate the mean squared error when I don\\'t want to take the square root of it?\", \\n\"Can I extract the root mean squared error directly from the metrics.root_meas_square() function?\", \\n\"In what situation would I want to calculate the mean squared error without taking the square root of it?\", \\n\"What function should I use to calculate the RMSE in Python, and how does it handle the square root calculation?\"\\n]',\n",
       " '55477da8': '[\\n\"How can I create a visual implementation of features importance in scikit-learn library?\",\\n\"What is the purpose of adding std.errors to features importance in a Decision Tree model?\",\\n\"Is there any way to increase the model\\'s explainability by analyzing features importance?\",\\n\"Can I trace the stability of features importance over different model parameters in scikit-learn?\",\\n\"How can I get the features importance with their corresponding std.errors in a Decision Tree model?\"',\n",
       " '6a245a05': '[\"What are common XGBoost errors and how do I troubleshoot them?\",\"Why am I getting an error when trying to use XGBoost with scikit-learn?\",\"How do I resolve an XGBoost error saying \\'sklearn needs to be installed\\' when trying to use it?\",\"Can XGBoost be used with scikit-learn and if so, what are the benefits?\",\"Do I need to install scikit-learn specifically to use XGBoost with my machine learning model?\"]',\n",
       " '4405bfca': '[\"How do we calculate information gain in a decision tree?\", \"What is the formula for mutual information in the context of decision trees?\", \"What does it mean when X is completely uninformative about Y?\", \"What does it mean when X is completely informative about Y?\", \"Can you explain the formula for information gain in more detail?\"]',\n",
       " '3e0acc25': '[\"What does data leakage occur in the context of decision trees and ensemble learning?\", \"Why does filling in missing values using an entire dataset cause data leakage?\", \"How does data leakage affect the performance of decision trees and ensemble learning?\", \"Is data leakage a specific problem in only a few algorithm choices or a wide range of ensemble learning methods?\", \"What are some common pitfalls or mistakes that can lead to data leakage in decision trees and ensemble learning?\"]',\n",
       " 'abaecdf8': '[\\n  \"If I\\'m getting an error with Serialized Model Xgboost, what steps can I take to resolve the issue?\",\\n  \"How can I save a model in Xgboost and later load it again?\",\\n  \"Can you provide an example of how to save a model in Xgboost?\",\\n  \"What is the correct method to load a previously saved Xgboost model?\",\\n  \"Why did my Xgboost model not save successfully and what can I do to fix the problem?\"\\n]',\n",
       " 'ff40f83b': '{\"[\" \"How do I decipher the Week 8 assignments?\", \"What are the necessary prerequisites for Week 8 neural networks?\", \"Can I use a specific framework to implement the Week 8 tasks?\", \"Are there any specific problems to be solved for Week 8?\", \"How do I verify the accuracy of the predictions in Week 8 neural networks?\"]\"}',\n",
       " '95a16746': '[\"How do I import a notebook into Kaggle?\", \"What are the steps to create a new notebook in Kaggle for deep learning?\", \"Can I specify the type of GPU to use when training a model on Kaggle?\", \"What is the role of accelerators in Kaggle for deep learning?\", \"Can I use Kaggle for training my entire deep learning project?\"]',\n",
       " '46acdd18': 'Here are the 5 questions based on the FAQ record:\\n\\n[\\n\"How can I import my existing notebook into Google Colab for deep learning?\",\\n\"What do I need to do to switch to the T4 GPU runtime type in Google Colab?\",\\n\"Is Google Colab a suitable environment for creating new notebooks for deep learning?\",\\n\"Can I access the Google Colab dropdown menu at the top right-hand side to change the runtime type?\",\\n\"Are there any specific setup requirements for running deep learning models in Google Colab?\"',\n",
       " 'f721d54b': 'Here are the 5 questions that the student might ask based on the FAQ record in a parsable JSON format:\\n\\n[\"How do I create an SSH private and public key?\", \"How do I add my public key to the SSH-agent?\", \"How do I add the default public key provided by Saturn Cloud to my Github repository?\", \"Can I just use the default public key provided by Saturn Cloud or do I need to generate my own?\", \"How do I check if my authentication is successful on Saturn Cloud?\"]',\n",
       " '69cd4897': '[\"Where is the Python TensorFlow template located on Saturn Cloud?\", \"How can I find the correct location of the Python TensorFlow template on Saturn Cloud?\", \"Can you please explain where the Python TensorFlow template is stored on Saturn Cloud?\", \"What happened to the Python TensorFlow template location shown in video 8.1b Setting up the Environment on Saturn Cloud?\", \"How do I access the \\'python deep learning tutorials\\' section on Saturn Cloud\\'s home page?\"]',\n",
       " '346e799a': '[\\n\"What happens when I get the error \\'module scipy not found\\' during model training in Saturn Cloud\\'s TensorFlow image?\",\\n\"Why do I get the \\'module scipy not found\\' error when training a model in Saturn Cloud\\'s TensorFlow image?\",\\n\"Is there a specific issue with scipy not being installed in the Saturn Cloud TensorFlow image where I\\'m trying to train my model?\",\\n\"What steps can I take to resolve the \\'module scipy not found\\' issue when training a model in Saturn Cloud\\'s TensorFlow image?\",\\n\"How do I ensure that scipy is installed when I create a new Jupyter server resource in Saturn Cloud for model training?\"',\n",
       " '551461b2': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\"How do I set up my Kaggle API token for uploading data to Saturn Cloud?\", \"Can I upload a single file from Kaggle to Saturn Cloud or do I need to download it locally?\", \"Why do I need to change the permissions of the kaggle.json file?\", \"What if I\\'m using a large dataset and downloading it locally takes a long time?\", \"How do I extract the files from the downloaded zip file to a specific folder in Saturn Cloud?\"]',\n",
       " 'c3ba4459': '[\"How do I install and run TensorFlow on my local machine?\", \"Why do I need to set up CUDA and cuDNN before installing TensorFlow?\", \"What is the process for setting up CUDA and cuDNN on Ubuntu 22.04?\", \"Can I use TensorFlow for deep learning without a GPU?\", \"How do I ensure TensorFlow is using my GPU for computation?\"]',\n",
       " 'a114ad55': 'Here are the 5 questions:\\n\\n[\"When I try to load a saved model, I get this error message: ValueError: Unable to load weights saved in HDF5 format into a subclassed Model which has not created its variables yet. Call the Model first, then load the weights. What\\'s going on?\", \\n\"What do I need to do before loading a saved model if I encounter a \\'ValueError\\'?\", \\n\"Why am I getting this error with saved models and not others?\", \\n\"What are the necessary steps to load a saved model without getting this error?\", \\n\"What is the purpose of loading a model first before loading its weights?\"]',\n",
       " 'dd3c8000': 'Here is the output:\\n\\n[\\n\"How can I avoid getting a permission denied error when connecting to Git on Saturn Cloud?\",\\n\"What alternative method can I use to set up Git in Saturn Cloud when I encounter permission issues?\",\\n\"Can I still access my Git repository through Saturn\\'s Jupyter server if I set up Git using the alternative method?\",\\n\"Are there detailed steps on how to set up Git in Saturn Cloud using the alternative method?\",\\n\"Can I find a tutorial on using Saturn Cloud\\'s Git repository feature?\"',\n",
       " '34b0ebfc': '[\"What are the correct steps to clone a repository when I get a \\'Host key verification failed\\' error?\", \"Can I still clone a repository if I don\\'t have my SSH key configured?\", \"Why do I get a \\'Host key verification failed\\' error when cloning a repository?\", \"How can I resolve the \\'Could not read from remote repository\\' error?\", \"Are there other ways to clone a repository besides using the default HTTPS method?\"]',\n",
       " '7d11d5ce': '[\"Why do my accuracy and loss remain the same or nearly the same even after training?\", \"What happens if I fail to set class_mode=\\'binary\\' when reading the data?\", \"What other factors could cause the same accuracy on epochs?\", \"How do I fix this issue with wrong optimizer settings?\", \"What affects my batch size and learning rate choices?\"]',\n",
       " 'e4e45f15': '[\\n\"What happens when resuming training after applying data augmentation if the loss skyrockets during the first epoch and the accuracy is around 0.5?\",\\n\"Why does the model become as good as a random coin flip after resuming training with data augmentation?\",\\n\"What should I check if my model\\'s loss becomes high during the first epoch after resuming training with augmented data?\",\\n\"Why do I need to include the option \\'rescale\\' in the Augmented ImageDataGenerator when resuming training?\",\\n\"What is the primary cause of a model\\'s poor accuracy when resuming training after data augmentation?\"',\n",
       " 'b3997e6f': 'Here are the 5 questions based on the FAQ record:\\n\\n[\"When I try to reload a saved model with tf.keras.models.load_model, I get a \\'Missing channel value error, what\\'s going on?\", \"Why do I get an error when the channel dimension of the inputs is not defined while reloading a model?\", \"How are model architecture and model weights saved when saving a model with tf.keras?\", \"When I reload a model with tf.keras.models.load_model, why does it complain about not knowing the number of channels?\", \"How can I specify the number of channels in the input layer of my model when reloading it with tf.keras.models.load_model?\"]',\n",
       " 'e414df91': '[\"How do I unzip a folder with an image dataset in a Jupyter notebook without seeing the output messages?\", \"Can I use the unzip command to extract an image dataset from a zipped folder?\", \"Is there a workaround to unzip a zipped folder in a Jupyter notebook without seeing all the output messages?\", \"How do I extract an image dataset from a zipped folder using Python code inside a Jupyter notebook?\", \"Can I use the%%capture command to suppress the output while unzipping a zipped folder in a Jupyter notebook?\"]',\n",
       " 'f20a3479': 'Here are the 5 questions based on the FAQ record:\\n\\n[\\n\"How does keras flow_from_directory determine the classes in images?\",\\n\"What is the default naming convention for classes in images when running train_gen.flow_from_directory?\",\\n\"Can I create a custom class folder name not found in the dataset?\",\\n\"How does keras flow_from_directory differentiate between folders and classes?\",\\n\"What if I accidentally create a folder with a random name, does it still get classified as a class?\"',\n",
       " 'e7af4968': '{\"questions\": [\\n\"Why do I get an error about scipy being missing when I use SaturnCloud with Tensorflow?\"]}\\n\\n{\"questions\": [\\n\"How can I solve the error about scipy being missing in SaturnCloud when using a new environment?\"]}\\n\\n{\"questions\": [\\n\"I installed scipy on my new SaturnCloud environment, but it doesn\\'t work. What\\'s the next step?\"]}\\n\\n{\"questions\": [\\n\"Why do I need to restart the kernel after installing scipy in SaturnCloud?\"]}\\n\\n{\"questions\": [\\n\"What\\'s the step-by-step process to resolve the error about scipy being missing in SaturnCloud?\"]}\\n\\nNote: The questions are based on the provided FAQ record, with some possible student questions that might arise from reading the answer. They are phrased in a way that is likely to be asked by a student who is trying to understand the solution.',\n",
       " '9fad096e': 'Here is the output:\\n\\n[\"How are numeric class labels determined in flow_from_directroy when using binary class mode?\", \"What is the probability predicted by a binary Keras model?\", \"How are class labels determined when reading folders in alphabetical order?\", \"How do I calculate the probability of class 0 given the probability of class 1 predicted by a Keras model?\", \"If a Keras model predicts binary labels, will it return two values?\"]',\n",
       " 'bcdf7407': '[\"What\\'s the significance of small changes in the actual values after using a neural network for prediction, considering it\\'s still a class-based outcome?\", \"Should the actual values after neural network prediction be looked at considering only the class of outcome rather than the exact value?\", \"Does the margin of error in actual values after prediction influence the class-based outcome?\", \"How important are the actual values after neural network prediction, considering it\\'s just a rough estimate for a class-based outcome?\", \"Can the actual values after prediction be considered as just a range within a class and not exact values?\"]',\n",
       " '8d1e7e20': 'Here are the 5 questions based on the FAQ record:\\n\\n[\\n\"Why do my accuracy and std training loss disagree with the Hardware answers?\",\\n\"What could be the reason for my mac laptop producing higher reported accuracy and lower std deviation than the Hardware answers?\",\\n\"Is it possible to adjust the runtime to make the model run faster on CPU?\",\\n\"Why did the same code produce answers that were closer on Google Collab than on my local machine?\",\\n\"Is there any conflict between the new and legacy version of SGD optimizer that could impact the results?\"',\n",
       " '2023a9dc': '[\"When running model.fit() for deep learning, is there a way to utilize multi-threading for data generation?\", \"Can I use multi-threading with the data generation in \\'model.fit()\\' method?\", \"How do I speed up data loading/generation when using \\'model.fit()\\'?\", \"What is the default number of workers used for data loading/generation in \\'model.fit()\\'?\", \"How do I determine the optimal number of workers for \\'model.fit()\\' on my system?\"]',\n",
       " '468f69ff': 'Here are the 5 questions based on the FAQ record:\\n\\n[\"How can I achieve reproducibility with TensorFlow when running my trained models multiple times?\", \\n\"What do I need to do to get consistent results from my neural network training runs?\", \\n\"Can I use a TF seed for reproducibility?\", \\n\"How can I ensure deterministic behavior in my TensorFlow script?\", \\n\"Is there a way to make my TensorFlow script invariant to random events?\"]',\n",
       " 'c4ff26e5': '[\"Can we use pytorch for this lesson/homework in order to do equivalent tasks as keras?\",\"Can we use kerass for this lesson/homework when pytorch has similar functions?\",\"Can the syntax in this lesson/homework be different from pytorch\\'s?\",\"Can we make pull requests to show equivalent tasks in pytorch for this lesson/homework?\",\"Can we use any other framework than keras in this lesson/homework without permission?\"]',\n",
       " '62722d72': 'Here are the 5 questions based on the FAQ record:\\n\\n[\\n\"What happens if I get the error \\'Failed to find data adapter\\' while training a Keras model?\",\\n\"Why am I getting the error \\'Failed to find data adapter that can handle input\\' during model training in Keras?\",\\n\"I\\'ve unintentionally passed an image generator instead of a dataset to the Keras model, what now?\",\\n\"Can you explain the difference between the image generator and the training dataset in Keras?\",\\n\"Why do I get an error when I pass the wrong variables to the Keras model\\'s fit function?\"',\n",
       " 'd1419be1': 'Here is the list of questions in JSON format:\\n\\n[\\n\"How do I run the \\'nvidia-smi\\' command in a loop without using \\'watch\\'?\",\\n\"Is there a way to automate running the \\'nvidia-smi\\' command with a specific interval?\",\\n\"Can I schedule the \\'nvidia-smi\\' command to run periodically without interruption?\",\\n\"Does the \\'nvidia-smi\\' command have an option to refresh its output at regular intervals?\",\\n\"How do I set the update interval for the \\'nvidia-smi\\' command?\"\\n]',\n",
       " 'a5f6f439': '[\"What is the Python package used for checking GPU and CPU utilization?\", \"How does the package \\'nvitop\\' compare to other tools like \\'htop\\' for CPU?\", \"Is \\'nvitop\\' an executable viewer?\", \"Are there any additional resources for using \\'nvitop\\'? \", \"Can I use \\'nvitop\\' on CPU only without a GPU?\"]',\n",
       " '879c1ec0': 'Here is the list of questions:\\n\\n[\"Where does the number of Conv2d layer\\'s params come from?\", \"Where does the number of \\'features\\' we get after the Flatten layer come from?\", \"How does the \\'model.summary()\\' function calculate the number of parameters for a Conv2d layer?\", \"What does the number 3*3*3 +1 in the calculation of the number of params mean?\", \"How does the Flatten layer transform the output shape of the previous layer into \\'features\\'?\"]',\n",
       " '3ac604c3': '[\"What is the key difference between a Sequential model and a Functional model in Keras?\", \"Why is the Sequential model API considered easier to work with?\", \"I need to recreate a full model from scratch, which API should I use?\", \"Can I use the Sequential model API for Transfer Learning when separating \\'Base\\' model vs. rest?\", \"Why should I do a \\'Fresh Run\\' on Neural Nets when correcting an architecture error?\"]',\n",
       " '0315aa96': 'Here are the 5 questions based on the FAQ record:\\n\\n[\"What can I do to fix out of memory errors when running TensorFlow on my Nvidia GPU?\", \"I keep getting out of memory errors when using TensorFlow on my GPU, what\\'s going on?\", \"Can you help me resolve the out of memory errors I\\'m experiencing when running TensorFlow on my system?\", \"How do I prevent out of memory errors when using TensorFlow on my GPU?\", \"What\\'s the simplest way to avoid out of memory errors when running TensorFlow?\"]',\n",
       " 'daf84bc3': '[\"When training the models in Google Colab with a T4 GPU, how do I speed up the process?\", \"In the fit function, how do I specify the number of workers/threads for model training?\", \"Can I use multiple threads when training models in Google Colab with a T4 GPU?\", \"How do I determine the optimal number of threads for my model training in Google Colab with a T4 GPU?\", \"Why is model training slower in Google Colab with a T4 GPU compared to other environments?\"]',\n",
       " '1e956ca7': '[\\n\"What is recommended for loading images instead of using ImageDataGenerator?\",\\n\"Can I still use ImageDataGenerator for new code?\",\\n\"Why was ImageDataGenerator deprecated?\",\\n\"What are the tutorials for loading images and augmenting images?\",\\n\"What is the preprocessing layer guide and how do I use it?\"\\n]',\n",
       " '3ee083ab': '[\"What are the prerequisites for Week 9 on Serverless Deep Learning?\", \"Can I use my own machine to deploy the serverless model?\", \"How can I determine which deep learning framework to use for serverless deployment?\", \"Are there any specific tools or libraries required for this week\\'s tasks?\", \"Can I refer to previous weeks\\' materials for clarification on any concepts used in Week 9?\"]',\n",
       " 'f826cba4': '[\"Where can I find the model for week 9, so I can use it for my project?\", \"How do I access the model for this week\\'s assignment?\", \"Can you provide me with the model for week 9, as I\\'m having trouble finding it?\", \"Where should I look for the week 9 model, as it\\'s not in the usual location?\", \"Is there a different location for the week 9 model, as I\\'m having trouble finding it?\"]',\n",
       " '60fa95ed': 'Here is the list of questions:\\n\\n[\"What happens when I execute the command echo ${REMOTE_URI}?\",\\n\"How can I resolve the issue of not getting the URI address in the terminal?\",\\n\"How do I set a local variable to use my URI address in the terminal?\",\\n\"Why do I need to use \\'export\\' before setting the local variable?\",\\n\"What is the difference between using \\'export\\' and \\'assign\\' to set the local variable?\"]',\n",
       " '53f3ee10': 'Here are the 5 questions this student might ask based on the FAQ record:\\n\\n[\\n\"How can I fix a syntax error while trying to use aws-cli to retrieve my password?\", \\n\"Why do I get an invalid choice error when running aws ecr get-login?\", \\n\"How do I simplify the login process for ECR?\", \\n\"What is an alternative command I can use instead of aws ecr get-login?\", \\n\"Can someone provide a full command to use for logging in with ECR and AWS CLI?\"',\n",
       " '93aa4278': '[\\n\"How can I avoid manually defining and training individual neural network models when working with a large number of parameters?\",\\n\"What are some strategies for optimizing the performance of complex neural networks in the serverless deep learning environment?\",\\n\"Can I use transfer learning for my serverless deep learning model and if so, how do I implement it?\",\\n\"In the context of serverless deep learning, how do I handle issues related to data preprocessing and feature engineering for my CNN model?\",\\n\"Is there a way to deploy a custom-built CNN model using the tensorflow library in a serverless environment?\"\\n]',\n",
       " '0edeb016': '[\\n\"What could cause the error \\'ERROR [internal] load metadata for public.ecr.aws/lambda/python:3.8\\' when building my docker image from the Amazon python base image?\",\\n\"How do I update my Docker Desktop to solve this issue?\",\\n\"Why is restarting Docker Desktop and terminal and building the image again a potential solution?\",\\n\"Is there a specific command I can use to bypass this issue if none of the above steps work?\",\\n\"Is there an alternative to the Docker BuildKit feature that I can use to build my image?\"',\n",
       " 'ba186de6': '[\"What commands can I use instead of \\'ls\\' for file listings in Windows Jupyter Notebooks?\", \"How can I troubleshoot issues with unrecognized commands in Jupyter Notebooks?\", \"Why do I get an error message when trying to use \\'ls\\' in Jupyter Notebooks?\", \"Can I use the same commands in Jupyter Notebooks as I would in a regular Windows command prompt?\", \"What is the reason behind the error message \\'ls\\' is not recognized as an internal or external command, operable program or batch file?\"]',\n",
       " 'da2f1cf4': '[\\n\"Why do I get an error message \\'ImportError: generic_type: type \\'InterpreterWrapper\\' is already registered!\\' when I run \\'import tflite_runtime.interpreter as tflite\\'?\",\\n\"What causes the \\'ImportError: generic_type: type \\'InterpreterWrapper\\' is already registered!\\' error when importing both tensorflow and tflite_runtime.interpreter?\",\\n\"Is it possible to import both tensorflow and tflite_runtime.interpreter in the same notebook without getting an error?\",\\n\"How do I fix the \\'ImportError: generic_type: type \\'InterpreterWrapper\\' is already registered!\\' error when importing tflite_runtime.interpreter?\",\\n\"Why do I need to restart the kernel to fix the \\'ImportError: generic_type: type \\'InterpreterWrapper\\' is already registered!\\' error?\"',\n",
       " '7fd648ca': 'Here are the 5 questions:\\n\\n[\"Why doesn\\'t my Windows version seem to be up-to-date?\",\\n\"What happens when I use the command $ docker build -t dino_dragon?\",\\n\"What error message do I see when I try to build a Docker image?\",\\n\"How do I know if the Docker daemon is running?\",\\n\"What programs might be stopping Docker from running on Windows?\"]',\n",
       " '42c09143': 'Here are the 5 questions this student might ask based on the FAQ record:\\n\\n[\\n\"What can I do to resolve the error when running docker build -t dino-dragon-model?\", \\n\"Why do I get a warning message saying I\\'m using an older version of pip?\", \\n\"Can I copy and paste the link to download the wheel file successfully?\", \\n\"What is the recommended wheel version for a specific Python version?\", \\n\"What are the common sources of errors when building a Docker image for serverless deep learning?\"\\n]',\n",
       " 'd6d534fc': 'Here are the 5 questions based on the FAQ record:\\n\\n[\"What are the steps to configure AWS after installing awscli?\", \"What should we input for Default output format when configuring aws configure?\", \"Is it okay to leave Default output format as \\'None\\' when configuring aws configure?\", \"Are the default values provided for Default Region Name and Default output format sufficient?\", \"Do we need to fill in all the fields when configuring aws configure or can we leave some blank?\"]',\n",
       " 'b2c0c554': 'Here are the 5 questions the student might ask:\\n\\n[\"What if I\\'m getting an error message like \\'Object of type float32 is not JSON serializable\\' while testing my lambda function with a running docker instance?\", \"I\\'m having trouble with some float32 values not being serializable, can you explain why this is happening?\", \"How can I fix the error message \\'Object of type float32 is not JSON serializable\\' when running a docker instance?\", \"I\\'m trying to return individual estimation values as numpy float32 values, but they\\'re not becoming \\'serializable\\', what\\'s going wrong?\", \"Why do I need to convert numpy float32 values to base-Python floats for them to be \\'serializable\\' in my lambda function?\"]',\n",
       " '819afebc': '[\\n\"What happens when I encounter an error with the line \\'interpreter.set_tensor(input_index, X)\\'\",\\n\"Why do I get a ValueError: Cannot set tensor: Got value of type UINT8 but expected type FLOAT32 for input 0, name: serving_default_conv2d_input:0\",\\n\"What is wrong with my X variable that is causing an error when I try to set the tensor\",\\n\"How can I fix the \\'Cannot set tensor\\' error when the X value is of the wrong type\",\\n\"Why does the value of type FLOAT64 but expected type FLOAT32 for input 0 cause an error\"',\n",
       " '74551c54': 'Here is the list of questions:\\n\\n[\"How do I get the file size in the PowerShell terminal?\", \"Is there a way to easily check the length of a file in PowerShell?\", \"Can I use PowerShell to get the size of a file in bytes?\", \"How can I display the file size in a specific unit like MB in PowerShell?\", \"Are there any reliable sources for learning PowerShell commands like this?\"]',\n",
       " '4d98cd09': '[\\n\"How do Lambda container images work with deep learning models? Can you provide some documentation or resources?\",\\n\"What are the benefits of using serverless deep learning with AWS Lambda?\",\\n\"How do I optimize the initialization of my lambda function for deep learning tasks?\",\\n\"Are there any limitations or challenges when using deep learning with AWS Lambda?\",\\n\"What is the typical use case for using deep learning with AWS Lambda?\"',\n",
       " '59a81fd5': 'Here are the 5 questions based on the FAQ record:\\n\\n[\"How do I create a docker image for AWS Lambda?\", \\n\"How can I push my docker image to AWS ECR?\", \\n\"Can I expose my rest api through API GatewayService in AWS Serverless Framework?\", \\n\"How do I set up AWS Serverless Framework to deploy on AWS Lambda?\", \\n\"Can I find a detailed walkthrough on how to deploy a serverless flask application to AWS Lambda?\"]',\n",
       " '35dbd6e2': 'Here are 5 questions based on the FAQ record:\\n\\n[\"Why do I get an error building a docker image on my M1 Mac?\", \"How do I solve the pip install error while building a docker image?\", \"What does it mean by the arm architecture problem with M1 Macs when building a docker image?\", \"Can I run the code on a PC or Ubuntu OS instead of an M1 Mac?\", \"How do I modify the docker build command to get it working on my M1 Mac?\"]',\n",
       " 'e5fe9efe': '[\\n\"How do I test API Gateway locally with an error message that says \\'Missing Authentication Token\\'?\"\\n\"Why am I getting a \\'Missing Authentication Token\\' error when I try to deploy my API gateways locally?\"\\n\"What is the correct way to invoke the API Gateway locally and what might be causing the \\'Missing Authentication Token\\' error?\"\\n\"How do I get the deployed API URL for a specific path to fix the \\'Missing Authentication Token\\' error?\"\\n\"What is the solution to the \\'Missing Authentication Token\\' error when trying to test API Gateway locally?\"',\n",
       " '5c043c62': 'Here is the list of questions the student might ask based on the FAQ record:\\n\\n[\\n\"How can I fix the error when trying to install tflite_runtime with !pip install --extra-index-url https://google-coral.github.io/py-repo/ tflite_runtime?\",\\n\"Are there any specific OS-Python version combinations that tflite_runtime is available for?\",\\n\"Why is tflite_runtime not available for my current OS-Python version combination?\",\\n\"How do I install a specific version of tflite_runtime that is compatible with my system?\",\\n\"Are there any alternative ways to use tflite_runtime if it\\'s not available for my system?\"',\n",
       " 'af0739da': '[\"When I try to run Docker for the serverless deep learning section, I encounter a \\'docker: Error response from daemon: mkdir /var/lib/docker/overlay2/37be849565da96ac3fce34ee9eb2215bd6cd7899a63ebc0ace481fd735c4cb0e-init: read-only file system\\' error. How do I resolve this?\", \"How can I restart the Docker services to fix the error \\'docker: Error response from daemon: mkdir /var/lib/docker/overlay2/37be849565da96ac3fce34ee9eb2215bd6cd7899a63ebc0ace481fd735c4cb0e-init: read-only file system\\'?\", \"I\\'m having trouble setting up Docker for the serverless deep learning section. Can you provide a solution to the error \\'docker: Error response from daemon: mkdir /var/lib/docker/overlay2/37be849565da96ac3fce34ee9eb2215bd6cd7899a63ebc0ace481fd735c4cb0e-init: read-only file system\\'?\", \"When I try to run a Docker container, I get the error \\'docker: Error response from daemon: mkdir /var/lib/docker/overlay2/37be849565da96ac3fce34ee9eb2215bd6cd7899a63ebc0ace481fd735c4cb0e-init: read-only file system\\'. What\\'s the solution to this issue?\", \"Can someone help me with the \\'docker: Error response from daemon: mkdir /var/lib/docker/overlay2/37be849565da96ac3fce34ee9eb2215bd6cd7899a63ebc0ace481fd735c4cb0e-init: read-only file system\\' error while running Docker in the serverless deep learning section?\"]',\n",
       " '451bc25d': '[\"How do I save a Docker image to my local machine?\", \"Can I view the contents of a Docker image as individual layers?\", \"What command can I use to save a Docker image to a tar file?\", \"How do I view the filesystem content of a Docker image layer?\", \"Is there a way to extract a Docker image layer to view its contents?\"]',\n",
       " 'ea2e7458': '[\"Why doesn\\'t my Jupyter notebook see the package after installing it with \\'pip install\\'?\",\"Why do I need to restart my Jupyter notebook for the imports to work after installing a new package?\",\"I installed a package in VS Code, but it\\'s not recognized in my Jupyter notebook. Why is that?\",\"How do I make my Jupyter notebook recognize a package that I installed with \\'pip\\'?\",\"What steps should I take if a package I installed is not recognized in my Jupyter notebook?\"',\n",
       " '6ce8e875': '[\"What happens if I run out of space on my AWS instance while experimenting with serverless deep learning?\", \"Why aren\\'t deleted docker images freeing up space on my instance?\", \"How do I make sure I don\\'t run out of space in the future when working with serverless deep learning?\", \"What are some best practices for managing storage space when working with AWS instances?\", \"Can you provide a summary of the common pitfalls to avoid when it comes to storage space on AWS instances?\"]',\n",
       " 'b50e9e2b': 'Here is the list of questions:\\n\\n[\"Using Tensorflow version for AWS deployment\", \"What Tensorflow version with python 3.11 works fine for AWS deployment\", \"Can I use a different Tensorflow version if the first one doesn\\'t work\", \"What are the supported python versions to install tf==2.4.4\", \"What happens if I install tf==2.4.4 on an unsupported python version\"]',\n",
       " '29311ef5': '[\"What happens when I run the command aws ecr get-login --no-include-email and it returns an error message saying \"aws: error: argument operation: Invalid choice…\", \"Can I use a different command instead of aws ecr get-login --no-include-email\", \"How do I troubleshoot the issue when the command aws ecr get-login --no-include-email is not working as expected\", \"Why is this happening when I\\'m trying to use aws ecr get-login --no-include-email in my script\", \"What other options do I have if I\\'m stuck with this error message from aws ecr get-login --no-include-email\"]',\n",
       " '1e0dc11c': 'Here are the 5 questions based on the FAQ record:\\n\\n[\"What IAM permission policy is needed to complete Week 9: Serverless?\",\\n\"How do I navigate to the IAM service in the AWS Console?\",\\n\"What are the ECR actions included in the JSON policy?\",\\n\"Can you provide more information about the \\'Review and create the policy\\' step?\",\\n\"Why do I need to delete the file ~/.docker/config.json to solve an error in WSL2?\"]',\n",
       " '1078aeb7': '[\"What is the solution to the \\'Docker Temporary failure in name resolution\\' issue in the context of Serverless Deep Learning?\", \"How can I troubleshoot DNS issues with Docker?\", \"Why am I experiencing name resolution problems with Docker, and what can I do to resolve them?\", \"Can you provide a step-by-step guide to fix the DNS resolution issue with Docker?\", \"How do I restart the Docker service after making changes to the daemon configuration file?\"]',\n",
       " '7daaca73': '[\"Why can\\'t I load my Keras model file .h5 into the code?\", \"What is causing the error weight_decay is not a valid argument, kwargs should be empty for `optimizer_experimental.Optimizer`?\", \"How can I resolve an issue with loading a Keras model file .h5 into the code?\", \"Why do I get an error when loading a Keras model file using `load_model` function?\", \"What is the correct way to load a Keras model file .h5 into the code with `load_model` function?\"]',\n",
       " '0cfbe2e2': 'Here are the 5 questions based on the FAQ record:\\n\\n[\"How can I set up AWS Lambda and Docker for local testing?\", \"Can I use a base AWS Lambda image built on Python 3.10 for local testing?\", \"What ports should I use for the docker run command?\", \"Can I test AWS Lambda deployment using a Unix OS?\", \"How can I resolve a Runtime.MarshalError like \\'Object of type float32 is not JSON serializable\\' in lambda_handler()?\" ]',\n",
       " '1460fb65': 'Here are the 5 questions a student might ask based on the FAQ record:\\n\\n[\"When I run my Python script, why do I get the error \\'Unable to import module\\'?\",\\n\"How do I fix the \\'No module named \\'tensorflow\\'\\' error?\",\\n\"Why do I still get the \\'No module named \\'tensorflow\\'\\' error after importing TensorFlow?\",\\n\"What is the issue with importing TensorFlow in my script?\",\\n\"What is the correct way to import TensorFlow in my Python script?\"]',\n",
       " 'd4f9efdc': 'Here are the 5 questions based on the FAQ record:\\n\\n[\"How do I install Docker (udocker) in Google Colab?\", \"What are the errors encountered when attempting to use Lambda API Gateway?\", \"Why am I unable to run pip install tflite_runtime from github wheel links?\", \"How can I troubleshoot issues with udocker in Google Colab?\", \"What are the requirements for the Authorization header in an API Gateway request?\"]',\n",
       " '6a417bfe': '[\"How can I get started with Week 10 of the course, as per the FAQ record?\", \"What are the necessary steps to set up TensorFlow Serving, as mentioned in the FAQs?\", \"Is there any recommended resources or tutorials for running TensorFlow Serving on Kubernetes?\", \"How do I troubleshoot common issues with TensorFlow Serving on Kubernetes, according to the FAQ?\", \"Can I use existing TensorFlow models with TensorFlow Serving, or do I need to retrain them?\"]',\n",
       " 'ed8b300d': 'Here are the 5 questions based on the FAQ record:\\n\\n[\\n\"How do I install TensorFlow on my Ubuntu WSL2 setup?\",\\n\"What are some ways to speed up running a CNN on my CPU?\",\\n\"How do I get CUDA support on my local machine?\",\\n\"Are there any specific requirements for installing TensorFlow with CUDA support?\",\\n\"Can I install both TensorFlow and PyTorch on my machine at the same time?\"',\n",
       " 'a64aed6b': 'Here is the list of questions:\\n\\n[\\n\"What are the symptoms of Allocator (GPU_0_bfc) ran out of memory trying to allocate\",\\n\"What are potential performance gains with more memory available\",\\n\"Can this be resolved by adding code to a cell at the beginning of a notebook\",\\n\"Why did the error still occur once more even after adding the code\",\\n\"Who is Martin Uribe and what did he contribute?\"',\n",
       " '727238ee': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\\n\"What do I do when trying to run the script gateway.py and getting the error TypeError: Descriptors cannot not be created directly?\",\\n\"What version of protoc is required to generate code for this session?\",\\n\"Why do I get this error when creating the virtual environment?\",\\n\"What are the possible workarounds for this error if I cannot generate the protos immediately?\",\\n\"How do I downgrade the protobuf package to a lower version?\"',\n",
       " '85d4901d': '[\"How to resolve the error message \\'Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\\' when trying to run a docker command?\", \"Why do I get this error message on WSL?\", \"What to do when Docker Desktop is not connecting to the WSL Linux distro?\", \"How to enable WSL Integration in Docker Desktop?\", \"Is it required to have the same default WSL distro for additional distros?\"]',\n",
       " 'df023a13': '[\"What if my HPA instance doesn\\'t run properly despite installing the latest version of Metrics Server from the components.yaml manifest?\", \"Why do my targets still appear as <unknown> even after the latest Metrics Server installation?\", \"How can I fix the formatting in the args section of the metrics-server deployment if it is not allowing the --kubelet-insecure-tls flag to be added?\", \"What should I do if my metrics-server deployment still has errors after editing the args section in the deployment yaml?\", \"Will making this change resolve the aforementioned issues with the HPA instance?\" ]',\n",
       " '48e92d65': '[\"How can I resolve HPA instance issues even after installing the latest version of Metrics Server\", \"Why do the HPA targets appear as <unknown>\", \"What is the correct way to update Metrics Server to the latest version\", \"How do I fix the HPA instance if the targets still appear as <unknown>\", \"Why do I need to use a metrics server deployment file with the --kubelet-insecure-tls option\"]',\n",
       " '1685cae4': 'Here is the list of 5 questions the student might ask based on the FAQ record:\\n\\n[\"What causes the OSError: [WinError 5] Access is denied error when installing packages with pip?\", \"Why do I need to use the --user option when installing packages with pip?\", \"What is the purpose of using the --user option with pip install?\", \"How can I resolve the issue of access being denied when installing packages with pip?\", \"What are some common solutions for permissions issues when installing packages with pip?\"]',\n",
       " '4fb7b21e': '[\\n\"What is the procedure to resolve a TypeError when running gateway.py in a modified environment after video 10.3?\",\\n\"Why do I get a TypeError: Descriptors cannot not be created directly. when importing protobuf files?\",\\n\"Can you suggest a solution for this error message when running a Python script with TensorFlow Serving?\",\\n\"How can I resolve this error when installing a specific version of protobuf using pipenv?\",\\n\"What are some possible workarounds if I cannot regenerate the protos immediately?\"',\n",
       " '8bd3bfc2': '[\\n\"How to install kubectl on Windows using the terminal in VS Code?\",\\n\"What is the step-by-step process to download and install kubectl using curl on Windows?\",\\n\"Can I install kind on Windows using the same curl command as for kubectl?\",\\n\"How do I add a folder path to the PATH environment variable for kubectl installation?\",\\n\"What is the tutorial link to follow for installing kubectl on Windows using VS Code PowerShell?\"\\n]',\n",
       " '03b5fc59': '[\"How do I install Kind through the Choco library in a PowerShell terminal with administrator privilege?\", \"How do I launch a PowerShell terminal with administrator privilege?\", \"What are the steps to install Choco library in PowerShell?\", \"What is the syntax to download the Chocolatey install script through PowerShell?\", \"How do I use the Chocolatey install script to install Choco library?\"]',\n",
       " '7c31bc9a': '[\\n\"How do I install Kind if I\\'m having trouble using the Windows Powershell and Choco Library?\",\\n\"Can I use Go to install Kind instead of the other methods?\",\\n\"Do I need to download and install Go separately to use Kind?\",\\n\"Can I confirm the Go installation by typing a specific command in Command Prompt?\",\\n\"How do I check if Kind is installed after installing it using Go?\"',\n",
       " '605efc12': 'Here are the 5 questions this student might ask based on the FAQ record:\\n\\n[\"What should I do if I encounter the error \\'The connection to the server localhost:8080 was refused - did you specify the right host or port?\\' when trying to use kubectl?\", \"How do I troubleshoot issues with kubectl not working?\", \"Is it possible to correct \\'The connection to the server localhost:8080 was refused - did you specify the right host or port?\\' error?\", \"What steps should I take if kubectl isn\\'t working for me?\", \"Can I resolve \\'The connection to the server localhost:8080 was refused - did you specify the right host or port?\\' issue by restarting everything?\"]',\n",
       " 'c5cde96c': '[{\"Why does deleting docker images not actually free up any storage space? I just removed a bunch but it didn\\'t help with my full instance\"}',\n",
       " 'd45d2da6': '[\\n\"In the context of HW10 Q6, what exactly does \\'correct value for CPU and memory\\' mean, since they seem like arbitrary specifications?\"\\n\"What\\'s the significance of providing CPU and memory values in the yaml file for HW10 Q6, if the port value is what\\'s being referred to?\"\\n\"Are CPU and memory values in the yaml file for HW10 Q6 just placeholders or do they serve a specific purpose?\"\\n\"How do I determine the \\'correct value\\' for CPU and memory in the yaml file for HW10 Q6, given that they\\'re seemingly arbitrary?\"\\n\"Why does HW10 Q6 require specifying CPU and memory values in the yaml file, when it seems like they\\'re not being utilized?\"',\n",
       " '59823c72': '[\"What do the cpu vals for Kubernetes deployment.yaml specs look like such as \\'100m\\' and \\'500m\\'?\", \"What does \\'m\\' mean in \\'100m\\' and \\'500m\\' cpu vals?\", \"Why do cpu requests and limits in Kubernetes use milliCPUs instead of whole CPU cores?\", \"Can you elaborate on how specifying cpu values as milliCPUs allows for fine-grained control over resources?\", \"How do \\'cpu: 100m\\' and \\'cpu: 500m\\' translate to cpu core equivalents?\"]',\n",
       " '665f7b27': '[\\n\"How do I fix the issue of being unable to load a docker image into a cluster when I\\'ve named the cluster?\",\\n\"Why am I getting an error message that says \\'no nodes found\\' when trying to load a docker image to a specified cluster name?\",\\n\"Can you help me troubleshoot why I\\'m experiencing problems loading a docker image to a cluster I\\'ve created?\",\\n\"What should I do if I\\'m unable to load a docker image to a cluster I\\'ve named and I\\'m getting an error message?\",\\n\"How can I successfully load a docker image to a cluster if I haven\\'t specified the cluster name correctly?\"\\n]',\n",
       " '0a406fe0': '[\\n\"How to resolve the \\'kind\\' is not recognized as an internal or external command, operable program or batch file. error in Windows?\", \\n\"Why do I need to rename the \\'kind\\' executable to kind.exe?\", \\n\"What is the correct process to download and set up \\'kind\\' on Windows?\", \\n\"Can I directly run the \\'kind\\' executable without renaming it?\", \\n\"What is the purpose of adding the \\'kind\\' executable to the PATH?\"',\n",
       " '64b209b0': '[\\n\"How do I run kind on a Linux machine using Rootless Docker or Rootless Podman?\",\\n\"What are the system-level changes required for running kind with Rootless Docker or Rootless Podman on Linux?\",\\n\"Can I use kind with Rootless Docker or Rootless Podman on my Linux machine without root privileges?\",\\n\"Are there any specific instructions for running kind with Rootless Docker or Rootless Podman on Linux?\",\\n\"What kind of Linux configurations are necessary for running kind with Rootless Docker or Rootless Podman?\" ]',\n",
       " '518c4cb8': 'Here are the 5 questions based on the FAQ record:\\n\\n[\"What are the steps to deploy the Kubernetes Dashboard?\", \"How do I access the Kubernetes Dashboard after deployment?\", \"Is it necessary to have specific knowledge of Kubernetes to access the dashboard?\", \"Can I customize the Kubernetes Dashboard to suit my needs?\", \"Will the tutorial on the dashboard also cover TensorFlow Serving?\"]',\n",
       " '00882c83': 'Here is the list of questions:\\n\\n[\"What is the correct version of AWS CLI that I should be using for eksctl?\", \"How can I check the version of AWS CLI that I am currently using?\", \"What are the instructions for migrating to AWS CLI v2?\", \"Can I still use eksctl with an older version of AWS CLI?\", \"Do I need to perform any special configuration to use eksctl with AWS CLI v2?\"]',\n",
       " 'd6d483ce': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"What do I do when I get a TypeError in Flask?\", \"Why do I get an error when importing Flask?\", \"Can you explain why I\\'m getting an error with Werkzeug and Flask versions?\", \"How can I resolve the issue with newer Flask version and older Werkzeug version?\", \"Can I simply import Flask without specifying the version?\"]',\n",
       " 'f9711723': '[\"What does the \\'Invalid choice...\\' error message mean when running the \\'aws ecr get-login\\' command?\", \"Why do we need to change the fields in the \\'aws ecr get-login\\' command?\", \"How do we get the correct region without specifying it manually?\", \"Is there an alternative command we can use instead of \\'aws ecr get-login\\'?\", \"Can we use our default region when running the \\'aws ecr get-login\\' command?\"]',\n",
       " '5bda3b94': '[\"How do I resolve the error when trying to download tensorflow/serving:2.7.0 on an Apple M1 Mac?\", \"Can I use docker run command for tensorflow/serving:2.7.0 on M1 Mac without getting an error?\", \"What are the key differences between running tensorflow/serving:2.7.0 and tensorflow/serving:latest\", \"Can I modify the MODEL_NAME environment variable in the docker run command for tensorflow/serving:2.7.0\", \"Why do I get a \\'qemu uncaught target signal 6 (Aborted)\\' error when running tensorflow/serving:2.7.0 on M1 Mac\"]',\n",
       " 'cccd31cf': 'Here are the 5 questions the student might ask:\\n\\n[\"What are the reasons behind the illegal instruction error when running tensorflow/serving image on Mac M2 Apple Silicon?\", \"Why do we get an error while trying to run the docker code on Mac M2 apple silicon?\", \"How can we resolve the illegal instruction error when running tensorflow/serving image on Mac M2 Apple Silicon?\", \"Can you provide a solution for running the tensorflow/serving image on Mac M2 Apple Silicon?\", \"Can you provide an alternative to using tensorflow/serving image, as it seems to be outdated?\"]',\n",
       " '57f49999': 'Here are the 5 questions based on the FAQ record:\\n\\n[\"Why doesn\\'t my HPA show CPU metrics?\", \"What could be the reason behind CPU metrics Showing Unknown?\", \"How can I fix the issue of unknown CPU metrics in HPA?\", \"What could I do to resolve the failed get resource metric issue in HPA?\", \"How can I get CPU utilization metrics when HPA is not showing them?\"]',\n",
       " '5cb58698': 'Here are 5 questions this student might ask based on the FAQ record:\\n\\n[\"What are the common errors that occur during Istio installation?\", \"Why does the command \\'curl -s \\\\\"https://raw.githubusercontent.com/kserve/kserve/release-0.9/hack/quick_install.sh\\\\\" | bash\\' fail to execute?\", \"What happens when trying to update resources with Istio and I\\'m on kubectl > 1.25.0?\", \"How do I resolve the issue of Istio resources not updating correctly?\", \"What are the possible solutions if I\\'m encountering errors during KServe installation?\"]',\n",
       " 'de650b41': 'Here is the list of questions:\\n\\n[\\n\"Can I work on my mid-term and capstone projects individually, or do I need to form teams with other students?\",\\n\"Are the project problems similar to the ones we\\'ve seen in class, or will they be completely new and challenging?\",\\n\"How do I know which section of the project solution is relevant to my own problem scenario?\",\\n\"What is the word count limit for the problem title and description, and how will it be evaluated?\",\\n\"Are the project problems strictly theoretical or will we have to implement some kind of simulation or code to solve them?\"',\n",
       " '9ffacaac': '[\\n\"How do I find the project deadlines?\", \"What are the project deadlines this year compared to last year?\", \"What is the due date for my midterm project?\", \"Where can I find the due dates for the projects?\", \"Are the project deadlines the same for everyone in the course?\"\\n]',\n",
       " '4dfb5d4f': 'Here are the 5 questions:\\n\\n[\\n\"To what extent are the midterm and capstone projects focused on solo work or group projects?\",\\n\"Are there any instances where group projects are allowed in the midterm and capstone projects?\",\\n\"Are the midterm and capstone projects collaborative efforts with peers?\",\\n\"Are the projects carried out individually for the midterm and capstone assessments?\",\\n\"Do the projects for midterm and capstone have to be done individually or can we work with a team?\"',\n",
       " '0b8739b7': '[\"What modules should a midterm/capstone project cover? Up to module-06?\", \"Can I do projects that include topics not covered in the class?\", \"What problem-sets should I include in my midterm/capstone project?\", \"Can I include extra topics in my project?\", \"Can I feature extra features in my capstone project?\"]',\n",
       " '9eb52679': 'Here are the 5 questions based on the FAQ record:\\n\\n[\"What are the crucial links for all projects in the course?\", \"Where can I find additional or different instructions for my cohort?\", \"How do I prepare a dataset for my project?\", \"What are the deliverables for the Midterm Project?\", \"Can I get bonus points for deploying the model to the cloud?\"]',\n",
       " '7a1fcfd9': 'Here are the 5 questions based on the FAQ record:\\n\\n[\"How do I conduct peer reviews for projects?\", \"Can I find instructions on how to conduct peer reviews on the previous cohorts\\' projects page?\", \"Where can I find the compile Google Sheet with links to submitted projects to review?\", \"Can I check the leaderboard for homework to see when I can start reviewing projects?\", \"What is the deadline to complete reviewing projects?\"]',\n",
       " '1cfa62c5': '[\"What is the method for computing the hash for project review?\", \"Can I use a specific tool to compute the hash for project review?\", \"How do I know if my project\\'s hash is calculated correctly?\", \"Is there a limitation on the number of times I can recalculate the hash for project review?\", \"Is there a way to verify the hash for the project review if it doesn\\'t match the expected value?\"]',\n",
       " '2a78f52e': '[\"What does a total value of 14 mean for the learning in public aspect of the midterm project?\", \"How many posts do we need to make for the learning in public, considering regular seven posts for each module with a value of 2?\", \"Is the total value of 14 referring to a single post with a value of 14?\", \"Can we make just one post with a total value of 14, or is it necessary to make multiple posts?\", \"Do we need to make 14 posts, one for each day, for the learning in public for this midterm project?\"]',\n",
       " '68aeab64': '[\"What if my dataset is too large to load in GitHub, and I need to upload it?\", \"How do I handle datasets of varying file formats, such as audio or video files?\", \"Can I use GitHub as a platform for sharing and collaborating on projects besides just coding?\", \"What if I get errors when trying to upload my capstone project to GitHub\\'s platform?\", \"Can GitHub be used to share and access files larger than 25 MB?\",]',\n",
       " '9a7c26e0': '[\"What happens if I submitted only two projects and didn\\'t submit the third one for the midterm and capstone projects? I\\'m worried I will be penalized.\", \"If I have submitted two midterms, but only one capstone project, will I receive the course certificate?\", \"Do I need to submit all three projects for the midterm and capstone section in order to pass the course?\", \"If I submitted two projects and peer-reviewed my peers\\' projects for each, will I still receive the course certificate even if I didn\\'t submit the third project?\", \"Are there any exceptions for the project submission requirements for the midterm and capstone section?\"]',\n",
       " '1fd83eb9': '{\"questions\": [\"I did the first two projects and skipped the last one so I wouldn\\'t have two peer reviews before submitting the second capstone is that correct?\", \"Is it necessary to submit the exact same project as specified in the template, or is it okay to add or remove certain sections?\", \"What if I missed the deadline for one of the previous projects can I still do it and receive credit?\", \"Are there any specific formatting or naming conventions I need to follow when submitting my project?\", \"I have a group project coming up and I\\'m responsible for communication with my team what are the best practices I should follow to ensure a smooth collaboration?\"]}',\n",
       " 'fbaa5b20': '[\"What are the general guidelines for the number of models I need to train for the midterm deliverables, similar to the example mentioned in Point 4?\", \"Can I train just one model and consider it the best model, or do I need to compare multiple models for the midterm project?\", \"In the context of the midterm project, how do I know I\\'ve trained enough models, if I\\'m comparing multiple models but not all of them are as good as others:\", \"What are the expectations regarding the number of models to train for the Capstone project, given that the midterm project mentions training multiple models?\", \"As I\\'m training multiple models for the midterm project, how do I effectively identify the best model from the ones I\\'ve trained?\"]',\n",
       " '37eab341': 'Here are the questions based on the FAQ record:\\n\\n[\"How does the project evaluation work for reviewers\", \"How do I find the list of submitted projects to be evaluated\", \"How do I calculate my hash value and derive my peer projects\", \"Where do I need to store and run the Python code\", \"Can I use a different hash value for review assignment\"]',\n",
       " '57754faf': '[\"What is the scoring approach used for project evaluation, considering both individual and peer\\'s scores?\", \"Are there any course-related questions that do not fit into specific categories or modules?\", \"Will I pass a project based on its total score or the average of everyone\\'s scores?\", \"How is it ensured that most students pass a project based on all the scores?\", \"Can I apply the same questions to multiple categories or modules in the course?\"]',\n",
       " '6979c5d1': '[\"What does my peers review my midterm project with?\", \"Why does my peers need to run my training process on their own system?\", \"Is the train.py file used for anything else?\", \"Why is it necessary to include the train.py file in my environment?\", \"Can\\'t you just review the notebook.ipynb file instead of the train.py file?\"]',\n",
       " 'a1bd8c34': '[\\n\"How do I install the PILLOW library so I can use it to load and convert images?\",\\n\"What is the proper syntax to open an image file using PILLOW?\",\\n\"How do I convert an image loaded with PILLOW to a numpy array?\",\\n\"Can I use a different image file name when opening an image with PILLOW?\",\\n\"What libraries or modules need to be imported to work with PILLOW and numpy arrays?\"',\n",
       " 'b2ab0fc1': '[\\n\"What if I have both train.py and train.ipynb files in my midterm project directory? Do I need to keep both or is one sufficient?\",\\n\"What is the main difference between running a train.py file and a train.ipynb file for training a model?\",\\n\"In a real-life scenario, why are train scripts usually Python scripts instead of Jupyter notebooks?\",\\n\"Can I still use a Jupyter notebook for training a model if I don\\'t have a train.py file?\",\\n\"What are the implications of not having a train.py file in my project, especially when submitting the project?\"',\n",
       " '80c439a9': '[\\n\"How can I create a form for users to input data for my model to process?\",\\n\"Can I make a mobile app or interface to manage these forms and ensure data validation?\",\\n\"Are there any guidelines for validating user input on the backend side?\",\\n\"Is it possible to integrate Streamlit with the model to handle form submissions?\",\\n\"Can you provide an example of a successful implementation of a form-based data submission system using the model?\"\\n]',\n",
       " 'ff93b86e': '[\"What are the correct steps to get feature importance for an XGBoost model, as the model.feature_importances_ method does not seem to work?\", \"Can you help me understand why I\\'m getting an AttributeError error when trying to get feature importance for an XGBoost model?\", \"How do I train an XGBoost model correctly to avoid the AttributeError error when trying to get feature importance?\", \"What is the correct method to use to get feature importance for an XGBoost model, as the model.feature_importances_ method is not working?\", \"Is there an alternative to the model.feature_importances_ method for getting feature importance in XGBoost?\"]',\n",
       " 'fcd86c8f': '[\"What if I encounter the \\'[Errno 12] Cannot allocate memory\\' error in the Elastic Container Service task log?\", \"How can I resolve memory allocation issues in AWS Elastic Container Service?\", \"What can I do to fix the \\'cannot allocate memory\\' error in Elastic Container Service?\", \"In AWS Elastic Container Service, how do I increase the memory and CPU?\", \"Why am I receiving the \\'[Errno 12] Cannot allocate memory\\' error when using Elastic Container Service?\"]',\n",
       " '236864c2': '[\"When running a docker container with waitress serving the app.py for making predictions, pickle will throw an error that can\\'t get attribute <name_of_class> on module __main__ why is that?\", \"What\\'s the difference between using Flask directly and using it through waitress?\", \"Why does pickle reference the class in the global namespace?\", \"Why doesn\\'t this issue occur when Flask is used directly, but not through waitress?\", \"How can I resolve this issue with the custom column transformer class?\"]',\n",
       " 'efc4a04f': 'Here is the list of questions:\\n\\n[\\n\"How do I edit my dataset to better suit analysis?\",\\n\"What are some common methods for removing or altering outliers from a dataset?\",\\n\"Can I use transformation or clipping to handle outliers in my dataset?\",\\n\"What are the best ways to tackle unusual data points in my dataset?\",\\n\"Are there any additional strategies for managing outliers that I should be aware of?\"\\n]',\n",
       " '15f361b7': 'Here is the list of questions:\\n\\n[\"What causes the error message \\'Failed to import module \"service\": No module named \\'sklearn\\'\" when creating a docker image with BentoML?\", \"Why do I get an error when loading Bento from a directory with BentoML?\", \"How do I resolve the issue of a failed loading Bento in a certain directory with BentoML?\", \"What package should I use instead of \\'sklearn\\' in bentofile.yaml?\", \"Can I use \\'xgboost\\', \\'scikit-learn\\', and \\'pydantic\\' as packages in bentofile.yaml?\"]',\n",
       " 'dbbce78b': 'Here is the list of questions the student might ask based on the FAQ record:\\n\\n[\\n\"Why does setting DictVectorizer or OHE to sparse during training cause issues with the –production flag?\",\\n\"What can I do to resolve the long error message and code 500 error when running the bentoml container with the –production flag?\",\\n\"Why do I get an empty string as output when running the bentoml container with the –production flag?\",\\n\"What is the meaning of sparse matrices in the context of bentoml and –production flag?\",\\n\"How do I set the batchable flag to False for bentoml model signatures during the saving process in train.py?\"',\n",
       " 'f3a00e15': '[\"Do we have to run every dataset and instruction in the course?\", \"How can I check if there are any obvious errors in the code?\", \"Must we run all the neural networks in the course?\", \"What does it mean to reproduce the results in the course?\", \"Can I see everything I need to reproduce the results?\"]',\n",
       " '9102b3c0': 'Here are 5 questions based on the FAQ record:\\n\\n[\"What happens if my model is too big for GitHub?\",\\n\"Can I compress my model to reduce its size?\",\\n\"How can I compress a large model in Python?\",\\n\"What is the best way to deal with a large model?\",\\n\"How do I make my model smaller so it can be uploaded to GitHub?\"]',\n",
       " '70d89fdf': '[\"When trying to push a docker image to Google Container Registry, why do I get an error saying I don\\'t have the needed permissions?\", \"What steps can I take if I get a message that says \\'unauthorized: You don\\'t have the needed permissions to perform this operation, and you may have invalid credentials.\\'?\", \"Why do I need to install the gcloud SDK first before being able to use the gcloud command?\", \"What does the \\'gcloud auth configure-docker\\' command do?\", \"Can going through the installation process for the gcloud SDK help me resolve issues with accessing the Google Container Registry?\"]',\n",
       " 'c5d6a804': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"What happens when I try to install tflite in a pipenv environment?!\", \"Why am I getting an error message saying that tflite_runtime couldn\\'t find a version that satisfies the requirement?\", \"How can I make tflite work with python 3.10?\", \"Are there any available versions of tflite that can run on python 3.9?\", \"Is there a way to install tflite_runtime without using the version mentioned in the lecture?\"]',\n",
       " '8c7f089f': '[\\n\"What if I encounter an error when running ImageDataGenerator.flow_from_dataframe?\", \\n\"How do I fix the error: ImageDataGenerator name \\'scipy\\' is not defined?\", \\n\"I installed scipy, but I\\'m still getting an error when running ImageDataGenerator.flow_from_dataframe?\", \\n\"What should I do if I encounter an error when running ImageDataGenerator.flow_from_dataframe even after restarting my jupyter kernel?\", \\n\"What steps can I take to resolve the issue of ImageDataGenerator.name being undefined?\"',\n",
       " '739bcccf': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"Can someone provide a guide on how to pass BentoML content to Amazon Lambda?\", \"Is there a tutorial available on how to pass a BentoML docker container to Amazon Lambda?\", \"How do I utilize BentoML\\'s resources to integrate it with Amazon Lambda?\", \"I\\'m having trouble with BentoML and Amazon Lambda, is there a solution on how to combine them?\", \"Can you explain the process of using BentoML with Amazon Lambda in more detail?\"]',\n",
       " '4603e4e5': '[\\n\"How can I fix the UnidentifiedImageError: cannot identify image file when deploying my model locally on a test-image data?\",\\n\"Why do I get an UnidentifiedImageError when trying to load an image from a URL in the model part?\",\\n\"What additional step should I take to resolve the UnidentifiedImageError: cannot identify image file issue in my model development?\",\\n\"Can I use the ?raw=true parameter with the image URL when loading images for my model testing?\",\\n\"What is the correct way to modify the URL when encountering UnidentifiedImageError: cannot identify image file during local testing of a deployed model?\"',\n",
       " '0a7c328e': 'Here are the 5 questions the student might ask based on the provided FAQ record:\\n\\n[\"What do I do if I get a [pipenv.exceptions.ResolutionFailure] warning when trying to install dependencies?\"]\\n[\"If I have a mismatch in my sub-dependencies, how can I resolve the issue manually?\",]\\n[\"Why do I need to run pipenv lock to fix the dependency files?\",]\\n[\"Can I somehow prevent the mismatch in sub-dependencies from happening in the first place?\",]\\n[\"Are there any common scenarios where the ResolutionFailure warning occurs, and how can I avoid them?\"]',\n",
       " '77efd069': '[\\n\"Can you explain why the get_feature_names() function stopped working?\",\\n\"Why did they remove the get_feature_names() function from the library?\",\\n\"What is the correct way to get feature names from a dictVectorizer instance?\",\\n\"I get an error when using get_feature_names() with a CountVectorizer, what\\'s going on?\",\\n\"How do I upgrade my library version to fix the get_feature_names() issue?\"',\n",
       " 'cc60f7bc': '[\\n\"What if my data isn\\'t in the correct shape when trying to send a predict-test to the server and get a JSON response?\", \\n\"Why does the JSON response expect a value on line 1 column 1 even though I\\'ve provided all the necessary data?\", \\n\"How do I convert my data from a dictionary to a numpy array to fix the error?\", \\n\"What does it mean when the server receives data in json format but the model requires something else?\", \\n\"Can you explain why the server is expecting a dict instead of a numpy array for the model\\'s input?\"',\n",
       " 'aa13dd66': '[\\n\"Why can\\'t I deploy my docker image on Render due to SIGTERM? Is 0.5GB RAM enough?\",\\n\"Are there any other free cloud alternatives to deploy my docker image?\",\\n\"Can I get more than 0.5GB RAM for free on Render?\",\\n\"Are there any free GPU instances available for my machine learning tasks?\",\\n\"What kind of free stuff does AWS and GCP offer for a long time?\"',\n",
       " 'c41e479c': 'Here is the list of questions:\\n\\n[\"How do I convert a column with integer values to a string in pandas?\", \"What is the simplest way to convert string month values to numerical values in pandas?\", \"Can I use the datetime module in pandas to get the day of the year?\", \"Is there a way to convert two separate columns into a single column of day of the year in pandas?\", \"How do I generate a date object from separate columns of day and month in pandas?\"]',\n",
       " '2f28dcf1': '[\"How can I visualize the predictions for different classes in a neural network after training?\", \"Can I use any other type of plot apart from bar charts to display the predictions?\", \"Do I need to format my dataset in a specific way for this visualization?\", \"How do I ensure that the classes are correctly categorized in the chart?\", \"Is there a way to display the actual class labels in the chart instead of just numbers?\"]',\n",
       " '7a69cccf': '[\"How do I convert dictionary values to a Dataframe table in Python?\", \"What is the simplest way to transform dictionary output into a pandas DataFrame?\", \"Can you show me the convention to transform dictionary to Dataframe format?\", \"Is there a way to convert dictionary into pandas DataFrame?\", \"Can I convert a dictionary into a DataFrame table for further analysis?\"]',\n",
       " '20174c95': 'Here is the list of questions:\\n\\n[\\n\"Can you explain the difference in layout between the image dataset for the competition and the one used in the dino vs dragon lesson?\",\\n\"Why did you write a script to generate the kitchenware dataset in a different layout from what was covered in the dino vs dragon lesson?\",\\n\"Is the script used to generate the kitchenware dataset included in the course materials?\",\\n\"How can I find the script to generate the kitchenware dataset?\",\\n\"Who is Martin Uribe and what is their relation to the kitchenware dataset generator?\"',\n",
       " 'f2cd48b6': '[\"How do I install Nvidia drivers for my system?\", \"Can I install the \\'tensorflow-gpu\\' package in Anaconda?\", \"Is it necessary to install the Tensorflow way for installing TensorFlow on WSL/Linux?\", \"Do I need to install CUDA toolkit and cuDNN separately for TensorFlow?\", \"What does it mean by \\'Learning in public links\\' in the FAQ record and how should I link my social media posts with the assignment?\"]',\n",
       " '59b4324f': '[\"What happens when I multiply two matrices in the wrong order?\", \"Why do I get different results when I swap the order of matrix multiplication?\", \"What could be the consequence of not considering the order of matrix multiplication?\", \"How do I ensure I\\'m getting the correct result when performing matrix multiplication?\", \"What key aspect of matrix multiplication can affect the result if not taken into account?\"]',\n",
       " 'e1dc1ed9': '[\\n\"What are the instructions for installing the environment on a Mac?\", \\n\"Why didn\\'t any of the videos provide instructions for installing the environment on a Mac?\", \\n\"Can you provide more information about installing the environment for Mac computers?\", \\n\"How can we install the environment on a Mac with an M1 chip?\", \\n\"Are there any resources that explain how to set up the environment on a Mac?\"\\n]',\n",
       " 'fc60bf3b': '[\\n\"In cases where the form is no longer accepting submissions, can I still submit my assignment for evaluation?\", \\n\"If I submit my assignment late, is it possible that I\\'ll still receive credit?\", \\n\"Can I still submit my assignment if the submission deadline has passed?\", \\n\"If I miss the submission deadline, will my assignment be evaluated even if I wait until the form is open again?\", \\n\"If my submission is late, will I still receive a score and feedback?\"\\n]',\n",
       " '1e60e888': '[\"How do I make sure my GitHub repository link is accessible to the people who correct my homework?\", \"What are the steps to install Conda environment on my local machine?\", \"What Integrated Development Environment (IDE) is recommended for machine learning?\", \"Can my GitHub repository be private if I share the link?\", \"How do I set up and use the Conda environment for this course?\"]',\n",
       " '44552c2e': '[\"What is the correct syntax to install wget in Google Colab?\", \"How do I use wget to download data in Google Colab?\", \"Can I download files from Google Drive using wget in Google Colab?\", \"What is the purpose of the -P option when using wget with Google Colab?\", \"Can I use wget to download files from URLs other than Google Drive in Google Colab?\"]',\n",
       " '7116b3be': '[\\n\"What are the features in scikit-learn?\",\\n\"How should I format a 1D array to make it acceptable by scikit-learn?\",\\n\"What are some common mistakes to avoid when working with datasets?\",\\n\"How can I use Pandas to filter a DataFrame?\",\\n\"What is the purpose of reshaping a 1D array in Python?\"\\n]',\n",
       " '5d4d206e': '[\"What are the steps to bypass the error when plotting using Matplotlib to check if median has a tail?\", \"How can I resolve the FutureWarning when using is_categorical_dtype in Matplotlib?\", \"Why is is_categorical_dtype deprecated and what is the alternative?\", \"What type of data can be used with the alternative approach mentioned in the answer to this question\", \"How will the removal of is_categorical_dtype in future versions affect my code?\"]',\n",
       " '387093cc': '[\\n\"What steps should I take when trying to run my docker file in Windows as opposed to developing in WSL/Linux, as it\\'s not working?\",\\n\"I got an error saying that Python wasn\\'t found on my system when trying to rerun my docker file, what\\'s the solution?\",\\n\"Is it possible to install specific versions of Python when using pipenv or asdf?\",\\n\"What has to be done to the PATH in order to resolve the error I\\'m getting when running my docker file?\",\\n\"Does restarting the system after updating the PATH solve the issue with running my docker file again?\"\\n]',\n",
       " 'd12a2657': '[\\n\"How do I deploy my project to DigitalOcean App Cloud?\",\\n\"What is the cost of deploying my project to DigitalOcean?\",\\n\"Do I need to keep the container running until the end of the project evaluation?\",\\n\"How do I specify the Dockerfile path in DigitalOcean App Cloud?\",\\n\"What additional steps do I need to take with my model files when deploying to DigitalOcean?\"',\n",
       " 'eb7a57a6': 'Here is the output in parsable JSON format:\\n\\n[\\n\"Is it really best to only train my model on the most important features I\\'ve identified using feature importance?\",\\n\"How can I decide which features to include in my model after doing feature importance?\",\\n\"What happens if I have a few features that are highly correlated with each other after doing feature importance?\",\\n\"Are there any feature selection algorithms that I should be aware of in this course?\",\\n\"Can I use L1 regularization (Lasso) to automatically select features with little predictive value?\"',\n",
       " 'd6f0c6ea': '[\\n\"How can I efficiently analyze large datasets with limited memory\",\\n\"What are some techniques I can use to work with datasets that don\\'t fit in memory?\",\\n\"Can I optimize the data types in Pandas to reduce memory usage\",\\n\"Is there a way to sample large datasets for exploratory analysis\",\\n\"How can I utilize Dask\\'s parallel processing capabilities for large datasets\"\\n]',\n",
       " '9f261648': 'Here is the list of 5 questions based on the FAQ record:\\n\\n[ \"Can I do the course in other languages like R or Scala?\",\\n\"Are some homeworks designed to use specific Python library versions?\",\\n\"Why may answers not match in MCQ options if using different languages?\",\\n\"Will my peer-reviewers know the other languages I\\'m using for my work?\",\\n\"Can I still use languages other than Python for my own learning, but not for submissions?\" ]',\n",
       " 'aa7ff0f7': '[\"What are the rules regarding the use of libraries like fast.ai or huggingface in the capstone and competition?\", \"Are online resources considered \\'too much help\\' for the capstone and competition?\", \"Are libraries like fast.ai or huggingface banned in the capstone and competition?\", \"Can we use libraries like fast.ai or huggingface to help with the capstone and competition?\", \"Is using libraries like fast.ai or huggingface a definite \\'no\\' for the capstone and competition?\"]',\n",
       " '387bdc5f': '[\\n\"Why do I need to build and test my Flask image before building and testing my TensorFlow Serving image?\",\\n\"Can I use any version of TensorFlow with TensorFlow Serving?\",\\n\"How do I ensure that the version of TensorFlow I\\'m using is compatible with TensorFlow Serving?\",\\n\"Can I fix the incompatibility issue by reinstalling TensorFlow Serving?\",\\n\"What are some common reasons why TensorFlow Serving images cannot be tested successfully?\"',\n",
       " 'c6a22665': '[\\n\"How can I add the Machine Learning Zoomcamp experience to my LinkedIn profile?\", \\n\"What is the recommended format for listing the Machine Learning Zoomcamp experience in my LinkedIn profile?\", \\n\"Can I consider the Machine Learning Zoomcamp experience as an official \\'job\\' or \\'internship\\' experience on my LinkedIn profile?\", \\n\"How can I incorporate the Machine Learning Zoomcamp experience into different sections of my LinkedIn profile?\", \\n\"Can I include my project link in my CV and LinkedIn profile as a showcase of my progress?\"',\n",
       " '0560e827': 'Here is the list of questions:\\n\\n[\\n\"How can I successfully set up my local environment for working on projects during the course?\", \\n\"What are some tips for coding in a jupyter notebook during the course?\", \\n\"What format should I use for my code submissions?\", \\n\"How do I get help with a specific technical issue with Jupyter notebook?\", \\n\"What are the most important software tools that I should have in order to complete the course?\"',\n",
       " '59812e77': '[\"Can you tell me how long the course is expected to take, or how long each module is expected to last?\", \"Is there a set deadline for each module, or is there flexibility?\", \"What kind of assignment do we have to do in each module?\", \"How much time do we have to spend on the capstone project?\", \"Is there a review process for the capstone project?\"]',\n",
       " 'dce0bb09': '[\"What\\'s the main difference between the 2023 and 2022 course offering?\", \"Are all the modules re-recorded in the 2023 course?\", \"Will any of the course modules have updated content in 2023?\", \"Are the homework assignments for the 2023 course new and changed?\", \"Will I still be able to access the same course content as students in 2022?\"]',\n",
       " '4920d4e9': '[\\n\"Will the 2024 Cohort include students who didn\\'t apply to the 2023 Cohort, or only those who previously applied?\",\\n\"When will the 2024 Cohort application deadline be?\",\\n\"What are the requirements for being accepted into the 2024 Cohort?\",\\n\"Can I complete the 2024 Cohort during summer break, or is it only for students taking full-time classes?\",\\n\"Will the learning management system for the 2024 Cohort be the same as the 2023 Cohort\\'s?\"\\n]',\n",
       " '0f1d2765': '[\"What if my answer is similar but not exactly the same as the multiple-choice options?\", \"Can I share my answer with others in the course slack channel?\", \"Can I use the same answer for multiple questions?\", \"Are the answers to the questions in the course publicly available?\", \"Can I discuss the questions with another student before submitting my answer?\"]',\n",
       " '4eef2f81': '[\"What are some potential datasets for the final project that we can use?\", \"Can we pick any dataset we want from Kaggle?\", \"Do we have to use the datasets provided by the university?\", \"Is it possible to work on a topic that\\'s not related to machine learning?\", \"Can we use a dataset from Google or AWS instead of another platform?\"]',\n",
       " '7f93c032': '[\\n\"Can I still graduate if I didn\\'t complete my homework for a whole week?\",\\n\"How does not finishing homework affect my chances of getting on the top 100 leaderboard?\",\\n\"What if I missed only one homework assignment and still finish the capstone project with good results?\",\\n\"Does not completing homework have any impact on my overall progress in the course?\",\\n\"Why do I need to finish homework if it\\'s not required to graduate?\"\\n]',\n",
       " 'ee6f7c89': '[\"Is the final project necessarily required to be deployed to the cloud?\", \"What does it mean to get cloud points for the local deployment?\", \"Can I use Kubernetes even if I deploy my project locally?\", \"Do I need to mimic an AWS setup for the project?\", \"Is it possible to deploy the project locally without using Kubernetes?\"]',\n",
       " 'b63b12e0': 'Here is the list of questions:\\n\\n[\\n\"How do I port-forward without using a similar IDE to Visual Studio?\",\\n\"Can I automate port-forwarding for Jupyter Notebook if I\\'m not using VSCode?\",\\n\"How do I set up port-forwarding for Jupyter Notebook without using Visual Studio?\",\\n\"What is the command to launch Jupyter Notebook when port-forwarding?\",\\n\"Can I add a line of code to the \\'~/.ssh/config file for Jupyter Notebook port-forwarding?\\'\"]',\n",
       " '892c22c1': 'Here is the list of questions in JSON format:\\n\\n[\"How do I open Jupyter in VSCode?\", \"Is the Jupyter extension still the recommended method for opening notebooks?\", \"Why do I need to install the Jupyter extension?\", \"What if I don\\'t want to use the Jupyter extension?\", \"Is there a better way to open Jupyter notebooks in VSCode?\"]',\n",
       " '13d38e8d': 'Here are the 5 questions based on the FAQ record:\\n\\n[\"How do I configure Github to work from the remote VM?\", \"Can I set up a Github repository for Homeworks through the remote VM?\", \"How do I set up Github on the AWS instance?\", \"What are the steps to set up keys on the AWS instance for Github?\", \"Can I push my code to a Github repository after setting it up?\"]',\n",
       " '7d64e9e0': '[\"How do I run Jupyter notebook in AWS?\", \"Why am I unable to access Jupyter notebook from my desktop?\", \"What are the possible reasons for not being able to access Jupyter notebook from my desktop?\", \"How do I set up Jupyter notebook in AWS manually?\", \"What changes do I need to make to the jupyter_notebook_config.py file?\"]',\n",
       " '645f0a55': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"How do I set up WSL on my windows machine?\", \"What are the steps to download and install Anaconda?\", \"How do I install VSCODE on WSL?\", \"What are the commands to install Jupyter?\", \"How do I install all the necessary softwares on WSL?\"]',\n",
       " '7297b7fc': 'Here is the list of questions based on the FAQ record:\\n\\n[\"How do I use .gitignore in my local repository?\", \"How do I ignore all files in a specific folder using .gitignore?\", \"What is the easiest way to create a .gitignore file?\", \"How do I ignore a specific file type using .gitignore?\", \"Can you provide more information on how to write .gitignore rules?\"]',\n",
       " '68154f64': '[\\n\"Can I please clarify what I need to check when stopping an EC2 instance to ensure it\\'s actually stopped?\",\\n\"Do other charges incur even when an EC2 instance is stopped?\",\\n\"How can I avoid incurring unwanted charges when using an EC2 instance?\",\\n\"What does it mean if an EC2 instance is \\'stopped\\' but still has ongoing costs?\",\\n\"Can you suggest ways to set up billing alerts to ensure I don\\'t go over budget?\"',\n",
       " 'dc7b6f51': '[\"How can I get an invitation code for IBM Cloud?\", \"What are the main differences between IBM Cloud and AWS?\", \"Is IBM Cloud a viable alternative to AWS?\", \"Can you recommend a resource to learn more about IBM Cloud?\", \"What are the key characteristics that distinguish IBM Cloud from AWS?\"]',\n",
       " 'b25c6ca3': 'Here is the list of questions in JSON format:\\n\\n[\"How can I prevent a significant cost increase when keeping an AWS instance running during the course?\", \"What happens to my public IP address when I restart my instance?\", \"Can I track my AWS costs automatically and receive notifications if a certain budget is exceeded?\", \"How can I estimate the cost of using an AWS instance?\", \"Is the monthly cost of $12 really an affordable amount, considering the instance is being used only 5 hours a day?\"]',\n",
       " '9f69ca26': '[\"How do we handle tasks that are not included in the AWS free tier?\", \"Can we use Amazon Kinesis for certain tasks in the course?\", \"How does the course approach scenarios where AWS resources are not available?\", \"Will we need to use alternative solutions alongside AWS for some practical exercises?\", \"Can we simulate some AWS services locally when needed?\"]',\n",
       " '0f1ddc9e': '{\\n\"question1\": \"Why am I getting an error when trying to open an AWS EC2 instance in a browser?\",\\n\"question2\": \"Can I access an EC2 instance from my browser and display its contents?\",\\n\"question3\": \"What should I do if I\\'m trying to connect to an EC2 instance but its IP address can\\'t be reached?\",\\n\"question4\": \"How do I connect to an EC2 instance from my local machine or a remote server?\",\\n\"question5\": \"What is the purpose of downloading and using a key with an EC2 instance?\"',\n",
       " '01f61154': '[\"What causes the \\'unprotected private key file\\' error when using SSH?\", \"Why do I need to change the file permissions of my private key file?\", \"How do I actually change the file permissions of my private key file?\", \"Is changing the file permissions of my private key file the only solution to the \\'unprotected private key file\\' error?\", \"Can I use a different solution than the one mentioned on the 99robots.com page to fix this error?\"]',\n",
       " 'd43c32ba': 'Here is the output in JSON format:\\n\\n[\"Why does my SSH connection to AWS EC2 instance drop continuously?\",\\n\"What causes my SSH connection to EC2 instance to drop without being able to reconnect?\",\\n\"Why do I get disconnected from my EC2 instance when running code, such as \\'import mlflow\\'?\",\\n\"How can I troubleshoot and resolve my AWS EC2 instance\\'s SSH connection errors?\",\\n\"What can I do if my AWS EC2 instance\\'s OS runs out of memory and kills critical processes?\"]',\n",
       " 'a044d267': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"How can I update the IP address of my EC2 instance every time I restart it?\", \"Why does my EC2 instance get different IP addresses every time I restart it?\", \"Do I have to update the config file manually every time my EC2 instance restarts?\", \"Is there a way to automate the IP address update process instead of manually updating the config file?\", \"How can I use a script to update the IP address of my EC2 instance?\"]',\n",
       " 'abf8ccdc': 'Here are 5 questions based on the provided FAQ record:\\n\\n[\"What can I do if VS Code crashes when I\\'m trying to connect to Jupyter?\",\\n\"How can I improve the performance of my instance when connecting to Jupyter?\",\\n\"Can I monitor my instance\\'s compute capabilities?\",\\n\"What are some requirements for running Jupyter Notebooks successfully?\",\\n\"How can I troubleshoot issues with my Jupyter Notebooks?\"]',\n",
       " '26918af3': 'Here is the list of questions:\\n\\n[\"What happens when I encounter an error that LinearRegression is expecting 525 features as input but my data set has 526 features?\",\"How do I resolve the issue when my Linear Regression Model doesn\\'t work on the validation dataset?\",\"Why does my LinearRegression model work on the training dataset but not on the validation dataset after using DictVectorizer?\",\"Can I use the same fitting process for both the training and validation datasets when using DictVectorizer?\",\"How do I ensure that the DictVectorizer treats both the training and validation datasets correctly?\"]',\n",
       " 'a5234ac0': '[\"What are some common reasons why some dependencies might be missing?\", \"How do I install the missing dependencies listed in the answer?\", \"I\\'m getting an error using pandas.read_parquet(), what could be the solution?\", \"Can I install dependencies using Conda if I\\'m not using pip?\", \"How do I ensure all necessary dependencies are installed before running the course?\"]',\n",
       " 'af22c52a': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"What can I do if my RMSE value doesn\\'t appear in the available options when evaluating the model?\", \"If I\\'m filtering outliers using a specific technique on the train data, why do I need to do the same on the February data when evaluating the model?\", \"How can I suppress warnings about deprecated modules in Python?\", \"Should I filter null values along with outliers when calculating the RMSE value?\", \"How can I round my RMSE value to display only 2 decimal points?\"]',\n",
       " '2aaac94c': '[\\n\"How do I replace a distplot with a histplot in the context of the course?\",\\n\"Can I use sns.histplot instead of sns.distplot in my data visualization assignment?\",\\n\"How do I modify the appearance of a histplot to get a similar result to a distplot?\",\\n\"What are some key arguments I can use with sns.histplot to get a more detailed histogram?\",\\n\"What\\'s the difference between using stat=\\'density\\' and stat=\\'count\\' when creating a histplot?\"\\n]',\n",
       " '9d15c9e9': '[\\n  \"What is the correct replacement for \\'PULocationID\\' or \\'DOLocationID\\' if I encounter a KeyError?\",\\n  \"Why do I get a KeyError when specifying \\'PULocationID\\' or \\'DOLocationID\\' in the code?\",\\n  \"Can you explain why replacing \\'PULocationID\\' or \\'DOLocationID\\' with a lower case \\'l\\' resolves the KeyError?\",\\n  \"How do I resolve the KeyError by modifying the references to \\'PULocationID\\' or \\'DOLocationID\\'?\",\\n  \"What is the workaround to fix the KeyError issue with \\'PULocationID\\' or \\'DOLocationID\\'?\\'\"',\n",
       " '79b88d0b': 'Here is the list of questions:\\n\\n[\"How do I solve the problem of reading large parquet files?\", \"What are some workarounds that may not be successful while reading large parquet files?\", \"What is the error message I may get while reading large parquet files?\", \"How did someone solve a similar problem with Jupyter?\", \"Can you recommend an alternative library for reading large parquet files?\"]',\n",
       " '45485322': '[\"What is the best approach to handling slow performance in distplot?\", \"How can I remove trips with unusual duration from a dataset before plotting?\", \"Is there a way to identify trips with unusual duration?\", \"Can I still use distplot if my data contains outliers and it\\'s not efficient?\", \"Are there any specific steps to apply before implementing the fix for slow performance in distplot?\"]',\n",
       " 'd5eab395': '[\\n\"Why does hot encoding the validation set with a previously fitted OneHotEncoder on the training set lead to a high RMSE on the test set?\",\\n\"Why do we need to handle unknown categories when hot encoding?\",\\n\"Can you explain how DictVectorizer treats categorical features?\",\\n\"Why do categorical features need to be put into binary columns during encoding?\",\\n\"What happens to unknown categories during the encoding process?\"',\n",
       " '282957fb': '[\"Why should I use OneHotEncoder instead of DictVectorizer?\", \"What are the differences between OneHotEncoder and pd.get_dummies?\", \"How do OneHotEncoder and DictVectorizer handle missing data?\", \"In what situations should I use DictVectorizer over OneHotEncoder?\", \"Can you explain the differences in column order between OneHotEncoder and DictVectorizer?\"]',\n",
       " '39ad14fd': '[\"Why did we not use OneHotEncoder from pandas library for one-hot encoding?\", \"Why not use OneHotEncoder from scikit-learn library to do one-hot encoding?\", \"What\\'s the benefit of using DictVectorizer instead of OneHotEncoder?\", \"Is there any specific reason not to use get_dummies from pandas library for one-hot encoding?\", \"Can one-hot coding also be done using NumPy library?\"]',\n",
       " 'e34df2a5': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"How do we verify if we correctly removed outliers from our data?\", \"What statistics can we use to check the data distribution after clipping the outliers?\", \"Can we use pandas alone to identify outliers in our dataset?\", \"In which step of the process do we use the pandas function describe()?\", \"What alternative methods can we use to check if outliers were removed successfully?\"]',\n",
       " 'c91b6b57': '[\\n\"How do I convert the values in PUlocationID and DOlocationID to strings when using pd.get_dummies and DictVectorizer for one-hot encoding?\",\\n\"What happens to NaN values in PUlocationID and DOlocationID when I convert them to strings\",\\n\"Why don\\'t I need to specifically use \\'-1\\' to represent NaN values in one-hot encoding\",\\n\"Will the RMSE change if I use a different string representation for NaN values instead of \\'nan\\'\",\\n\"Can I still get decent results if I use categorical values instead of numerical values for PUlocationID and DOlocationID\"',\n",
       " '4aa8eafc': '[\\n\"My LinearRegression RSME is different from the answer, even though it\\'s very close; is this normal?\" , \\n\"How do I ensure my LinearRegression always outputs the same results?\", \\n\"How can I check if I\\'m treating outliers properly in both my train and validation sets?\", \\n\"What are the common mistakes I should check when doing one hot encoding?\", \\n\"Why are my one hot encoded feature matrices showing multiple features?\"',\n",
       " 'a9daaab0': '[\"What can I do when I\\'m facing an extremely low RMSE score?\",\\n\"Can you explain why my model\\'s RMSE score is as low as my model\\'s target variable?\",\\n\"What is the issue with passing the target variable as a parameter while fitting the model?\",\\n\"Why does my X_train contain any part of my y_train?\",\\n\"What\\'s the difference between X_train and y_train in machine learning?\"]',\n",
       " '931f9626': 'Here are the 5 questions this student might ask based on the FAQ record:\\n\\n[\"How do I enable auto-completion in Jupyter notebook?\", \"What is the issue with tab not working for autocompletion in Jupyter notebook?\", \"Why do I need to upgrade Jedi to a specific version 0.17.2 when enabling auto-completion?\", \"Can I install Jedi via other means other than pip?\", \"Are there any specific installation instructions for JedI?\"]',\n",
       " '782e1723': 'Here are the questions:\\n\\n[\"How do I download the NY Taxis dataset without getting a 403 Forbidden error?\",\\n\"Why do I get an error when trying to download the dataset using wget?\",\\n\"Can I download the dataset directly from the provided link or do I need to modify it?\",\\n\"What is the cause of the \\'OSError: Could not open parquet input source\\' error?\",\\n\"How do I ensure that the link I use is not changed or moved by NYC in the future?\"]',\n",
       " '4e08c86a': '[\\n\"Can I use PyCharm with a conda environment on a remote server, and how can I set it up?\", \\n\"How do I make PyCharm recognize the conda environment on a remote server?\", \\n\"I\\'m having trouble activating my conda environment on the remote server, can you explain the solution?\", \\n\"What is the problem with using conda environments in PyCharm when working remotely?\", \\n\"Can you provide steps on how to add a conda environment to PyCharm from a remote server?\" \\n]',\n",
       " '34bcad27': 'Here are the 5 questions:\\n\\n[\"What happens when I run out of memory in the course?\", \"Why did the output of DictVectorizer take up too much memory?\", \"How can I set the parameter in the scikit-learn website example?\", \"What is the effect of setting \\'sparse\\' as False in DictVectorizer?\", \"Why doesn\\'t my 16 GB machine fit the linear regression model?\"]',\n",
       " '96144e66': 'Here are the 5 questions:\\n\\n[\"How can I activate Anaconda env if installing Anaconda didn\\'t modify my .bashrc profile?\", \"What is the problem when installing Anaconda doesn\\'t modify the .bashrc profile?\", \"What solution is there for the .bashrc profile not being modified after installing Anaconda?\", \"How do I initiate conda again to add entries for Anaconda in the .bashrc file?\", \"How do I reload the .bashrc file after editing it?\"]',\n",
       " '840f739d': '[\\n\"Why is the feature size different for training and validation sets?\",\\n\"Why do we need to use fit_transform over transform for the dictionary vectorizer?\",\\n\"How do I know which parts of the code I need to run in HW1?\",\\n\"What is the purpose of the premade dictionary vectorizer in HW1?\",\\n\"Why do I need to execute the fit pipeline on the model for the dictionary vectorizer?\"\\n]',\n",
       " 'bf006ff9': 'Here is the list of questions based on the FAQ record:\\n\\n[\"How do I get access to my machine again when I\\'ve removed my public key on an AWS machine?\",\\n\"Why do I get a \\'Permission denied (publickey)\\' error when I\\'ve removed my public key on an AWS machine?\",\\n\"Is there a guide on how to fix permission denied errors on Linux instances in AWS?\",\\n\"How do I create a public key again on my instance using Session Manager on AWS?\",\\n\"Can I recover my old public key, and if so, what command do I need to run to do so?\"]',\n",
       " 'f178d4a0': 'Here is the list of questions:\\n\\n[\\n\"How can I reduce the absurdly high RMSE on my validation dataset?\",\\n\"Why is using the February dataset as a validation/test dataset a problem?\",\\n\"Why do I get an incorrect result when I convert a sparsematrix result to an ndarray?\",\\n\"What is causing the train dataset to be stripped of outliers in a similar manner to the validation/test dataset?\",\\n\"What can I do to ensure I\\'m not overfitting in my model?\"\\n]',\n",
       " 'b80401a2': 'Here is the list of questions in parsable JSON format:\\n\\n[\"What are the exact steps to resolve the issue of scipy library not importing correctly?\", \"How do I fix the problem of not being able to import the DictVectorizer from sklearn?\", \"Why I\\'m facing trouble in installing the scikit-learn library?\", \"What is the most effective way to troubleshoot the error of not being able to import sklearn?\", \"Can I get a detailed guide on how to resolve the sklearn import issue?\"]',\n",
       " '88002d35': '[\\n\"How can I troubleshoot an \\'Access Denied\\' issue at Localhost:5000, specifically when trying to access a page?\",\\n\"Why do I keep getting an \\'Access Denied\\' error at Localhost:5000?\",\\n\"I\\'m experiencing an \\'Access Denied\\' problem at Localhost:5000 - what could be causing this?\",\\n\"What are the common reasons behind the \\'Access Denied\\' error at Localhost:5000?\",\\n\"What can I do to resolve an \\'Access Denied\\' issue at Localhost:5000?\"',\n",
       " 'fe61aa5b': 'Here is the list of 5 questions based on the FAQ record:\\n\\n[\"What is running on the 5000 port and how do I stop it?\", \"How do I kill a process that is using port 5000?\", \"Is there a way to kill all the processes using port 5000?\", \"How do I change the default port used by mlflow?\", \"What command do I use to kill a process that is running using a python script?\"]',\n",
       " 'b9adeb39': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"Why am I getting the ValueError: could not convert string to float error?\", \"What is the correct format for the objective function in the hpo.py file?\", \"Why should we log the parameters in the with statement in the objective function?\", \"What is the purpose of the \\'with\\' statement in the objective function?\", \"How can we log the parameters of the search space dictionary in the correct format?\"]',\n",
       " 'ebc13686': 'Here are the 5 questions based on the FAQ record:\\n\\n[\"Why isn\\'t my experiment visible in the MLflow UI?\",\\n\"How do I ensure the mlflow UI is launched from the correct directory?\",\\n\"Why do I need to specify the correct tracking_uri when navigating to the mlflow UI?\",\\n\"Can I use an absolute path to mlflow.db instead of a relative path?\",\\n\"How can I launch the mlflow UI from within a notebook cell?\"]',\n",
       " '939f9c33': 'Here are the 5 questions based on the FAQ record:\\n\\n[\"What can cause a \\'Hash Mismatch Error\\' during mlflow\\'s installation process?\", \"What happens when npm gets installed through mlflow and gives an error?\", \"Why does the error occur when npm is installed separately?\", \"How can we solve the Hash Mismatch Error?\", \"Is this issue consistently simulatable when installing mlflow?\"]',\n",
       " 'b5c3e6af': '[\"How can I view my experiment tracking data if it\\'s not showing up in the MLFlow UI?\", \"What kind of connection do I need to have to delete an experiment from the MLFlow UI?\", \"How do I confirm if an experiment was deleted permanently from MLFlow UI?\", \"Can I use SQL to retrieve a specific experiment from the MLFlow database?\", \"How do I get started with using SQL in Jupyter Notebook to interact with the MLFlow database?\"]',\n",
       " '80554fc2': 'Here are the 5 questions this student might ask based on the FAQ record:\\n\\n[\"How do I ensure my own changes are not overwritten when updating the public Git repo?\", \"What is the best approach to managing experiment tracking in our course?\", \"Why am I seeing changes overwritten when I push to my own Git repository?\", \"How can I fork a repo on GitHub and then push changes to my own repository?\", \"What is the role of the \\'Fetch and Merge\\' feature in GitHub and how do I use it to update my own repository?\"]',\n",
       " '943df153': 'Here is the list of questions in JSON format:\\n\\n[\"What happens when the image size of 460x93139 pixels is too large?\", \"Why is the image size too large in direction?\", \"What causes the image size to be too large when running the xgboost module?\", \"How can I fix the image size issue when using xgboost version 1.6.1?\", \"How can I downgrade the xgboost package to a lower version?\"]',\n",
       " 'b8d3c55e': 'Here are the 5 questions the student might ask:\\n\\n[\"What causes the error message \\'MlflowClient object has no attribute \\'list_experiments\\'\\'?\", \"Why was the \\'list_experiments\\' method deprecated?\", \"What can I substitute for the \\'list_experiments\\' method?\", \"How can I resolve this issue if I\\'re still using an earlier version of mlflow?\", \"What are the safe options to track experiments in the recent versions of mlflow?\"]',\n",
       " '67bf60c6': 'Here are the 5 questions based on the FAQ record:\\n\\n[\\n\"Why isn\\'t MLflow\\'s autolog feature working\",\\n\"What are the necessary dependencies for MLflow autolog to function\",\\n\"Can autolog be used before the start of a run\",\\n\"Will the MLflow autolog feature work if I install the necessary dependencies after the start of a run\", \\n\"Is it possible to use autolog with frameworks other than MLflow\"]',\n",
       " '336f5e36': 'Here are the 5 questions that the student might ask based on the FAQ record:\\n\\n[\"When I try to open MLflow using the provided URL http://127.0.0.1:5000, nothing happens. How do I resolve this issue?\", \"How do I forward the port for MLflow if I\\'m running it on a remote VM like we did for Jupyter notebook in Module 1?\", \"What should I do if I\\'m running MLflow locally but the port 127.0.0.1:5000 doesn\\'t work and I get a blank page instead?\", \"Can you please provide a screenshot of adding the port in VS Code like you did in Module 1 for Jupyter notebook?\", \"How do I check if I\\'m running MLflow correctly and why does the URL http://127.0.0.1:5000 not work as expected?\"]',\n",
       " 'fd2b9972': '[\"What is the usual warning message that occurs when using \\'mlflow.xgboost.autolog()\\', which initially caused confusion during model signature failure?\", \"Why is it recommended to check the MLflow UI to verify tracking of the model, especially when filter tags are not included?\", \"In what situation did Warrie Warrie report seeing this warning message during \\'mlflow.xgboost.autolog()\\'?\", \"How did Bengsoon Chuah resolve the MLflow.xgboost Autolog Model Signature Failure issue that he was facing?\", \"What is the possible consequence if \\'tag\\' filters are included in the MLflow UI while checking model tracking?\"]',\n",
       " '75cd9b7a': '[\\n\"Can I still set a deleted experiment as the active experiment in mlflow\",\\n\"What are the ways to resolve a MlflowException when trying to set a deleted experiment\",\\n\"Why am I getting a MlflowException when trying to set a deleted experiment as active\",\\n\"How do I permanently delete an experiment in mlflow\",\\n\"What are some alternative solutions to permanently delete a deleted experiment\"',\n",
       " '51c99586': '[\"How do I solve the issue of not having enough disk space to install the requirements for the experiment tracking module?\", \"What are the possible ways to increase the base EBS volume for installing the requirements?\", \"Can I add an external disk to my instance to install the requirements?\", \"How do I confirm that the additional disk is successfully mounted after adding it to my vm?\", \"Is there an alternative to Anaconda to install conda installation on an external disk?\"]',\n",
       " '089c8c18': '[\"What are the common reasons for Parameters Mismatch in Homework Q3?\", \"How can I troubleshoot when my parameters don\\'t match?\", \"Can old versions of sklearn cause mismatched parameters?\", \"Why was min_impurity_split for randomforrestRegressor deprecated in the latest sklearn version?\", \"Is upgrading to the latest version of sklearn a solution for parameters mismatch?\"]',\n",
       " 'f4b82056': '[\\n\"Why is there an error when I\\'m trying to install the MLflow module from my requirements.txt document?\",\\n\"I get an error when I try to run mlflow from my terminal after installing the necessary libraries. What\\'s the solution?\",\\n\"Can you help me troubleshoot the issue with installing protobuf for MLflow?\",\\n\"I\\'m having trouble with protocol buffers when installing MLflow. How can I resolve the issue?\",\\n\"I\\'ve noticed that my protobuf module is not installed correctly, which is causing errors when I run MLflow. What\\'s the correct version to use?\"',\n",
       " 'dd2e7dc9': '[\"What does it mean to run mlflow ui or mlflow server command in the right directory for setting up Artifacts folders?\", \"How do I check my current directory in relation to setting up Artifacts folders?\", \"Can I still set up Artifacts folders outside of the module 2?\", \"Do I need to run custom commands or is it a specific mlflow command for setting up Artifacts folders?\", \"What if I encounter issues while setting up Artifacts folders?\"]',\n",
       " '3fcbd80e': '[\"How do I set up MLflow experiment tracker on GCP if I\\'m experiencing issues?\", \"Can I get a reliable resource on setting up MLflow for GCP experiment tracking?\", \"Are there any specific links I can visit to troubleshoot my MLflow setup on GCP?\", \"What are some reliable sources I can use to learn about setting up MLflow for experiment tracking on GCP?\", \"Can I find a comprehensive guide to setting up MLflow experiment tracker on GCP?\"]',\n",
       " '924fcf47': '[\"What are the recommended solutions for dealing with the Setuptools Replacing Distutils issue mentioned in the MLflow Autolog Warning?,\", \"How do I make sure I don\\'t encounter this problem in the future?,\", \"Is it necessary to perform downgrades of installed packages to resolve this issue?,\", \"What was the exact version downgrade you used to resolve the problem, and how do you know which versions are compatible?,\", \"Why did you choose to downgrade setuptools specifically, rather than trying other approaches?\"]',\n",
       " '58240887': 'Here are the 5 questions:\\n\\n[\"How do I view and sort my experiment runs in MLflow?\", \"Why can\\'t I sort my runs in MLflow?\", \"What is the difference between table view and list view in MLflow?\", \"Is there a way to sort my runs in MLflow when I\\'m in list view?\", \"What do I need to do to see my runs sorted in MLflow?\"]',\n",
       " '67d343f2': 'Here is the list of questions the student might ask based on the provided FAQ record in parsable JSON format:\\n\\n[\"What happens when I run $ mlflow ui on a remote server and try to open it in my local browser?\", \"Why do I get an exception and the page with mlflow ui isn\\'t loaded?\", \"Is there a way to fix the TypeError: send_file() unexpected keyword \\'max_age\\' issue?\", \"How can I remove the old version of Flask in my base conda env?\", \"What do I need to do to install the correct version of Flask in my new work environment?\"]',\n",
       " '6de95c2a': '[\"Why do I get a FileNotFoundError when I run mlflow ui on Windows?\", \"How do I fix the error \\'The system cannot find the file specified\\' when using mlflow ui on Windows?\", \"What is causing the FileNotFoundError in mlflow ui on Windows even though mlflow is installed successfully?\", \"Why do I need to add a path to the PATH when using mlflow ui on Windows?\", \"How do I resolve the error FileNotFoundError: [WinError 2] when running mlflow ui on Windows?\"]',\n",
       " '2ff28e5b': '[\"What does error \\'TypeError: unsupported operand type(s) for -: \\'str\\' and \\'int\\' mean in relation to running hpo.py?\", \"How does the data format in the --data_path argument get handled in python?\", \"Why do I get an error in my Experiment tracking for the homework?\", \"Can I get a detailed explanation of the full traceback in hpo.py?\", \"Why should I add type=int to the --max_evals argument in hpo.py??\"]',\n",
       " '29c6bbf1': '[\"What is causing the unsupported Scikit-Learn version warning when running mlflow.sklearn?\", \"Why do I need to upgrade/downgrade Scikit-Learn to a supported version when running mlflow.sklearn?\", \"What is the supported range of Scikit-Learn versions for mlflow.sklearn?\", \"How do I fix the unsupported Scikit-Learn version warning when running mlflow.sklearn?\", \"What if I encounter errors during autologging with an unsupported Scikit-Learn version?\"]',\n",
       " 'bd09df94': 'Here are 5 questions the student might ask based on the provided FAQ record:\\n\\n[\\n\"How do I get the experiments to show up when using the Mlflow CLI?\", \\n\"Why doesn\\'t the mlflow experiments list command return any experiments?\", \\n\"I\\'m having trouble listing my experiments using the Mlflow CLI, can you help?\", \\n\"When I use the mlflow experiments list command, I don\\'t see any of my experiments, why?\", \\n\"I\\'m trying to use the Mlflow CLI to list my experiments, but it\\'s not working, what\\'s causing this issue?\"',\n",
       " 'af887c59': '[\"How do I fix the issue where most mlflow cli commands can\\'t find the experiments that have been run with the tracking server?\", \"Why do I have to set the MLFLOW_TRACKING_URI environment variable?\", \"Can I view the experiments from the command line after setting the MLFLOW_TRACKING_URI environment variable?\", \"Why doesn\\'t the tracking URI seem to be automatically used by the mlflow gc command?\", \"How do I ensure the tracking URI is used by the mlflow gc command every time I run it?\"]',\n",
       " 'ee7c59ea': 'Here are the 5 questions:\\n\\n[\"How do I view the experiment tracking data in SQLite?\", \"Can I delete experiments manually in MLflow?\", \"How do I inspect the SQLite database provided by MLflow?\", \"Is the entity structure of the data being stored in MLflow useful for systematic archiving?\", \"Can I use SQL to query the data in MLflow for longer periods?\"]',\n",
       " 'a2531c75': '[\\n\"What does it mean to launch a tracking server locally for purposes of remote hosting a server?\",\\n\"How does running a mlflow server locally compare to running it on multiple laptops?\",\\n\"What would be the benefits of having multiple engineers connect to the same mlflow server?\",\\n\"Can mlflow be hosted on multiple servers simultaneously?\",\\n\"What are some scenarios where running mlflow remotely might be beneficial?\"',\n",
       " 'bc4b2320': '[\"What happens when you try to add a parameter in the scenario where max_depth is not recognized?\", \"Why do parameters not get recognized when you add them after the model registry?\", \"How can you solve the problem when you encounter an unrecognised parameter? \", \"What should you do before you try to log your model?\", \"How can you directly append parameters to the data.run.params dictionary?\"]',\n",
       " 'f69fb077': 'Here are 5 questions this student might ask based on the FAQ record:\\n\\n[\\n\"What happens when I add the `mlflow.log_params` and `Max_depth` is still not recognized?\",\\n\"Why does adding `mlflow.log_params` not update my `Max_depth` parameter immediately?\",\\n\"I\\'ve added `mlflow.log_params` to my `hpo.py` script, but why do I see multiple models with different parameters in my experiment tracking?\",\\n\"What should I do if I run into issues with appended models in my experiment tracking after using `mlflow.log_params`?\",\\n\"Why does using `mlflow.log_params` not create a new experiment run with the updated parameters?\"',\n",
       " 'e223524c': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\\n\"What does AttributeError mean in the context of the register_model.py script?\",\\n\"Why does the script fail and spit out the error message when I copy it into a Jupyter notebook?\",\\n\"What is the tb_frame and why is it responsible for the AttributeError?\",\\n\"What are click decorators and why are they causing the issue in the register_model.py script?\",\\n\"Can I still use Jupyter notebook with register_model.py after removing the click decorators?\"',\n",
       " '0f08bec7': 'Here is the list of questions:\\n\\n[\"Why do I get an error when running the preprocess_data.py file?\", \"How can I resolve the WandB API error?\", \"What is the issue with the wandb: ERROR api_key not configured (no-tty) error?\", \"How do I obtain my WandB API key?\", \"How do I add and run the WandB login cell in my notebook?\"]',\n",
       " '8b4b1685': '[\\n\"How do I ensure that autologging is enabled before constructing the dataset if MLflow.xgboost throws a failed to infer model signature error?\",\\n\"What if after enabling autologging, I still encounter the \\'failed to infer model signature\\' issue?\",\\n\"Why do I need to make sure my data is in format compatible with XGBoost when constructing the dataset?\",\\n\"Can you explain what \\'could not sample data to infer model signature\\' means in the MLflow.xgboost error message?\",\\n\"How do I construct my dataset correctly if MLflow.xgboost fails to infer the model signature?\"',\n",
       " 'ecfc5c07': 'Here are the 5 questions based on the FAQ record:\\n\\n[\"Why is wget not working in the Visual Studio notebook?\", \"How do I download data using the notebook provided by Visual Studio?\", \"Why isn\\'t pip recognized in the python virtual environment?\", \"How can I use python commands in the notebook?\", \"Why do I need to use python -m for commands like wget?\"]',\n",
       " 'a1b68c52': 'Here are the 5 questions in parsable JSON format:\\n\\n[\"How can I open or run a GitHub notebook (.ipynb) directly in Google Colab?\", \"How can I navigate the Wandb UI?\", \"What are some issues you\\'ve faced with tracking your experiments in terms of versions?\", \"Why is it difficult for me to work with the Wandb UI without the official documentation?\", \"Why doesn\\'t the notebook open directly from \\'github.com\\' in Google Colab?\"]',\n",
       " '483e7d61': 'Here is the list of 5 questions:\\n\\n[\"Why do we use Jan/Feb/March for Train/Test/Validation Purposes?\", \"What does it mean when I get an error while trying to run the mlflow server on AWS CLI with S3 bucket and POSTGRES database?\", \"How can I avoid data leakage and overfitting in time series predictions?\", \"Why do we report model metrics to leadership, regulators, auditors, and analyze target drift for our models?\", \"What is target drift?\"]',\n",
       " 'e5c33f50': '[\"What are the key elements to consider when designing an orchestration workflow?\", \"How can I troubleshoot issues with my orchestration configuration?\", \"What are the best practices for managing multiple tasks and dependencies in an orchestration workflow?\", \"Which of the following scenarios would require a more complex orchestration design?\", \"Can I reuse orchestration logic across multiple workflows?\"]',\n",
       " 'cbf13b19': '[\\n\"What is the recommended deployment method for Prefect flows?\", \\n\"How do I troubleshoot Prefect deployment issues?\", \\n\"Is it viable to use Prefect with a headless browser deployment?\", \\n\"Can I use Prefect to deploy a set of tasks in a sequence?\", \\n\"What should I do if I encounter an error while deploying Prefect?\"',\n",
       " '39861d6e': '[\"What is the required AWS CLI version to use ECR with Docker?\", \"Why do I get an \\'Invalid choice\\' error when running \\'aws ecr get-login\\'?\", \"How can I fix the \\'Invalid choice\\' error in \\'aws ecr get-login\\'?\", \"Do I need to use a specific Docker command when logging in to ECR?\", \"Can you provide more information about logging in to ECR with Docker?\"]',\n",
       " '3dac15ff': 'Here are the 5 questions based on the FAQ record:\\n\\n[\"How do I write multiline commands in Windows Powershell?\", \"How do I escape quotation marks in Windows Powershell?\", \"What is the purpose of using `$env:` to create environment variables?\", \"Can you provide an example of a multiline string in Windows Powershell?\", \"How do I use environment variables in a Windows Powershell command?\"]',\n",
       " '32686722': 'Here is the list of questions:\\n\\n[\"What happens if I get pipenv failures for the pipenv install command?\", \"Why do I get Python-related errors for pipenv?\", \"Can I use the system Python for pipenv installation?\", \"How do I install pipenv with a non-system Python?\", \"What should I do if pipenv installation with my previous setup doesn\\'t work?\"]',\n",
       " '22521751': 'Here is the list of questions:\\n\\n[\"What could be the cause of the module not being available, considering it\\'s not possible to connect to a HTTPS URL?\", \"How do I know if the SSL module is correctly configured?\", \"What should I do if upgrading pipenv in my current environment resolves the problem?\", \"Why might there be trouble connecting to a HTTPS URL if not connected to a specific module?\", \"Is it possible to troubleshoot further if the issue persists after upgrading pipenv?\"]',\n",
       " '81ad4784': '{\"questions\": [\\n\"Why do I get a \\'ModuleNotFoundError: No module named \\'pip._vendor.six\\' error when installing scikit-learn?\",\\n\"Are there any specific steps I should take when installing scikit-learn via pipenv?\",\\n\"Can you help me resolve the issue where Python can\\'t find the \\'pip._vendor.six\\' module?\",\\n\"What has to be installed before I can install scikit-learn with pipenv?\",\\n\"Should I clear some environment variables or remove a package when I encounter issues with scikit-learn installation?\"]}',\n",
       " '29b5651e': '[\\n\"How do I install Jupyter and its kernel using Pipenv?\", \\n\"Can I use Pipenv with Jupyter notebooks in VS Code?\", \\n\"What is the purpose of running `python -m ipykernel install --user --name=my-virtualenv-name` in the Pipenv shell?\", \\n\"How do I register the kernel with Jupyter after installing it with Pipenv?\", \\n\"What happens when I run the command to install the kernel in the Pipenv shell with Jupyter notebooks in VS Code?\"',\n",
       " 'ca79bbe8': '[\\n\"Why doesn\\'t my starter notebook give any output when I run it in a Pipenv environment with Python 3.10 and scikit-learn 1.2.2 installed?\",\\n\"Why is my Jupyter notebook not producing any output when I run it in Pipenv, and how can I fix this issue with Tornado?\",\\n\"Why is the Tornado version used by Jupyter causing problems when I run my notebook in Pipenv?\",\\n\"How can I resolve the issue of no output in my Jupyter notebook when I use Pipenv with Python 3.10 and scikit-learn 1.2.2\",\\n\"Why does Jupyter use Tornado under the hood, and what are the implications for my notebook\\'s output in Pipenv?\"\\n]',\n",
       " '668f1ad9': '[\\n\"What can cause the \\'Invalid base64\\' error when running `aws kinesis put-record`?\",\\n\"What happens if I\\'m using AWS CLI version 2 and get the \\'Invalid base64\\' error?\",\\n\"How do I fix the \\'Invalid base64\\' error for `aws kinesis put-record` on my local machine?\",\\n\"What does the \\'--cli-binary-format raw-in-base64-out\\' argument do for `aws kinesis put-record`?\",\\n\"Why is the instructor using AWS CLI version 1 in lecture 4.4 when version 2 is available?\"',\n",
       " '7a6f23eb': '[\\n\"What happens when I run the starter.ipynb file in the Q1 homework?\", \\n\"What causes an error with index 311297 when loading a parquet file?\", \\n\"How do I resolve the issue with the error index 311297?\", \\n\"What is updated in pandas dependencies to solve the deployment error?\", \\n\"What is the connection between the error and the starter.ipynb file in Q1 homework?\"\\n]',\n",
       " '232e5557': '[\\n  \"What do I do when Pipfile.lock is not created along with Pipfile?\",\\n  \"How can I force the creation of Pipfile.lock?\",\\n  \"What is the correct command to use if Pipfile.lock is not generated automatically?\",\\n  \"Can you explain how to create Pipfile.lock manually?\",\\n  \"What are the steps to take if Pipfile.lock is missing and Pipfile is present?\"\\n]',\n",
       " 'e44ec04a': '[\"How do I resolve a Permission Denied issue when using Pipenv for deployment due to the pythonfinder module?\", \"Why am I getting a permission denied error when using pipenv for module 4 deployment?\", \"Can you help me debug a pipenv permission denied issue related to the pythonfinder module?\", \"What are the possible causes of a permission denied error when using pipenv for deployment in module 4?\", \"How do I fix a pipenv permission denied error that\\'s linked to the pythonfinder module within module 4?\"]',\n",
       " '55fdb8b9': '[\\n\"What happens when passing arguments to a script via command line and converting it to a 4 digit number using f\\'{year:04d}\\'\",\\n\"Why do inputs from the command line read as string by the script\",\\n\"How can we convert all inputs from the command line to numeric/integer?\",\\n\"Is there a way to avoid this error when using the click library\",\\n\"How do we edit a decorator to fix this error when using the click library\"\\n]',\n",
       " 'bf9082a2': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\"How do I ensure I\\'m using the correct Docker image when deploying my application?\", \\n\"What is the recommended way to copy data from my local machine to the Docker image?\", \\n\"I noticed using absolute paths in the Docker image can cause issues. What\\'s an alternative?\", \\n\"In the Docker commands you provided, what does the \\'-t\\' and \\'--rm\\' flags do?\", \\n\"What is the significance of the \\'<mlops-learn>\\' name used in the Docker commands?\"]',\n",
       " 'e7906e44': 'Here are the 5 questions:\\n\\n[\"What happens if I define multiple services in the Dockerfile with CMD?\", \"How do I run multiple services in the same container without conflicts?\", \"Is there a better way to start multiple services in a Docker container?\", \"Do I need to expose all ports defined by the services in the Dockerfile?\", \"Can I just define both services in the Docker file with CMD and expect them to run?\"]',\n",
       " '76d8892e': '[\\n\"How do I resolve an InstallationError when trying to generate pipfile.lock? The error message mentions a command \\'python setup.py egg_info\\' failing with error code 1.\",\\n\"What happens when the command \\'python setup.py egg_info\\' fails with error code 1 during pipfile.lock generation?\",\\n\"I got an InstallationError while trying to generate pipfile.lock. The error message mentions pip9.exceptions.InstallationError. What should I do?\",\\n\"Why do I need to manually upgrade pipenv and wheel when trying to generate pipfile.lock?\",\\n\"Is there a specific command I can use to force and upgrade wheel and pipenv to resolve InstallationError during pipfile.lock generation?\"',\n",
       " 'c5c2c82a': '[\\n\"How do I connect my S3 bucket to MLFLOW?\",\\n\"What are access keys used for when connecting to AWS using boto3?\",\\n\"What happens if I don\\'t have access keys when trying to connect to my S3 bucket?\",\\n\"How can I ensure I have the right permissions to access my S3 bucket when using boto3?\",\\n\"Is it possible to make my S3 bucket publicly accessible so I don\\'t need access keys?\"\\n]',\n",
       " '82b6c143': '[\\n  \"What do I do when uploading to s3 fails with the message \\'An error occurred (InvalidAccessKeyId) when calling the PutObject operation: The AWS Access Key Id you provided does not exist in our records\\' even though the upload works using aws cli and boto3 in Jupyter notebook?\",\\n  \"How do I fix the issue where uploading to s3 fails with the \\'InvalidAccessKeyId\\' error despite using aws cli and boto3 in Jupyter notebook?\",\\n  \"Can you help me resolve the \\'InvalidAccessKeyId\\' error that occurs when calling the PutObject operation during an s3 upload, even though I can use aws cli and boto3 successfully in Jupyter notebook?\",\\n  \"Why do I get an \\'InvalidAccessKeyId\\' error when trying to upload to s3 with aws cli and boto3 in Jupyter notebook, while the upload works correctly using those tools?\",\\n  \"What should I do if I\\'m experiencing issues uploading to s3 with the error \\'An error occurred (InvalidAccessKeyId) when calling the PutObject operation\\', which works fine with aws cli and boto3 in Jupyter notebook?\"\\n]',\n",
       " '77d9a742': 'Here are 5 questions that the student might ask based on the FAQ record:\\n\\n[\"What happens when the problem is related to an image not found in Docker?\", \"Why do we need to install libgomp1 in a Docker file?\", \"Can you give an example of how to modify the Docker file to install libgomp1?\", \"What should we do if we encounter an issue with lib_lightgbm.so?\", \"How can we troubleshoot issues with image not found in Docker?\"]',\n",
       " '1667e95d': '[\\n\"Why does mlflow\\'s pyfunc.load_model return an error when executed in a lambda function? Shouldn\\'t it work normally?\", \\n\"What happens when mlflow raises an unexpected error in a lambda function? Is there a way to fix it?\", \\n\"Can mlflow work properly in a lambda function without failing with errors?\", \\n\"How do I resolve the AttributeError when using mlflow in a lambda function?\", \\n\"What should I consider when deploying a module to a lambda function to avoid these kinds of errors?\"',\n",
       " '624a3525': 'Here are the 5 questions based on the FAQ record:\\n\\n[{\"What is the end state of the video in Module 4: Deployment?\", \"How is the notebook related to the video and the repository?\", \"Can I just watch the video and expect everything to work?\", \"Why are we following a video notebook in Deployment Module?\", \"How does the notebook use mlflow pipelines?\"]',\n",
       " '1db86601': 'Here are the 5 questions based on the FAQ record:\\n\\n[\"How do I set up environment variables for my AWS credentials when running a Docker image?\", \"Why do I need to specify the environment variables before the image name when running a Docker container?\", \"Can I set AWS credentials using a configuration file instead of environment variables?\", \"Why is it necessary to specify the Docker run command options in a particular order?\", \"How can I mount the AWS credentials file into a Docker container from the host machine?\"]',\n",
       " '047baefe': 'Here are the 5 questions based on the FAQ record:\\n\\n[\"How do I see the model in the docker container in the app directory?\", \"What do I need to do to view the contents of the app directory in a docker container?\", \"Can I run a script inside a docker container using the docker run command?\", \"What is the purpose of the \\'CMD\\' command in a Dockerfile?\", \"Do I need to rebuild the Dockerfile every time I make changes to it?\"]',\n",
       " '4f240372': '[\"What are the possible reasons for the requested image\\'s platform not matching the detected host platform?\", \"How can I resolve the mismatch between the requested and detected platforms?\", \"What does the platform tag do in the \\'docker build\\' command?\", \"Is it necessary to include the platform tag every time I\\'m building a docker image?\", \"What are the benefits of specifying the platform when building a docker image?\"]',\n",
       " '7aef625b': '[\"What happens when I call apply_model() in score.ipynb and encounter an HTTPError with a status code of 403:\", \"Why do I get a HTTP 403 error when calling apply_model() in score.ipynb?\", \"Can you help me resolve this HTTPError 403 issue in apply_model()?\", \"What\\'s the correct URL to use when calling apply_model() in score.ipynb to avoid an HTTPError 403?\", \"Why can\\'t I access the NYC taxi data using the provided URL in the course\"]',\n",
       " 'a3aa3a7d': '{\"questions\": [\"Why do I get the error ModuleNotFoundError: No module named \\'pipenv.patched.pip._vendor.urllib3.response\\'?\", \"How can I resolve the error ModuleNotFoundError: No module named \\'pipenv.patched.pip._vendor.urllib3.response\\'?\", \"What does the command \\'pip install pipenv --force-reinstall\\' do?\", \"Why do I get an error when installing a package using pipenv?\", \"What packages should I install to resolve the issue with urllib3.response?\"]}',\n",
       " 'd2719204': '[\"If I\\'m running docker-compose up as shown in video 5.2, why do I get a login window when I go to http://localhost:3000/\", \"Do I need to enter a specific username and password when logging in to Grafana for the first time?\", \"What are the default login credentials for Grafana?\", \"Is there a specific location I can look for additional information about login credentials?\", \"Can I change my password after logging in with the default credentials?\"]',\n",
       " '30b8e8e6': '[\"Why do I get a message saying \\'unknown flag: --build\\' in the command prompt when trying to start monitoring services in Linux?\", \"What is the correct command to start services using docker compose in Linux?\", \"In video 5.2, why are we shown using \\'docker compose up --build\\' to start services when it\\'s not the correct command?\", \"Do I need to install docker-compose separately in Linux?\", \"Is there a difference in using \\'docker-compose up --build\\' instead of \\'docker compose up --build\\'?\"]',\n",
       " 'f33fc6e9': 'Here are 5 questions based on the FAQ record:\\n\\n[\"When running prepare.py, I get a KeyError \\'content-length\\'. Why does this happen?\", \"In prepare.py, why does the link to download taxi data seem to be broken?\", \"How do I resolve the KeyError \\'content-length\\' when running prepare.py?\", \"Can the broken link in prepare.py be replaced with a working one?\", \"What is the correct URL to use in prepare.py for downloading trip data?\"]',\n",
       " 'd828de2a': '[\\n\"What does it mean when a service exits with code 2 when running \\'docker-compose up --build\\' and sending data to the real-time prediction service?\",\\n\"Is a service exit with code 2 due to \\'app.py\\' in evidently service unable to import \\'from pyarrow import parquet as pq\\' a common issue?\",\\n\"What can I do if I encounter a \\'Max retries exceeded with url: /api\\' error when sending data to the real-time prediction service?\",\\n\"Why did installing pyarrow module \\'pip install pyarrow\\' solve the problem of \\'app.py\\' in evidently service not being able to import \\'from pyarrow import parquet as pq\\'?\",\\n\"If a machine restart and commenting out the pyarrow module did not solve the problem, what should I do next?\"',\n",
       " '03f20ec1': '[\\n\"What do I do when I get a ValueError: Incorrect item instead of a metric or metric preset was passed to Report?\",\\n\"What is the common reason behind ValueError: Incorrect item instead of a metric or metric preset was passed to Report?\",\\n\"Can you provide a solution to ValueError: Incorrect item instead of a metric or metric preset was passed to Report?\",\\n\"Why do I get ValueError: Incorrect item instead of a metric or metric preset was passed to Report?\",\\n\"How do I fix the error ValueError: Incorrect item instead of a metric or metric preset was passed to Report?\" ]',\n",
       " '249726fe': \"Here are the 5 questions:\\n\\n[What happens if we don't add a target='duration_min' when using RegressionQualityMetric()?, Can we use RegressionQualityMetric() without adding a target, How do we correctly use RegressionQualityMetric() in our code, What is the error we get if we don't include target='duration_min', Is it required to include 'duration_min' in our dataset when using RegressionQualityMetric()]\",\n",
       " '4e492af0': '[\"What can I do when I get a ValueError saying \\'Found array with 0 sample(s) (shape=(0, 6)) while a minimum of 1 is required by LinearRegression\\'?\",\\n\"What is the cause of the error \\'Found array with 0 sample(s) (shape=(0, 6)) while a minimum of 1 is required by LinearRegression\\'?\",\\n\"I am getting an error \\'Found array with 0 sample(s) (shape=(0, 6)) while a minimum of 1 is required by LinearRegression\\', how can I fix it?\",\\n\"Can you explain why I am getting the error \\'Found array with 0 sample(s) (shape=(0, 6)) while a minimum of 1 is required by LinearRegression\\'?\",\\n\"I recently installed the latest update, but I keep getting the error \\'Found array with 0 sample(s) (shape=(0, 6)) while a minimum of 1 is required by LinearRegression\\', what should I do?\"]',\n",
       " '10011dc1': 'Here are the 5 questions:\\n\\n[\\n\"How can I avoid \\'target columns\\' \\'prediction columns\\' not present errors after adding a metric to my monitoring?\",\\n\"Can you provide guidelines on what is required or optional when adding a custom metric?\",\\n\"How do I correctly add a metric that evaluates for correlations among the features?\",\\n\"What\\'s the correct procedure for troubleshooting \\'target columns\\' \\'prediction columns\\' not present errors?\",\\n\"Are there any specific parameters I need to configure when adding a new metric?\"',\n",
       " '92fb909a': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"Why does the standard login in Grafana not work?\", \"How do I fix the issue with standard requisites not working for login?\", \"What are the default username and password for Grafana?\", \"How can I resolve the problem with an error after trying to login to Grafana?\", \"How do I reset the admin password for a Grafana container?\"]',\n",
       " '2b8cb640': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"Why doesn\\'t my chart in Grafana get updates?\", \"How do I set a refresh interval in Grafana?\", \"Why is my chart not reflecting the updated data?\", \"How do I ensure my local timezone is set correctly in my metric generation script?\", \"What are some common issues that can cause charts not to update in Grafana?\"]',\n",
       " 'd4ceab0b': 'Here are 5 questions this student might ask based on the FAQ record:\\n\\n[\"Why did the Prefect server not run locally? \", \"How do I troubleshoot when the Prefect server stops immediately after starting? \", \"What is the recommended way to run a script if the Prefect server is not running?\", \"Is it normal for the Prefect server to stop immediately after starting when running locally?\", \"What should I do if I encounter an issue with the Prefect server running locally?\"]',\n",
       " '482e575f': 'Here are 5 questions based on the FAQ record:\\n\\n[\"When I try to run \\'docker compose up\\' in Module 5, why do I get a \\'no disk space left\\' error?\", \"How can I remove unused items in my Docker setup to free up disk space?\", \"What\\'s causing the high disk usage in my Docker setup, and how can I identify the main culprits?\", \"Can I remove Docker build cache to free up disk space?\", \"How can I use the Docker CLI to clean up unused containers and images?\"]',\n",
       " '33e775eb': 'Here are the 5 questions this student might ask based on the FAQ record:\\n\\n[\"What happens when I run docker-compose up –build and see an error code \\'php_network_getaddresses: getaddrinfo failed: Address family for hostname not supported\\'?\",\\n\"What is the problem with running the base command \\'docker-compose up –build\\'?\",\\n\"How do I troubleshoot the \\'Failed to listen on :::8080\\' error in Module 5: Monitoring?\",\\n\"Why might I experience issues with the Docker container\\'s IP address in PHP?\",\\n\"What do I need to do to resolve the \\'Address family for hostname not supported\\' error when connecting to a local server?\"]',\n",
       " '19a3d34a': 'Here is the list of questions in the desired format:\\n\\n[\"How can I generate charts like Evidently inside Grafana?\", \"Are there any native ways to recreate the Evidently dashboard in Grafana?\", \"How can I design my own plots in Grafana if I want to recreate the Evidently dashboard?\", \"Can I export the Evidently output in JSON format for external visualization?\", \"What additional options do I need to add to Evidently to include everything I need for non-aggregated visuals?\"]',\n",
       " '55c68f23': 'Here is the list of questions in JSON format:\\n\\n[\"What are the potential causes of getting an error \\'Unable to locate credentials\\' after running localstack with kinesis?\", \"How can I fix the error \\'Unable to locate credentials\\' while running localstack with kinesis?\", \"Do I need to add environment variables like AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY when running localstack with kinesis?\", \"What are the environment variables I need to add to my docker-compose.yaml file for running localstack with kinesis?\", \"Is it possible to configure AWS credentials before running localstack with kinesis using the aws command?\"]',\n",
       " '54020f0a': '```\\n[\"What causes the error \\'unspecified location constraint is incompatible\\' while creating a bucket with localstack and the boto3 client?\", \"How to fix the error \\'unspecified location constraint is incompatible\\' when calling the CreateBucket operation?\", \"What are the relevant region specific endpoints for the CreateBucket operation?\", \"How does specifying a LocationConstraint in the CreateBucketConfiguration affect the CreateBucket operation?\", \"Why does using AWS_DEFAULT_REGION help resolve the unspecified location constraint incompatibility issue?\"]',\n",
       " 'b6249d2c': 'Here is the list of questions:\\n\\n[\"When executing an AWS CLI command, why do I get the error \", \"After running an AWS CLI command, I get an AWSRequest object error at 0x7fbaf2666280. How do I fix it?\", \"How can I resolve the error \\'<botocore.awsrequest.AWSRequest object at 0x7fbaf2666280>\\' when running AWS CLI commands?\", \"I get an error \\'<botocore.awsrequest.AWSRequest object at 0x7fbaf2666280>\\' in AWS CLI. What should I do to get rid of this error?\", \"Why do I receive \\'<botocore.awsrequest.AWSRequest object at 0x7fbaf2666280>\\' when I execute an AWS CLI command like \\'aws s3 ls\\'?\"]',\n",
       " '31543d95': 'Here are the 5 questions that the student might ask based on the FAQ record:\\n\\n[\"What causes an \\'error at every commit: \"mapping values are not allowed in this context\"\\' when using pre-commit triggers?\", \\n\"What is the common problem that triggers \\'mapping values are not allowed in this context\\' at commit time?\", \\n\"Why are pre-commit hooks not executed when the module has an indentation error in .pre-commit-config.yaml?\", \\n\"What is the specific issue that M. Ayoub C. mentioned, especially the \\'repo\\' statement?\", \\n\"How do I fix an indentation issue in .pre-commit-config.yaml to avoid \\'mapping values are not allowed in this context\\'?\"]',\n",
       " 'e147bbb6': 'Here is the list of questions:\\n\\n[\\n\"How can I reconfigure pytest to zero after completing the previous folder?\",\\n\"Can I remove pytest test altogether?\",\\n\"What should I do with the .vscode folder from the previous testing?\",\\n\"Will I need to remove any folders before starting the new testing?\",\\n\"How do I tackle issues with pytest test reconfiguration?\"\\n]',\n",
       " 'dc55657f': '[\"What should I do when the get records command returns empty Records after following the video 6.3?\", \"How can I troubleshoot the issue when the get records command returns empty Records?\", \"Why does the get records command return empty Records even though I followed the instructions?\", \"Can I use the --no-sign-request option with the Kinesis get records call to resolve the issue?\", \"What does the --no-sign-request option do when used with the Kinesis get records call?\"]',\n",
       " 'f6979915': '[\\n\"What happens when trying to commit changes using Git after creating a pre-commit yaml file in Powershell?\",\\n\"How can I resolve an encoding error that occurs when executing \\'git commit\\' in Powershell?\",\\n\"I\\'m creating a pre-commit yaml file using Powershell, but I\\'m getting an error saying that it can\\'t detect byte \\'0xff\\'. Why is this happening?\",\\n\"What is the correct way to create the pre-commit yaml file in Powershell to avoid encoding errors?\",\\n\"Can I set the encoding for the yaml file when creating it, and if so, how do I do that?\"',\n",
       " '1076a121': '[\"What happens when running \\'git commit -m \\'Updated xxxxxx\\'\\' in Module 6, and how can I resolve an error \\'PythonInfo\\' object has no attribute \\'version_nodot\\',\", \"How do I address an \\'An unexpected error has occurred: CalledProcessError: command: ... return code: 1\\' when using a pre-commit hook with Git?\", \"Why am I seeing a \\'Problem description\\' with a high level of output when trying to commit using Git?\", \"Can I customize or reset default settings for a virtualenv in Module 6, and if so, how?\", \"How can I fix a \\'CalledProcessError\\' error when trying to install an environment for a pre-commit hook in this module?\"]',\n",
       " 'aa203ca7': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"What happens when I use custom packages in my source code but get a \\'module not found\\' error in Pytest?\", \"Why does running python test_model_service.py from the sources directory work but running pytest ./test/unit_tests fails?\", \"How do I resolve the issue of \\'No module named \\'production\\'\\' error when running pytest?\", \"Why does pytest not add the path where pytest is run to the sys.path?\", \"What alternative way is there to run pytest other than using python -m pytest?\"]',\n",
       " '8b04605d': 'Here are the 5 questions:\\n\\n[\\n\"What happens if I have custom packages in my source code and I\\'m using pre-commit hooks with pytest, causing the error \\'module not found\\'?\",\\n\"Why does my project structure pose a problem when using pre-commit hooks with pytest?\",\\n\"What does it mean when the error \\'No module named \\'production\\'\\' is raised during a \\'git commit -t \\'test\\'\\'?\",\\n\"How do I use the pytest hook instead, considering I have custom packages in my source code?\",\\n\"What should I do to ensure that the script in run.sh sets the right directory and runs pytest correctly?\"',\n",
       " 'a3b9af04': '[\"What are the common errors that occur when executing script files in a GitHub Actions workflow?\", \"Why do I receive a \\'Permission denied\\' error when running a script file in a GitHub Actions workflow? \", \"How can I troubleshoot issues with executing script files in a GitHub Actions workflow?\", \"What changes can I make to the CI YAML file definition to avoid permission errors when executing script files?\", \"How do I add execution permission to a script file in a GitHub Actions workflow using Git?\"]',\n",
       " 'b16aae74': 'Here are the 5 questions based on the FAQ record:\\n\\n[\"How do I run multiple Docker containers without overwhelming my system resources?\", \"How can I test a subset of containers in a complex `docker-compose` file?\", \"Can I dynamically select which containers to run when using `docker-compose`?\", \"How do I customize the behavior of my `docker-compose` services for testing?\", \"Is there a way to simplify the process of running and testing multiple Docker containers at once?\"]',\n",
       " '66326a87': 'Here are the 5 questions this student might ask:\\n\\n[\"Why do AWS regions need to match when running integration tests with Kinesis?\", \"What problems can occur if my AWS regions don\\'t match in my docker-compose file?\", \"How do I ensure my AWS regions match when running integration tests?\", \"What do I need to check if my integration tests are failing due to incorrect AWS region?\", \"Can you provide an example of how to set my AWS region in the docker-compose file?\"]',\n",
       " 'fb3c4150': 'Here are the 5 questions the student might ask:\\n\\n[\\n\"How do I resolve isort repo-related issues in pre-commit command?\", \\n\"What changes did Erick Calderin make to resolve the isort pre-commit problem?\", \\n\"What is the exact fix for the isort pre-commit failure?\", \\n\"Why did the pre-commit command fail with isort repo?\", \\n\"Can I manually set the version of isort to resolve the pre-commit issue?\"',\n",
       " '886d1617': 'Here are the 5 questions:\\n\\n[\"How do I destroy infrastructure created via GitHub Actions in Module 6: Best practices?\", \"What are the steps to destroy infrastructure created in AWS with CD-Deploy Action?\", \"Can I initialize Terraform in AWS locally?\", \"How do I configure Terraform to use a specific backend state file?\", \"What are the steps to reconfigure Terraform with a vars file in prod environment?\"]'}"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "65a96ae9-273f-46e8-beca-a4731dd3484f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('question_list_pk.pkl', 'wb') as file:\n",
    "    pickle.dump(question_list, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "3a11f198-351a-424f-b85e-91e25bb9fc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('question_list_pk.pkl', 'rb') as f_in:\n",
    "    results = pickle.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "b57aace3-e33d-4933-bd71-26f51f87c724",
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting ',' delimiter: line 1 column 27 (char 26)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[248], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m parsed_results \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc_id, json_questions \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m----> 4\u001b[0m     parsed_results[doc_id] \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_questions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.10.13/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m/usr/local/python/3.10.13/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m/usr/local/python/3.10.13/lib/python3.10/json/decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;124;03ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;124;03mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m \n\u001b[1;32m    351\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting ',' delimiter: line 1 column 27 (char 26)"
     ]
    }
   ],
   "source": [
    "parsed_results = {}\n",
    "\n",
    "for doc_id, json_questions in results.items():\n",
    "    parsed_results[doc_id] = json.loads(json_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "7ac99b0f-79d0-4c27-acc0-0c6c4f86f236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"How do I fix the error \"Error: Makefile:2: *** missing separator.  Stop.\" when trying to convert tabs in my document to spaces in VS Code?\", \"Why do I have to convert tabs to spaces in VS Code?\", \"Is it possible to use both tabs and spaces in VS Code?\", \"How can I solve the issue of tabs not being recognized as tabs in my document?\", \"Is there a specific stack or resource I should refer to for information on converting tabs to spaces in VS Code?\"]\n"
     ]
    }
   ],
   "source": [
    " print(json_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "21dc8fa1-fa4c-4ce0-af67-4aa9f440d9c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (1818083251.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[250], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    json_questions =[\"How do I fix the error \"Error: Makefile:2: missing separator.  Stop.\" when trying to convert tabs in my document to spaces in VS Code?\", \"Why do I have to convert tabs to spaces in VS Code?\", \"Is it possible to use both tabs and spaces in VS Code?\", \"How can I solve the issue of tabs not being recognized as tabs in my document?\", \"Is there a specific stack or resource I should refer to for information on converting tabs to spaces in VS Code?\"]\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "json_questions =[\"How do I fix the error \"Error: Makefile:2: missing separator.  Stop.\" when trying to convert tabs in my document to spaces in VS Code?\", \"Why do I have to convert tabs to spaces in VS Code?\", \"Is it possible to use both tabs and spaces in VS Code?\", \"How can I solve the issue of tabs not being recognized as tabs in my document?\", \"Is there a specific stack or resource I should refer to for information on converting tabs to spaces in VS Code?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "6ad0cd42-395d-4258-a617-14e5249deb78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\"What are the first steps to troubleshoot issues I\\'m encountering in the course?\", \"Can I search for solutions to my problem using specific keywords?\", \"What should I do if I\\'m unable to resolve an issue after trying the troubleshooting steps?\", \"How do I properly ask a question on Stackoverflow and other platforms?\", \"What should I do if I\\'m stuck on a problem and can\\'t seem to find a solution?\"]'"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.dumps(json_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "75a5d60e-62fb-48d9-a8ce-0eefed1a9418",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[doc_id] = json.dumps(json_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "88115efd-dc87-440a-a489-cc0e6c6e5cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_index = {d['id']: d for d in documents}\n",
    "\n",
    "final_results = []\n",
    "for doc_id, questions in parsed_results.items():\n",
    "    course=doc_index[doc_id]['course']\n",
    "    for q in questions:\n",
    "        final_results.append((q, course, doc_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "e65360ef-b085-4572-8fdc-17a6b3088310",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(final_results, columns=['question', 'course', 'document'])\n",
    "df.to_csv('ground_truth.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "037dfc01-c062-48be-ae84-2d1e5778e9b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>course</th>\n",
       "      <th>document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When will the course, which is focused on tech...</td>\n",
       "      <td>data-engineering-zoomcamp</td>\n",
       "      <td>c02e79ef</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can I still register for the course after it h...</td>\n",
       "      <td>data-engineering-zoomcamp</td>\n",
       "      <td>c02e79ef</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What platform should I use to stay updated on ...</td>\n",
       "      <td>data-engineering-zoomcamp</td>\n",
       "      <td>c02e79ef</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Will the course be available on multiple devic...</td>\n",
       "      <td>data-engineering-zoomcamp</td>\n",
       "      <td>c02e79ef</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How can I communicate with other students and ...</td>\n",
       "      <td>data-engineering-zoomcamp</td>\n",
       "      <td>c02e79ef</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>How do I set up a local repository on my compu...</td>\n",
       "      <td>data-engineering-zoomcamp</td>\n",
       "      <td>f2945cd2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>How do I ignore large database and other unwan...</td>\n",
       "      <td>data-engineering-zoomcamp</td>\n",
       "      <td>f2945cd2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>What is the best way to store my notes and ver...</td>\n",
       "      <td>data-engineering-zoomcamp</td>\n",
       "      <td>f2945cd2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>Should I store my passwords and keys in a Git ...</td>\n",
       "      <td>data-engineering-zoomcamp</td>\n",
       "      <td>f2945cd2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>Are there any other resources you would recomm...</td>\n",
       "      <td>data-engineering-zoomcamp</td>\n",
       "      <td>f2945cd2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>206 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              question  \\\n",
       "0    When will the course, which is focused on tech...   \n",
       "1    Can I still register for the course after it h...   \n",
       "2    What platform should I use to stay updated on ...   \n",
       "3    Will the course be available on multiple devic...   \n",
       "4    How can I communicate with other students and ...   \n",
       "..                                                 ...   \n",
       "201  How do I set up a local repository on my compu...   \n",
       "202  How do I ignore large database and other unwan...   \n",
       "203  What is the best way to store my notes and ver...   \n",
       "204  Should I store my passwords and keys in a Git ...   \n",
       "205  Are there any other resources you would recomm...   \n",
       "\n",
       "                        course  document  \n",
       "0    data-engineering-zoomcamp  c02e79ef  \n",
       "1    data-engineering-zoomcamp  c02e79ef  \n",
       "2    data-engineering-zoomcamp  c02e79ef  \n",
       "3    data-engineering-zoomcamp  c02e79ef  \n",
       "4    data-engineering-zoomcamp  c02e79ef  \n",
       "..                         ...       ...  \n",
       "201  data-engineering-zoomcamp  f2945cd2  \n",
       "202  data-engineering-zoomcamp  f2945cd2  \n",
       "203  data-engineering-zoomcamp  f2945cd2  \n",
       "204  data-engineering-zoomcamp  f2945cd2  \n",
       "205  data-engineering-zoomcamp  f2945cd2  \n",
       "\n",
       "[206 rows x 3 columns]"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e929fe8-22a4-414a-be2f-7a3f4a40a08e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
